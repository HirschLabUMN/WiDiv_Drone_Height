---
title: "WiDiv Panel Analysis Project"
author: "Dorothy Sweet"
date: "4/15/2020"
output: html_document
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(message = FALSE)
```

# Manipulate the Data locally before completing GWAS

  - Uses raw data exported from MATLAB 
  - local computer R script was run using R version 3.6.2
  
## Create Plot Order Files

  - has to be run for each year and location 
  - puts the plots in order of extraction
  
```{r}
## Clearing the global environment
rm(list=ls(all=TRUE))

################ PRODUCES PLOT OUTPUT ORDER BY DETERMINING PLOTS NORTH TO SOUTH THEN WEST TO EAST ######################
######## CHANGE INPUTS ########################
Year <- "2019" # options - 2018, 2019, 2020, 2021
Location <- "X5" # options - X3, X5, X3, X2
Path <- "/Users/dorothykirsch/Desktop/Projects_Data/Flights/"
Field = 'WIDIV'
# give the dimensions of the plot grid
plot_grid_width =  48    # number of plots in a range
plot_grid_length =  22   # number of ranges
total_num_plots = 1056
################################################################################################################################
# read in ROI output file from MatLab
plot_roi <- read.csv(paste0(Path,Year,"/SaintPaul/",Location,"_",Year, "/Data_Analysis/Plot_Order/Plots_Matlab_Roi_",Year, ".csv"))
# read in list of plots
plot_list <- as.matrix(read.csv(paste0(Path,Year,"/SaintPaul/",Location,"_",Year,"/Data_Analysis/Plot_Order/Plots_only_",Field,".csv")))


options(digits = 15) # change the number of digits seen in files
# remove unnecessary columns
plot_roi$Geometry <- NULL
plot_roi$BoundingBox <- NULL
plot_roi$left <- NULL
plot_roi$top <- NULL
plot_roi$right <- NULL
plot_roi$bottom <- NULL

# split x column 
plot_roi <- splitstackshape::cSplit(plot_roi, "X", sep = ",")

# remove extra X columns
plot_roi$X_6 <- NULL
plot_roi$X_5 <- NULL
plot_roi$X_4 <- NULL
plot_roi$X_3 <- NULL
plot_roi$X_2 <- NULL

# remove the bracket from X_1
plot_roi <- splitstackshape::cSplit(plot_roi, "X_1", sep = "[")

# rename keep row and remove unnecessary rows
plot_roi$X_coor <- plot_roi$X_1_2
plot_roi$X_1_1 <- NULL
plot_roi$X_1_2 <- NULL

# Split y column, remove extra Y columns, remove the bracket, and rename the keep row
plot_roi <- splitstackshape::cSplit(plot_roi, "Y", sep = ",")
plot_roi <- splitstackshape::cSplit(plot_roi, "Y_1", sep = "[")
plot_roi$Y_coor <- plot_roi$Y_1_2
plot_roi$Y_1_1 <- NULL
plot_roi$Y_1_2 <- NULL
plot_roi$Y_6 <- NULL
plot_roi$Y_5 <- NULL
plot_roi$Y_4 <- NULL
plot_roi$Y_3 <- NULL
plot_roi$Y_2 <- NULL

# Organize the plots X increasing and Y decreasing
plot_roi_temp <- plot_roi[order(plot_roi$X_coor),]
plot_roi.2 <- data.frame()
for(i in 1:plot_grid_length){
  if(nrow(plot_roi_temp)>=plot_grid_width){
    plot_roi_temp2 <- plot_roi_temp[1:plot_grid_width,]
  } else{
    plot_roi_temp2 <- plot_roi_temp[1:nrow(plot_roi_temp),]
  }
  plot_roi_temp2 <- plot_roi_temp2[order(plot_roi_temp2$Y_coor, decreasing = T),]
  plot_roi_temp <- plot_roi_temp[(plot_grid_width +1):nrow(plot_roi_temp),]
  
  plot_roi.2 <- rbind(plot_roi.2, plot_roi_temp2)
}

# add new column with non-serpentine order
plot_roi.2$non_serp <- 1:(total_num_plots) 

# reorder the list of plots so no longer serpentine
Count = 1  # setup for 'for loop'
plot_list2 = matrix(nrow = (total_num_plots), ncol = 1) # make a new plot list file to put the new order in

for (Count in 1:plot_grid_length){
  if (Count %% 2 == 1){
    for (plot in ((((Count - 1)* plot_grid_width)+1):(Count *plot_grid_width))){
      plot_list2[plot,1] = plot_list[plot,1]}
    Count = Count + 1
  }else if (Count %% 2 == 0){
    for (plot in ((((Count - 1)* plot_grid_width)+1):(Count *plot_grid_width))){
      if ((plot %% plot_grid_width) != 0){
        plot_list2[((Count * plot_grid_width) - ((plot %% plot_grid_width)-1)),1] = plot_list[plot,1]
      }else if ((plot %% plot_grid_width) == 0){ 
        plot_list2[(plot - (plot_grid_width -1)),1] = plot_list[plot,]}}
    Count = Count + 1
  }
  }

# add non-serpentine plot order column to new plot_list2 so will have common column with plot_roi.2
plot_list2 <- as.data.frame(plot_list2)
plot_list2$non_serp <- 1:(total_num_plots)

# add new plot order list to plot_roi.2
plot_roi.3 <- merge(plot_roi.2, plot_list2, by = "non_serp")

## reorganize back into original roi order (as output would be -> ordered by fields)
plot_order.4 <- plot_roi.3[order(Fields)]
plot_order.4$Plot <- plot_order.4$V1

plot_order.5 <- plot_order.4$Plot

#Print out the Plot Order text file
write.table(plot_order.4$Plot, file = paste0(Path,Year,"/SaintPaul/",Location,"_",Year,"/Data_Analysis/Plot_Order/","Plots_",Field,".txt"), sep = "\n", row.names = F, col.names = "Plot")
```

## Plot Manual Measurements vs. Extracted Measurements

  - Not for 2018 or 2019 since there are no manual measurements in YC
    from those years
  - Has to be run individually for year, location, and date

```{r}
## Clearing the global environment
rm(list=ls(all=TRUE))

library(ggplot2)
library(RColorBrewer)
######## CHANGE INPUTS ##########
Year <- "2021" # options - 2020, 2021
Location <- "X2" # options - X3, X2
fields <- c("WIDIV")
Path = "/Users/dorothykirsch/Desktop/Projects_Data/Flights/"
date = "08062021" # options - 06032020,06082020,06162020,06232020,07012020,07222020,07292020,06072021,06152021,06232021,06282021,07082021,07132021,07232021,08062021
####################
# reading in the data from the flights and pulling out the corrected 95th percentile. 
for(name in fields){
assign(paste0("field_",name),read.delim(paste0(Path,Year,"/SaintPaul/",Location,"_",Year,"/Data_Analysis/Height/Means/",name,"/97min3Mean_",date,"newAgisoft_",name,".txt"), header = F)) 
}

fields_files_list <- mget(ls(pattern = "field_" )) # making a list of the files with field data for later use

# reading in the plot order files
for(name in fields){
  assign(paste0("plot_",name),read.delim(paste0(Path,Year,"/SaintPaul/",Location,"_",Year,"/Data_Analysis/Plot_Order/Plots_",name,".txt"), header = T, sep = "\n", dec = "."))
}

plots_list <- ls(pattern = "plot_") # making a list of the NAMES of the files with plot data for later use
plots_files_list <- mget(ls(pattern = "plot_")) # making a list of the files with plot data for later use

# combine the plot names and the extracted heights 
combined_all <- data.frame()
for(i in 1:length(plots_list)){
  combined.temp <- cbind(plots_files_list[[i]],fields_files_list[[i]])
  combined_all <- rbind(combined_all, combined.temp)
}

 ##### read in data from hand height measurements 
height <-read.csv(paste0(Path,Year,"/SaintPaul/",Location,"_",Year,"/manual_measurements/",date,".csv"), header=TRUE) #Read in the file with the hand measurements (originally read in with ,row.names =1 to make the row names the first collumn but changed on 21 Nov 2019 so it could be put in a graph easier.)

height$Mean <- (height$Height1 + height$Height2 + height$Height3 + height$Height4 + height$Height5)/5 #Calculate the average plant height per plot

# combine all the extracted heights
all <- merge(height, combined_all, by = "Plot", all.x = T) # combine all extracted and hand heights

# plot all the measurements
plot <- ggplot(all, aes(x = V1, y = Mean)) +
  geom_text(aes(label = Plot)) +
  theme_classic() +
  xlab("Plant Height Extraction") +
  ylab("Manual Measurements") +
  geom_smooth(method = "lm", se = F)

plot

pdf(file = paste0(Path,Year,"/SaintPaul/",Location,"_",Year,"/Extracted_Manual_Plots/Rough_Extract2Manual_",date,".pdf"), height = 5, width = 10)
plot
dev.off()
```

## Make File of Extracted Heights for Day

  - Has to be run for each year, location, and date
  - outputs a .csv file of the plot name with the extracted height for each flight
  
```{r}

## Clearing the global environment
rm(list=ls(all=TRUE))

library(ggplot2)
library(RColorBrewer)
######## CHANGE INPUTS ##########
Path = "/Users/dorothykirsch/Desktop/Projects_Data/Flights/"
Year <- "2019" # options - 2018, 2019, 2020, 2021
Location <- "X5" # options - X3, X5, X3, X2
date = "08082019" # options - 05232018,06052018,06132018,06282018,07092018,07122018,07172018,07242018,07312018,08092018,08172018,06022019,06062019,06082019,06102019,06132019,06172019,06252019,06282019,07022019,07082019,07102019,07122019,07152019,07172019,07182019,07192019,07222019,07252019,07312019,08082019,08212019,05212020,06032020,06082020,06162020,06232020,07012020,07222020,07292020,05132021,06012021,06072021,06152021,06232021,06282021,07082021,07132021,07162021,07232021,08062021
fields = c("WIDIV")
################################
# reading in the data from the flights and pulling out the corrected 95th percentile. 
for(name in fields){
assign(paste0("field_",name),read.delim(paste0(Path,Year,"/SaintPaul/",Location,"_",Year,"/Data_Analysis/Height/Means/",name,"/97min3Mean_",date,"newAgisoft_",name,".txt"), header = F)) 
}

fields_files_list <- mget(ls(pattern = "field_" )) # making a list of the files with field data for later use

# reading in the plot order files
for(name in fields){
  assign(paste0("plot_",name),read.delim(paste0(Path,Year,"/SaintPaul/",Location,"_",Year,"/Data_Analysis/Plot_Order/Plots_",name,".txt"), header = T, sep = "\n", dec = "."))
}

plots_list <- ls(pattern = "plot_") # making a list of the NAMES of the files with plot data for later use
plots_files_list <- mget(ls(pattern = "plot_")) # making a list of the files with plot data for later use

# combine the plot names and the extracted heights 
combined_all <- data.frame()
for(i in 1:length(plots_list)){
  combined.temp <- cbind(plots_files_list[[i]],fields_files_list[[i]])
  combined_all <- rbind(combined_all, combined.temp)
}

# Print a text file with all of the extracted heights with their plots
write.table(combined_all, file = paste0(Path,Year,"/SaintPaul/",Location,"_",Year,"/Extracted_Plot_Heights/Plots_with_Heights",date,".txt"), sep = "\t", row.names = F, col.names = c("Plot", paste0("X",date)))
```

## File of Extracted Heights over Season
  
  - plots the extracted heights over the entire growing season in order
  - has to be completed for each year and location. The list of dates for the year has to be changed for each.

```{r}
## Clearing the global environment
rm(list=ls(all=TRUE))

library(ggplot2)
library(RColorBrewer)
library(tidyr)
library(stringr)
library(plotly)

################################## CHANGE INPUTS ##################################
Path = "/Users/dorothykirsch/Desktop/Projects_Data/Flights/"
Year <- "2019"
Location <- "X5"
dates = c("06022019","06062019","06082019","06102019","06132019","06172019","06252019","06282019","07022019","07082019","07102019","07122019","07152019","07172019","07182019","07192019","07222019","07252019","07312019","08082019","08212019")
##################################################################################
# read in all of the files from the previous code ...
for(flight in dates){
assign(paste0("Flight_",flight),read.delim(paste0(Path,Year,"/SaintPaul/",Location,"_",Year,"/Extracted_Plot_Heights/Plots_with_Heights",flight,".txt"), header = T)) 
}

# make a list of objects with flight data
flight_files_list <- mget(ls(pattern = "Flight_")) # making a list of the files with plot data for later use

# remove all of the repeating variables in Plot (fill)
## Initialize combined dates matrix with first date
combined_dates <- flight_files_list[[1]][grep("Inbred", flight_files_list[[1]]$Plot, invert = TRUE),] # remove inbred fill
combined_dates <- combined_dates[grep("Hybrid", combined_dates$Plot, invert = TRUE),] # remove hybrid fill
combined_dates <- combined_dates[grep("PH207", combined_dates$Plot, invert = TRUE),] #remove PH207 fill

## append all of the other dates while removing fills
for(time in 2:length(flight_files_list)){
  flight.temp <- flight_files_list[[time]][grep("Inbred", flight_files_list[[time]]$Plot, invert = TRUE),]
  flight.temp <- flight.temp[grep("Hybrid", flight.temp$Plot, invert = TRUE),]
  flight.temp <- flight.temp[grep("PH207", flight.temp$Plot, invert = TRUE),]
  combined_dates <- merge(combined_dates, flight.temp, by = "Plot")
}

# Export combined matrix
write.table(combined_dates, file = paste0(Path,Year,"/SaintPaul/",Location,"_",Year,"/Extracted_Plot_Heights/Plots_with_Heights_all_dates.txt"), sep = "\t", row.names = T)

# Make a plot 
## Long format 
combined_long <- pivot_longer(combined_dates, cols = (2:length(combined_dates)), names_to = "Date", values_to = "Heights")

## Plot the data #############CHECK THIS OUT SO CAN REMOVE FILTER!!!
### TOTAL 
pdf(file = paste0(Path,Year,"/SaintPaul/",Location,"_",Year,"/Extracted_Plot_Heights/Rough_alldates_wgroundsub.pdf"), height = 2.5, width = 3)
combined_long %>%
  separate(Date, c(NA,"Date"), "X") %>%
  filter(Heights <= 275) %>%
  ggplot(aes(x = Date, y = Heights, group = Plot)) +
  geom_line() +
  theme_classic() +
  ggtitle("Whole Field") +
  theme(
    axis.text.x = element_text(angle = 90)
  )
dev.off()

# LOOK AT PROBLEM DATES
summary(combined_dates)
```

## Combine Flight and Genotype

  - makes a combined file of extracted plant heights and genotype information
  - needs to be run for each year and location
  
```{r}
## Clearing the global environment
rm(list=ls(all=TRUE))

library(ggplot2)
library(RColorBrewer)
library(tidyr)
library(stringr)
library(plotly)

#################################CHANGE THESE#####################################
Path <- "/Users/dorothykirsch/Desktop/Projects_Data/Flights/"
Year <- "2019" # options - 2018,2019,2020,2021
Location <- "X5" # options - X3, X5, X3, X2
Loc <- "StPaul"
################################################################
# read in files
flight_data <- read.delim(paste0(Path,Year,"/SaintPaul/",Location,"_",Year,"/Extracted_Plot_Heights/Plots_with_Heights_all_dates.txt"))
genotype_data <- read.csv(paste0(Path,Year,"/SaintPaul/",Location,"_",Year,"/Data_Analysis/", Year, "_Widiv_FieldBook.csv"))

#remove any non-YC data
flight_data <- flight_data%>%
  filter(str_detect(string = Plot, pattern = "YC") )
# Make a Rep column based on first number after the : in the Plot column
genotype_data$Plot <- as.character(genotype_data$Plot) #make Plot a character 
genotype_data$temp <- sapply(genotype_data$Plot, function(x) strsplit(x, split = ":", fixed=T)[[1]][2]) #make a temporary column just the last part of the plot name
genotype_data$Rep <- sapply(genotype_data$temp, function(x) str_sub(x, start=1, end = 1)) # break apart temp to get the Rep
genotype_data$temp <- NULL # get rid of temp column

# merge all of the data into one file
flight_genotype_data <- merge(genotype_data, flight_data, by = "Plot")

# export the complete plant height data with genotype file
write.table(flight_genotype_data, file = paste0(Path,Year,"/SaintPaul/",Location,"_",Year,"/Data_Analysis/Genotype_Plant_Height_Data_", Loc,"_", Year, ".txt"), sep = "\t", row.names = T)
```

## Normalize Extracted Heights for comparability
  
  - Two separate scripts for steps in normalizing the extracted plant heights to each other
  
### Calculate Exported GCP Totals

  - needs to be run for each year and location
  - the GCP_dates need to be changed for all of the dates within that year and location
  
```{r}
## Clearing the global environment
rm(list=ls(all=TRUE))

library(ggplot2)
library(tidyverse)
##### READ IN ALL EXTRACTED PIXEL VALUES FOR THE GCPS ##############################################
Path <- "/Users/dorothykirsch/Desktop/Projects_Data/Flights/"
Year <- "2019"
Field <- "X5"
GCP_dates <- c("06022019","06062019","06082019","06102019","06132019","06172019","06252019","06282019","07022019","07082019","07102019","07122019","07152019","07172019","07182019","07192019","07222019","07252019","07312019","08082019","08212019")
# initiate file for all GCP data for the year
num.GCPs <- list.files(paste0(Path, Year, "/SaintPaul/", Field, "_", Year, "/Data_Analysis/Height/All_Pixels/GCP/"), pattern = paste0("All_pixels_", GCP_dates[1]), full.names = F)
num.GCPs <- length(num.GCPs)/20
  
GCP.tot.all <- as.data.frame(matrix(ncol = 1, nrow = num.GCPs))
colnames(GCP.tot.all) <- "GCP.num"
for (g in 1:num.GCPs) {
  GCP.tot.all[g,1] <- g
}
# for loop for individual flight days
for (y in 1:length(GCP_dates)) {
  # read in extracted pixel values for individual days
  pixel_filenames <- list.files(paste0(Path, Year, "/SaintPaul/", Field, "_", Year, "/Data_Analysis/Height/All_Pixels/GCP/"), pattern = paste0("All_pixels_", GCP_dates[y]), full.names = F)
  # initiate temp file
  temp_all_pix <- as.data.frame(matrix(nrow = 0,ncol = 2))
  colnames(temp_all_pix) <- c("GCP.num", "Pixel.val") 
  # initiate GCP height file
  GCP_height <- as.data.frame(matrix(nrow = 0, ncol = 2))
  colnames(GCP_height) <- c("GCP.num", "Extracted_height")
  x <- 1
  while( x < 11) {
    for (p in ((((x-1)*20)+1):(x*20))) {
      # read in individual file
      temp_pix <- read.delim(paste0(Path, Year, "/SaintPaul/", Field, "_", Year, "/Data_Analysis/Height/All_Pixels/GCP/", pixel_filenames[p]), header = F)
      # unlist columns in temp_pix
      temp_pix <- data.frame(x = unlist(temp_pix))
      # add to temp_all_pix
      temp_all_pix[((nrow(temp_all_pix) + 1):((nrow(temp_all_pix) + nrow(temp_pix)))), 1] <- x # adds the GCP number first
      temp_all_pix[(((nrow(temp_all_pix) - nrow(temp_pix)) +1) : nrow(temp_all_pix)), 2] <- temp_pix[(1:nrow(temp_pix)),1] # adds temp_pix values to temp_all_pix
      ########## THIS IS WHERE THE SUBSET THING SHOULD BE ADDED GCP.num == x and add to GCP_height and then export outside of the while loop
      sub.1 <- subset(temp_all_pix, GCP.num == x)
      # add to a separate file with the 'extracted height' for GCPs
      GCP_height[x,1] <- x
      GCP_height[x,2] <- (max(sub.1$Pixel.val) - min(sub.1$Pixel.val))
    }
    # add to x 
    if (x < (length(pixel_filenames)/20)) {
      x <- x + 1
    } else {
      x <- 11
    }
  }
  # export the extracted GCP heights
  write.csv(GCP_height, paste0(Path, Year, "/SaintPaul/", Field, "_", Year, "/Extracted_GCP_for_Normalization/Extracted_GCP_",GCP_dates[y], ".csv"), row.names = F)
  # use temp_all_pix to make histogram faceted on GCP
  temp_all_pix$Pixel.val.100 <- temp_all_pix$Pixel.val * 100
  # export the plot 
  pdf(paste0(Path, Year, "/SaintPaul/", Field, "_", Year, "/Normalization_graphs/All_pixels_hist_", GCP_dates[y], ".pdf"), height = 3, width = 4)
  PLOT <- ggplot(temp_all_pix, aes(x = Pixel.val.100)) +
    geom_histogram(bins = 100) +
    ggtitle( GCP_dates[y]) +
    facet_wrap(~ GCP.num ) +
    xlab("Extracted Pixel Values")
  print(PLOT)
  dev.off()
  # read in manual measurements for the field and extracted measurements for the rest of the field
  GCP_manual <- read.csv(paste0(Path, Year, "/SaintPaul/", Field, "_", Year, "/Extracted_GCP_for_Normalization/Manual_GCP.csv"))
  # merge extracted and manual measurements by GCP.num
  GCP.tot <- merge(GCP_height, GCP_manual, by = "GCP.num")
  # calculate proportion of actual divided by extracted
  GCP.tot$Proportion <- GCP.tot$Manual_height_ft/GCP.tot$Extracted_height
  # combine the GCP.tot with the proportion included in master file for the year
  GCP.tot.all <- merge(GCP.tot.all, GCP.tot[,c(1,5)], by = "GCP.num")
  names(GCP.tot.all)[names(GCP.tot.all) == 'Proportion'] <- paste0("X", GCP_dates[y])
}
# Export combined GCP.tot for the year
write.csv(GCP.tot.all, paste0(Path, Year, "/SaintPaul/", Field, "_", Year, "/Extracted_GCP_for_Normalization/Manual_Extracted_Proportion_GCP.csv"), row.names = F)
```

### Normalize Extracted Heights and Export

  - Make sure adjustments are made to Manual_Extracted_Proportion_GCP.csv for the year before running this script (Remove plots based on All_pixels_hist_date.pdf) - if it does not show a peak for the ground and a peak for the top of the GCP it needs to be removed
  - needs to be run for each year and location
  - exports a plot of rough normalized plant heights

```{r}
## Clearing the global environment
rm(list=ls(all=TRUE))

library(ggplot2)
library(tidyverse)
##### READ IN ALL EXTRACTED PIXEL VALUES FOR THE GCPS ##############################################
Path <- "/Users/dorothykirsch/Desktop/Projects_Data/Flights/"
Year <- c("2021")
Field <- c("X2")
###################################################################################################
# Year for loop
for (y in 1:length(Year)) {
  # read back in file with actual to extracted proportion
  GCP.tot.all <- read.csv(paste0(Path, Year[y], "/SaintPaul/", Field[y], "_", Year[y], "/Extracted_GCP_for_Normalization/Manual_Extracted_Proportion_GCP.csv"))
  # read in file with extracted plot heights
  extract_plant <- read.delim(paste0(Path, Year[y], "/SaintPaul/", Field[y], "_", Year[y], "/Data_Analysis/Genotype_Plant_Height_Data_StPaul_",Year[y],".txt"))
  # calculate average proportion from remaining GCPs for each date
  GCP.tot.all$GCP.num <- as.character(GCP.tot.all$GCP.num) # change the GCP.num column to charactr for adding average
  GCP.tot.all[(nrow(GCP.tot.all) + 1),1] <- "average" # make a row for the average
  GCP.tot.all <- as.data.frame(GCP.tot.all) # change from tibble to data frame
  # initialize duplicate data frame to put normalized values in
  normal_plant <- extract_plant
  # for loop for normalization of heights through each date 
  for (g in 2:ncol(GCP.tot.all)) {
    GCP.tot.all[nrow(GCP.tot.all),g] <- mean(na.omit(GCP.tot.all[,g])) # calculate the average value for every GCP proportion
    # for loop for normalization through each plot
    for (p in 1:nrow(extract_plant)) {
      normal_plant[p,(g+4)] <- GCP.tot.all[nrow(GCP.tot.all),g]*extract_plant[p,(g+4)] # multiply proportion by extract to get normalized
    }
  }
  # export normalized plant heights
  write.table(normal_plant, paste0(Path,Year[y],"/SaintPaul/",Field[y],"_",Year[y],"/Data_Analysis/Normalized_Genotype_Plant_Height_Data_StPaul_",Year[y],".txt"))
  # create a long format of normalized plant height
  normal_long <- normal_plant %>%
    pivot_longer(cols = (6:ncol(normal_plant)), names_to = "Date",values_to = "Heights")
  # export rough plot of normalized height over time
  pdf(paste0(Path, Year[y], "/SaintPaul/", Field[y], "_", Year[y], "/Normalization_graphs/Rough_all_dates_Normalized_Heights_",Year[y],".pdf"), height = 2.5, width = 3)
  normal_height <- normal_long %>%
    separate(Date, c(NA,"Date"),"X") %>%
    ggplot(aes(x = Date, y = Heights, group = Plot)) +
    geom_line() +
    theme_classic() +
    ggtitle("Whole Field") +
    theme(
      axis.text.x = element_text(angle = 90)
      )
  print(normal_height)
  dev.off()
}
```

## Remove Plant Height File Out-liers

  - Has to be completed for each year and location. Need to change Year, Location, and planting_date
  - plots could be removed due to too many decreases from one flight to the next, a stand count < 10, or never growing

```{r}
## Clearing the global environment
rm(list=ls(all=TRUE))

library(ggplot2)
library(RColorBrewer)
library(tidyverse)
library(tidyr)
library(stringr)
library(plotly)
library(data.table)
############################### CHANGE THESE ################################
Path <- "/Users/dorothykirsch/Desktop/Projects_Data/Flights/"
Year <- "2021"
Location <- "X2" # 2018:X3, 2019:X5, 2020:X3, 2021:X2
Loc <- "StPaul" 
number_rows <- 48 #StPaul2021:48; StPaul2020:48; StPaul2019: 48; StPaul2018: 48
number_ranges <- 22 #StPaul2021:22; StPaul2020:22; StPaul2019: 22; StPaul2018: 22
contains_yield = "NO" # always "NO"
Lowest_acceptable_change <- 0
# List_percentages <- c(0,0.05,0.10,0.15,0.20)
planting_date <- "05062021" ### StPaul2018:05142018, StPaul2019:05302019, StPaul2020:05072020, StPaul2021:05062021
#############################################################################
# Read in plant height data
  num_cols = 0
  Full_file <- read.delim(paste0(Path,Year,"/SaintPaul/",Location,"_",Year,"/Data_Analysis/Normalized_Genotype_Plant_Height_Data_", Loc, "_", Year, ".txt"),sep = " ")

# removing erroneous days
Full_file$X06102019 <- NULL
Full_file$X07122019 <- NULL
Full_file$X07182019 <- NULL 
Full_file$X07192019 <- NULL

Full_file$Change <- (Full_file[ , (ncol(Full_file) - num_cols)] - Full_file[ , 6])

#Add stand count to the file 
stand_count <- read.csv(paste0(Path,Year,"/SaintPaul/",Location,"_",Year,"/StandCount_",Loc,"_",Year,".csv"))
stand_count$Stand <- as.numeric(stand_count$Stand)
Full_file <- merge(Full_file,stand_count,by = "Plot")
Full_file <- Full_file[,c(1,2,3,4,5,ncol(Full_file),6:(ncol(Full_file)-1))] #reorganize the file 

summary(Full_file$Change)

#Full_file$X07062020 <- NULL
Full_file_wo_zeros <- Full_file
#Change All Plots that never grow to NAs
for (row in 1:(nrow(Full_file))) {
  if (is.na(Full_file[row,(ncol(Full_file))])) {
    next()
  } else if (Full_file[row,(ncol(Full_file))] <= Lowest_acceptable_change) {
    for (col in 7:(ncol(Full_file)-1)) {
      Full_file_wo_zeros[row,col] <- NA
    }
  } else {
    for (col in 7:(ncol(Full_file)-1)) {
      Full_file_wo_zeros[row,col] <- Full_file[row,col]
    }
  }
}

#Read in separate file with all of the plots from the field (not just those with yield)
Data_file <- read.delim(paste0(Path,Year,"/SaintPaul/",Location,"_",Year, "/Data_Analysis/Normalized_Genotype_Plant_Height_Data_",Loc,"_",Year,".txt"),sep=" ")

# make a separate file of just the plots with new columns for range and row
Plots <- Data_file[,1, drop = F]
Plots$Range <- 0
Plots$Row <- 0

# assign range number for each plot
for (range_count in 0:number_ranges-1) {
  for (i in 1:number_rows) {
    Plots[((range_count * number_rows)+i),2] <- (range_count +1)
  }
}

#assign row number for each plot
ranges <- seq(from = 1, to = nrow(Plots), by = number_rows)
for (i in 1:length(ranges)) {
  if (i %% 2 == 1) {
    for (n in 0:(number_rows -1)) {
      Plots[((ranges[i]) + n),3] <- n+1
    }
  } else if (i %% 2 == 0) {
    for (n in ((number_rows - 1) :0)) {
      Plots[((ranges[i]) + n), 3] <- number_rows - n 
    }
  }
}

# removes the yield column if it is included in the data frame
if (contains_yield == "YES"){
  remove_columns = 3
}else if (contains_yield == "NO"){
  remove_columns = 2
}

# Export Heatmap of the file before removing plots
Heatmap_file <- merge(Plots, Full_file_wo_zeros, by = "Plot")
for (num_column in 9:(ncol(Heatmap_file) - (remove_columns-1))) {
  other_columns <- as.data.frame(Heatmap_file[,c(2,3,num_column)])
  other_columns_names <- colnames(other_columns)
  pdf(paste0(Path,Year,"/SaintPaul/",Location,"_",Year, "/Data_Analysis/Heatmaps/Heatmap_Initial_Plots_X", other_columns_names[3], "_", Loc, "_", Year, ".pdf"))
  
  j <- ggplot(other_columns, aes(x = Row, y = Range, fill = other_columns[,3])) +
    geom_tile() +
    scale_fill_gradient(low = "blue", high = "orange") + 
    theme_classic() +
    theme(legend.title = element_blank())
  
  print(j)

  dev.off()
}

##############################################################################################################################

hist_stand <- merge(Plots, stand_count, by = "Plot")

pdf(paste0(Path,Year,"/SaintPaul/",Location,"_",Year, "/Data_Analysis/Heatmaps/StandCount_", Loc, "_", Year, ".pdf"))

c <- ggplot(hist_stand, aes(x = Row, y = Range, fill = Stand)) +
  geom_tile() +
  scale_fill_gradient(low = "blue", high = "orange") + 
  theme_classic() +
  theme(legend.title = element_blank())

print(c)

dev.off()

write.table(summary(hist_stand),paste0(Path,Year,"/SaintPaul/",Location,"_",Year, "/Data_Analysis/StandCount_Summary_", Loc, "_", Year, ".txt"))

# for loop to remove plots based on stand count
for (row in 1:nrow(Full_file_wo_zeros)) {
  if (Full_file_wo_zeros[row,6] < 10) {
    Full_file_wo_zeros[row,c(7:ncol(Full_file_wo_zeros))] = NA
  }
}

plots_removed_stand <- summary(is.na(Full_file_wo_zeros[7]))

# Temporarily remove descriptive columns
dips_file <- Full_file_wo_zeros[,c(1,7:(ncol(Full_file_wo_zeros)-1))]
####################################################################################
# Remove individual plots on individual days for dips and individual plots overall #
############### Heavily based on Julian Cooper's script ############################
####################################################################################
# Function to filter peaks and valleys
big_data <- data.frame() #make empty data frame
filter_peaks <- function(df) { #Input df into function
  for (i in seq_len(nrow(df))) { #For each row in data frame
    row <- df[i,] #Subset row
    names <- colnames(row) # Make list of column names
    # Pivot row into long
    row <- pivot_longer(row, cols = names[2:length(names)], names_to = "flight", values_to = "height")
    # print(row)
    left <- 1 # Set left end of sliding evaluation window
    right <- 3 # Set right end of sliding evaluation window
  # Check for dips
    row[1,4] <- as.character(NA) # Nothing to compare first flight to, automatically label good
    for (j in 2:(length(names)-2)) { #For each column in row starting at 3
      if (is.na(row[j,3])) { # adding an if statement for dealing with NA values for height
        next()
      } else {
        # Set sliding evaluation window
        left <- j - 1
        right <- j + 1
        # Evaluate for a dip
        if(row[j,3] < (row[left,3] * 0.80))
          if(row[right,3] > row[j,3])
            if(row[left,3] < row[right,3])
            {row[j,4] <- "dip"}
        # Evaluate for a peak
        if(row[j,3] > (row[right,3] * 1.20))
          if (row[right,3] > row[left,3])
            if (row[left,3] < row[j,3])
            {row[j,4] <- "peak"}
    }
    }
    # Rules for checking if last data point is valley, can't check for peaks b/c nothing to compare to on right
    if (is.na(row[(length(names)-1),3 ])) {
      row[(length(names)-1),4] <- as.character(NA)
    } else if ((row[(length(names)-1),3]/row[(length(names)-2),3] < 0.80)){
      row[(length(names)-1),4] <- "dip"
    #((row[(length(names)-1), 3]/row[(length(names)-2), 3]) < .90) {
    #  row[(length(names)-1),4] <- "dip"
    } else {
      row[(length(names)-1),4] <- NA
    }
    big_data <- rbind(big_data, row) # Bind each row back to the empty data frame created outside the function
    }
    dipdrop <<- big_data # Export data frame with peaks and valleys labeled outside of function
}
# Run Function
filter_peaks(dips_file)
# Make new column copying height
dipdrop$heightNA <- dipdrop$height
# Replace data points with NA if tagged as peak or valley
dipdrop$heightNA <- ifelse(is.na(dipdrop$...4), dipdrop$height, ifelse(dipdrop$...4 == "dip",  NA, ifelse(dipdrop$...4 == "peak", NA, dipdrop$height)))
# Remove extra columns
dipdrop <- dipdrop[,c(1, 2, 5)]
# Pivot wide To count # of NA per row
wide <- pivot_wider(dipdrop, names_from = "flight", values_from = "heightNA")
# Make new column with na counts
wide$na_count <- apply(wide, 1, function(x) sum(is.na(x)))
# Pivot back to long to filter out na counts past certain threshold
long <- wide %>%
  pivot_longer(cols = 2:(ncol(wide) -1), names_to = "flight", values_to = "height")
# Count na
plot_na <- long %>%
  summarise(sum(is.na(height))) 
# Filter plots with na count > 3
filter <- long
filter$height <-  ifelse(filter$na_count >= 3, NA, filter$height)
# Count number of plots
filter.2 <- long %>%
  filter(na_count < 3)
#filter %>%
#   summarise(unique(Plot)) # Keeps 838 of 859 plots
# Count na
filter%>%
  summarise(sum(is.na(height))) # 212 NA data points
# Pivot back to wide
final <- pivot_wider(filter, names_from = "flight", values_from = "height")
final.2 <- pivot_wider(filter.2, names_from = "flight", values_from = "height")
# Save data frame
write.csv(final.2, paste0(Path,Year,"/SaintPaul/",Location,"_",Year, "/Data_Analysis/PlantRatio_plot_stand_dips_removed_", Year, ".csv"),  row.names = FALSE)
# remove na_count column
final$na_count <- NULL
##################
# QC
# Plot
ggplot(data=filter, aes(x=flight, y=height)) +
  #geom_point(color="gray", size=.1) +
  geom_line(aes(group=Plot), color="gray") +
  stat_smooth(method="loess", formula = y ~ x) +
  stat_summary(aes(flight, height), geom = 'point', fun = mean, shape = 17, size = 3, col = "red") +
  theme_bw() +
  xlab('Flight (GDD)') +
  ylab('Plant Ratio') +
  ggtitle(paste0(Year, " Double Dilated Percent Canopy Cover - Weeds, Stand, Peaks and Valleys"))
# merge the number of decreases with the full file missing plots that never grew
Full_file_with_decreases <- merge(Full_file_wo_zeros[,c(1:6,ncol(Full_file_wo_zeros))], final, by = "Plot")

#remove unnecessary columns
Full_file_with_decreases$Change <- NULL
# Full_file_with_decreases$Decreases <- NULL

# CHANGE THE FILE ORGANIZATION TO LONG FORMAT TO CHANGE THE DATES TO GDD
Full_temp <- Full_file_with_decreases %>%
  pivot_longer(cols = starts_with("X"), names_to = "Date", values_to = "Heights") %>%
  separate(col = "Date", into = c("Letters","Date"), sep = "X", remove = TRUE,)
Full_temp$Letters <- NULL

#change date format in file
Full_temp$Date <- strptime(as.character(Full_temp$Date), "%m%d%Y")
Full_temp$Date <- format(Full_temp$Date, "%m%d%Y")

# READ IN WEATHER TEMPERATURE DATA
weather <- read.csv(paste0(Path,Year,"/SaintPaul/",Location,"_",Year, "/Temperature_Data_", Loc, "_", Year, ".csv"))
weather$Date <- strptime(as.character(weather$Date), "%m/%d/%Y") #tell r that Date is actually a Date
weather$Date <- format(weather$Date, "%m%d%Y") #change the date format to match the format in the other file

# CALCULATE THE GDD FOR EACH DAY OF THE GROWING SEASON
weather$GDD <- 0

for (wrow in 1:nrow(weather)){
  if (((weather[wrow,2] + weather[wrow,3])/2) > 50) {
    temp.max <- fifelse(weather[wrow,2] > 86, 86, weather[wrow,2])
    temp.min <- fifelse(weather[wrow,3] < 50, 50, weather[wrow,3])
    weather[wrow,4] = (((temp.max + temp.min)/2) - 50)
  }else {
    weather[wrow,4] = 0
  }
}

# calculate cumulative gdd after the planting date
weather_dap <- weather[!(weather$Date < planting_date), ] # remove all the days prior to planting
weather_dap$cum.GDD <- weather_dap[2,4] # cumulative starts the day after planting 

#for loop to calculate cumulative GDD
for (wdaprow in 3:nrow(weather_dap)) {
  weather_dap[wdaprow,5] <- (weather_dap[(wdaprow -1),5] + weather_dap[wdaprow,4])
}

#add removed days back on
weather$cum.GDD <- 0
weather_temp <- weather[!(weather$Date >= planting_date), ]
weather_tot <- rbind(weather_temp,weather_dap)

# print weather_tot for later use
write.csv(weather_tot, paste0(Path,Year,"/SaintPaul/",Location,"_",Year,"/GDD_Accumulation_",Year,".csv"))

# CHANGE THE FLIGHT DAY TO GROWING DEGREE DAYS (THE )
Full_temp_GDD <- data.frame()
Full_temp_GDD <- merge(Full_temp, weather_tot[,c("Date", "cum.GDD")], by = "Date")
Full_GDD <- Full_temp_GDD 
##################### CAN LOOK AT DATE TO CUM.GDD IN FULL_TEMP_GDD##################

Full_GDD$Date <- NULL #get rid of date before changing back to wide format

pdf(paste0(Path,Year,"/SaintPaul/",Location,"_",Year, "/Data_Analysis/Full_field_growth_rate_", Loc, "_", Year, ".pdf"), width = 3, height = 2.5)


c <- ggplot(data = Full_GDD, aes(x = cum.GDD, y = Heights, group = Plot)) +
  geom_line() +
  #geom_point() + 
  theme_classic() +
  xlab("Cumulative Growing Degree Days") #+
  #theme(text = element_text(size=20))
print(c)

dev.off()

# CHANGE BACK TO WIDE FORMAT - PLOT THE DATA
Full_GDD$cum.GDD <- as.character(Full_GDD$cum.GDD) #change cum.GDD to character 
Full_GDD <- Full_GDD %>%
  pivot_wider(names_from = "cum.GDD", values_from = "Heights")

#Full_GDD$Stand <- NULL
#Full_GDD$Range <- NULL
#Full_GDD$Row <- NULL

# Add Range and ROw back in
Full_GDD <- merge(Full_GDD,Plots, by = "Plot")
Full_GDD <- Full_GDD[,c(1:6,(ncol(Full_GDD)-1),ncol(Full_GDD),7:(ncol(Full_GDD)-2))]

# REORGANIZE THE COLUMNS TO MATCH THE ORDER IN JONATHAN'S FILE (Maybe move much earlier)
FINAL_FILE <- Full_GDD[, c(1,4,2,5,3, (6:ncol(Full_GDD)))]
# Export Heatmap of the final file 
for (num_column in 9:ncol(FINAL_FILE)) {
  other_columns <- as.data.frame(FINAL_FILE[,c(7,8,num_column)])
  other_columns_names <- colnames(other_columns)
  pdf(paste0(Path,Year,"/SaintPaul/",Location,"_",Year, "/Data_Analysis/Heatmaps/Heatmap_Remaining_Plots_X", other_columns_names[3], "_", Loc, "_", Year, ".pdf"))
  
  j <- ggplot(other_columns, aes(x = Row, y = Range, fill = other_columns[,3])) +
    geom_tile() +
    scale_fill_gradient(low = "blue", high = "orange") + 
    theme_classic() +
    theme(legend.title = element_blank())
  
  print(j)

  dev.off()
}

# Export data to compare date and GDD
write.csv(unique(Full_temp_GDD[, c("Date", "cum.GDD")]), file = paste0(Path,Year,"/SaintPaul/",Location,"_",Year, "/Data_Analysis/Date_to_GDD_", Loc, "_", Year, ".csv"), col.names = TRUE, row.names = FALSE)
# EXPORT THE DATA FILE TO BE USED IN JONATHAN'S FILE
write.csv(FINAL_FILE, file = paste0(Path,Year,"/SaintPaul/",Location,"_",Year,"/Data_Analysis/Final_File_b4_ANOVA_", Loc,"_", Year, ".csv"), row.names = FALSE)

# make plots comparing the reps
final.file.subset <- FINAL_FILE[,c(2:ncol(FINAL_FILE))]

for (i in 8:ncol(final.file.subset)) {
  final.date.subset <- final.file.subset[,c(1,2,3,4,i)]
  final.date.subset <- as.data.frame(final.date.subset)
  final.date.subset$Date <- final.date.subset[,5]
  final.date.subset[,5] <- NULL
  
  final.date.wide <- final.date.subset %>%
  pivot_wider(names_from = Rep, names_prefix = "X", values_from = Date) 
  

  pdf(paste0(Path,Year,"/SaintPaul/",Location,"_",Year,"/Data_Analysis/Rep_2_Rep_comparison_",colnames(final.file.subset[i]),"_", Loc,"_", Year, ".pdf"))
  
  a <- ggplot(final.date.wide, aes(x = X1, y = X2, color = Block)) +  ######## WHERE THE CHANGES HAVE TO BE MADE################
    geom_point()
  print(a)
  
  dev.off()
}
```

## ANOVA

  - If this script is rerun, delete the previous outputs from this script - specifically stats

```{r}
## Script written by Jonathan Renk
## 7 June 2020

## Clearing the global environment
rm(list=ls(all=TRUE))

## Loading packages
library("car")
library("lme4")
library("dfoptim")
library("lmerTest")

################################### CHANGE THESE #################################################
Path <- "/Users/dorothykirsch/Desktop/Projects_Data/Flights/"
Field <- c("X3", "X5", "X3", "X2")
Year <- c("18", "19", "20", "21")
Loc <- "StPaul"
#############################################################################
# initiate normality file
all_norm <- data.frame()
# initiate heritability file
all_herit <- data.frame()

for (y in (1:4)) {
  ## Loading in the data
  data <- read.csv(paste0(Path, "20",Year[y],"/SaintPaul/",Field[y], "_20",Year[y],"/Data_Analysis/Final_File_b4_ANOVA_", Loc,"_20", Year[y], ".csv"), header=T, stringsAsFactors=F, row.names = NULL)

  ## Removing Plot Name
  #data <- data[,c(-1)]

  ## Setting variables as factors for genotype, block, rep, and env
  data[,1] <- as.factor(data[,1])
  data[,2] <- as.factor(data[,2])
  data[,3] <- as.factor(data[,3])
  data[,4] <- as.factor(data[,4])
  data[,5] <- as.factor(data[,5])
  data[,6] <- as.numeric(data[,6])
  data[,7] <- as.factor(data[,7])
  data[,8] <- as.factor(data[,8])

  str(data)

  traits <- colnames(data)[9:ncol(data)]

  for(trait in traits){
    pdf(paste0(Path, "20",Year[y],"/SaintPaul/",Field[y], "_20",Year[y], "/Data_Analysis/Output_from_Jonathan/Plots/plots_", trait, ".pdf"), width = 15, height = 5)
    par(mfrow=c(1,2))
  
    # Histogram of raw data
    hist(data[,trait],
        main=paste0("Histogram of normalized\n", trait, " values"),
        xlab=trait,
        col="cadetblue")
  
    # Stripchart of raw data across Blocks originally from environment
    stripchart(data[,trait] ~data$Block,
              main=paste0("Stripchart of normalized\n", trait, " values"),
              xlab="Block",
              ylab=trait,
              vertical=TRUE, 
              method="jitter",
              bg="cadetblue",
              pch=21
    )
  
  dev.off()
  
  if(paste0(Path, "20",Year[y],"/SaintPaul/",Field[y], "_20",Year[y],"/Data_Analysis/Output_from_Jonathan/Stats/stats_", trait, ".txt") %in% list.files()){
    system(paste0("rm stats_", trait, ".txt"))}
  # Summary statistics of the trait
  summary <- summary(data[,trait], )
  out <- capture.output(summary)
  cat(out, file=paste0(Path, "20",Year[y],"/SaintPaul/",Field[y], "_20",Year[y],"/Data_Analysis/Output_from_Jonathan/Stats/stats_",trait,".txt"), sep="\n", append=TRUE)
  
  # Test for normality
  normality <- shapiro.test(data[,trait])
  out <- capture.output(normality)
  cat(out, file=paste0(Path, "20",Year[y],"/SaintPaul/",Field[y], "_20",Year[y], "/Data_Analysis/Output_from_Jonathan/Stats/stats_",trait,".txt"), sep="\n", append=TRUE)
  
  # accumulate normality values
  norm.temp <- data.frame("GDD" = trait, "Normality.P" = normality$p.value, "Year" = paste0("20", Year[y]))
  all_norm <- rbind(all_norm, norm.temp)
  
  # Run an ANOVA (switched : in Env:Rep and Rep:Block for nesting)
  #model <- lm(get(trait) ~ Genotype + Env + Env/Rep + Rep/Block + Genotype:Env, data=data) #from Jonathan's 
  
  model <- lm(get(trait) ~ Genotype + Rep/Block + Genotype:Block, data=data)
  anova <- anova(model)
  out <- capture.output(anova)
  cat(out, file=paste0(Path, "20",Year[y],"/SaintPaul/",Field[y], "_20",Year[y], "/Data_Analysis/Output_from_Jonathan/Stats/stats_",trait,".txt"), sep="\n", append=TRUE)
  
  # Run a random effects model
  #model.1 <- lmer(get(trait) ~ (1|Genotype) + (1|Env) + (1|Env/Rep) + (1|Rep/Block) + (1|Genotype:Env), data = data, REML = TRUE) #from Jonathan
  model.1 <- lmer(get(trait) ~ (1|Genotype) + (1|Rep/Block) + (1|Genotype:Block), data = data, REML = TRUE)
    # Decreasing stopping tolerances
  strict_tol <- lmerControl(optCtrl=list(xtol_abs=1e-8, ftol_abs=1e-8))
  if (all(model.1@optinfo$optimizer=="nloptwrap")) {
    model <- update(model.1, control=strict_tol)
  }
  summary(model, correlation=FALSE)
  random_effects <- ranef(model)
  # Write out BLUPs for Genotypes
  write.table(random_effects$Genotype, paste0(Path, "20",Year[y],"/SaintPaul/",Field[y], "_20",Year[y], "/Data_Analysis/Output_from_Jonathan/Blups/blups_", trait, ".csv"), col.names=F, row.names=F, sep=",")
  # Summary of random effects
  summary <- summary(model, correlation=FALSE)
  out <- capture.output(summary)
  cat(out, file=paste0(Path, "20",Year[y],"/SaintPaul/",Field[y], "_20",Year[y], "/Data_Analysis/Output_from_Jonathan/Stats/stats_",trait,".txt"), sep="\n", append=TRUE)
  # Write out residuals from ANOVA
   df <- data.frame("plot" = as.character(data[which(!is.na(data[,trait])), "Plot"]),
                   "resid" = resid(model.1))
  
  write.table(df, paste0(Path, "20",Year[y],"/SaintPaul/",Field[y], "_20",Year[y], "/Data_Analysis/Output_from_Jonathan/Resids/resids_", trait, ".csv"), col.names=F, row.names=F, sep=",")
  # Calculate hertiability 
  model_variances <- as.data.frame(VarCorr(model))
  #h2 <- model_variances$vcov[2]/(model_variances$vcov[2]+(model_variances$vcov[1]/5)+(model_variances$vcov[8]/10)) #from Jonathan
  #  For that section [2] is genotype, [1] is environment, and [8] is residual. 5 in my case was the number of environments and 10 was the number of environments (5) multiplied by the number of reps (2). So for your case you would need to change the code to have genotype/(genotype+residual/(number of reps)) and the numbers would correspond to your model if that makes sense?
  h2 <- model_variances$vcov[2]/(model_variances$vcov[2]+(model_variances$vcov[5]/2))
  out <- capture.output(h2)
  cat(out, file=paste0(Path, "20",Year[y],"/SaintPaul/",Field[y], "_20",Year[y], "/Data_Analysis/Output_from_Jonathan/Stats/stats_",trait,".txt"), sep="\n", append=TRUE)
  
  pdf(paste0(Path, "20",Year[y],"/SaintPaul/",Field[y], "_20",Year[y], "/Data_Analysis/Output_from_Jonathan/Assumptions/assumptions_", trait, ".pdf"), width = 15, height = 5)
  par(mfrow=c(1,3))
  
  # accumulate heritability
  herit.temp <- data.frame("GDD" = trait, "Heritability" = h2, "Year" = paste0("20", Year[y]))
  all_herit <- rbind(all_herit, herit.temp)
  
  # Model Fit with REML
  plot(fitted(model), residuals(model), pch=19, col="dark blue", ylab="Residuals", xlab="Predicted")
  abline(h=0,col="red", lwd=1, lty=1)
  # histogram of residuals
  hist(residuals(model),main="Histogram of residuals",freq=F, xlab="Residuals", ylab= "Freq", col="palegreen", col.main="darkblue")
  x=seq(-5e-15,9e-15,5e-15)
  curve(dnorm(x,mean(residuals(model)),sd(residuals(model))),add=T,lwd=2, col="red", lty=1)
  # qq plot
  qqPlot(residuals(model), pch=19, col="dark blue", col.lines="red", xlab="Pred quantiles", ylab="Obs quantiles") 
  
  dev.off()
  
  }
}


# Plot All Shapiro-Wilk P-values - HOW TO FIX?
pdf(paste0(Path, "All/ShapiroWilk_PValue_AllYears.pdf"), height = 4, width = 6.5)

plot <- all_norm %>%
  separate(GDD, c("X", "newGDD"), sep = "X") %>%
  ggplot(aes(x = as.numeric(newGDD), y = log10(Normality.P), color = Year)) +
  geom_point() +
  theme_classic() +
  xlab("Growing Degree Days") +
  ylab("log10(P Value)") +
  ggtitle("Shapiro-Wilk Normality Test") +
  theme(
    axis.text.x = element_text(angle = 90)
  )
print(plot)
dev.off()

# Plot all Heritability
pdf(paste0(Path, "All/Heritability_Flights_AllYears.pdf"), height = 4, width = 6.5)

plot <- all_herit %>%
  separate(GDD, c("X","newGDD"),sep = "X") %>%
  ggplot(aes(x = as.numeric(newGDD), y = Heritability)) +
  geom_point(aes(color = Year)) +
  geom_line(aes(color = Year)) +
  geom_vline(xintercept = c(500:1000), alpha = 0.1) +
  geom_text(aes(750,0, label = "Highest Growth \n Rate", vjust = -1)) +
  theme_classic() +
  xlab("Growing Degree Days") +
  ylab("Heritability") +
  ggtitle("Heritability for each Flight") +
  theme(
    axis.text.x = element_text(angle = 90)
  )
print(plot)

dev.off()
```

## Heatmap of Residuals

  - plot the residuals in heatmap from the ANOVA analysis (previous script) and use to determine if flights are erroneous
  - needs to be rerun for Year and Location
  - needs to change GrowingDegreeDays_list for each year
  
```{r}
## Clearing the global environment
rm(list=ls(all=TRUE))

library(ggplot2)
library(RColorBrewer)
library(tidyr)
library(stringr)
library(plotly)
############################### CHANGE THESE ################################
Path <- "/Users/dorothykirsch/Desktop/Projects_Data/Flights/"
Year <- "2021" # options 2018, 2019, 2020, 2021
Loc <- "StPaul" 
Location <- "X2" # options - X3, X5, X3, X2
GrowingDegreeDays_list <- c("X9","X258","X413.5","X619","X765.5","X874","X1096","X1190","X1251","X1421.5","X1759.5") 
#2018:c("X130", "X430", "X585", "X946", "X1267", "X1356", "X1495", "X1662.5", "X1812", "X2031.5", "X2248.5") 2019: c("X50.5","X121.5","X172","X239","X302","X428.5","X491","X583","X722","X769.5","X890.5","X942","X1055","X1115","X1230.5","X1418","X1650.5") 2020: c("X60.5","X278","X375.5","X512.5","X654.5","X835","X1340","X1492.5") 2021: c("X9","X258","X413.5","X619","X765.5","X874","X1096","X1190","X1251","X1421.5","X1759.5") 
#############################################################################
# read in the complete plot data file
Other_plot_data <- read.csv(paste0(Path,Year,"/SaintPaul/",Location,"_",Year,"/Data_Analysis/", "Final_File_b4_ANOVA_", Loc, "_", Year, ".csv"))

# For loop to read in all of the appropriate and necessary files and create a heatmap.
for (time in GrowingDegreeDays_list) {
  # filter out all of the NAs for the day so it will match the residuals files
  plot_data <- Other_plot_data %>%
    filter(!is.na(Other_plot_data[,time])) 
  # pull out just the plot, range, row, and data from the date of interest
  means_data <- plot_data[,c("Plot","Range","Row",time)]
  # read in the residuals
  temp_resid_file <- read.csv(paste0(Path,Year,"/SaintPaul/",Location,"_",Year,"/Data_Analysis/","Output_from_Jonathan/Resids/resids_", time, ".csv"), header = FALSE)
  # create a file with the plots and the residuals
  temp_resid_file$Plot <- temp_resid_file$V1
  temp_resid_file$V1 <- NULL
  resid_file <- merge(plot_data[,c(1,6,7,8)], temp_resid_file, by = "Plot")

  # Heatmap of the residuals for each day
  pdf(paste0(Path,Year,"/SaintPaul/",Location,"_",Year,"/Data_Analysis/", "Output_from_Jonathan/Resids/Residuals_heatmap_", time, "_", Loc, "_", Year, ".pdf"))
    
   a =  ggplot(resid_file, aes(x = Row, y = Range, fill = abs(resid_file$V2))) +
      geom_tile() +
      scale_fill_gradient(low = "blue", high = "orange") + 
      theme_classic() +
      labs(title=paste0(Loc, "_20", Year,"_", time)) +
      theme(legend.title = element_blank())
   print(a)
  
    dev.off()  
}
```

## Fit Model to Growth Curves

  - has to be run for each year and Field 
  - uses k-means cross validation to determine the best span (uses best span for each year independently)
  - fits Loess models to growth curves with 50 GDD intervals

```{r}
# Fitting Loess Models to growth Curves
# Dorothy Sweet
# 03/05/2021

## Clearing the global environment
rm(list=ls(all=TRUE))

library(caret)
library(mlbench)
library(tidyverse)

##################################### CHange before running ###########################
## ADJUST THE RANGE FOR LOESS CURVE DEVELOPMENT FOR THE YEAR
Path <- "/Users/dorothykirsch/Desktop/Projects_Data/Flights/"
Year <- c("2018")# 2018, "2019", "2020", "2021")
Loc <- c("SaintPaul")#, "SaintPaul", "SaintPaul", "SaintPaul")
Field <- c("X3")#"X3" "X5", "X3", "X2")
Loc_Name <- c("StPaul")#, "StPaul", "StPaul", "StPaul")
#########################################################################################
for (x in 1:length(Year)) {

# Read in the data
Model_data <-as.data.frame(read.csv(paste0(Path, Year[x],"/", Loc[x], "/", Field[x], "_", Year[x], "/Data_Analysis/Final_File_b4_ANOVA_", Loc_Name[x], "_", Year[x], ".csv")))

Model_data$FINAL_YIELD <- NULL
### Determine the ideal span with a k-means cross-validation
Model_long <- Model_data %>%
  pivot_longer(cols = c(9:ncol(Model_data)), names_to = "XDate", values_to = "Height")
Model_long <- separate(Model_long, col = XDate, c(NA, "Date"), sep = "X")

Model_long$Date <- as.numeric(Model_long$Date)

span.seq <- seq(from = 0.15, to = 0.95, by = 0.05) #explores range of spans
k <- 100 #number of folds
set.seed(1) # replicate results
folds <- sample(x = 1:k, size = nrow(Model_long), replace = TRUE)
cv.error.mtrx <- matrix(rep(x = NA, times = k * length(span.seq)), 
                        nrow = length(span.seq), ncol = k)

for(i in 1:length(span.seq)) {
  for(j in 1:k) {
    loess.fit <- loess(formula = Height ~ Date, data = Model_long[folds != j, ], span = span.seq[i])
    preds <- predict(object = loess.fit, newdata = Model_long[folds == j, ])
    cv.error.mtrx[i, j] <- mean((Model_long$Height[folds == j] - preds)^2, na.rm = TRUE)
    # some predictions result in `NA` because of the `x` ranges in each fold
 }
}

# mean error from each fold
cv.errors <- rowMeans(cv.error.mtrx)

# determine which span was the best span
best.span.i <- which.min(cv.errors)
best.span.i
span.seq[best.span.i]

# plot the span errors and make the lowest error red
pdf(paste0(Path, Year[x], "/", Loc[x], "/", Field[x], "_", Year[x], "/Data_Analysis/All_Spans_Plot_", Loc_Name[x], "_", Year[x], ".pdf"), height = 3, width = 4)

  plot(x = span.seq, y = cv.errors, xlab = "Spans", ylab = "Mean Cross Validation Error",type = "l", main = "CV Plot")
  points(x = span.seq, y = cv.errors, pch = 20, cex = 2, col = "blue")
  points(x = span.seq[best.span.i], y = cv.errors[best.span.i], pch = 20, cex = 3, col = "red")

dev.off()
# run a loess curve for the whole dataset 
best.loess.fit <- loess(formula = Height ~ Date, data = Model_long,
                        span = span.seq[best.span.i])

x.seq <- seq(from = min(as.numeric(Model_long$Date)), to = max(as.numeric(Model_long$Date)), length = 100)

# plot the loess curve 
(pdf(paste0(Path, Year[x], "/", Loc[x], "/", Field[x], "_", Year[x], "/Data_Analysis/Best_Span_Plot_", Loc_Name[x], "_", Year[x], ".pdf"), height = 3, width = 4))

  plot(x = Model_long$Date, y = Model_long$Height, xlab = "Date (GDDs)", ylab = "Normalized Height", main = "Best Span Plot")
  lines(x = Model_long$Date, y = Model_long$Height)
  lines(x = x.seq, y = predict(object = best.loess.fit, newdata = data.frame(Date = x.seq)), col = "red", lwd = 2)

dev.off()

# make a list for too many NAs
Too_many_NA <- list()

for (Plot in 1:nrow(Model_data)) {
  # break out individual plots
  Individual <- Model_data[Plot,]
  Indiv_long <- Individual %>%
    pivot_longer(cols = c(9:ncol(Individual)), names_to = "XDate", values_to = "Height") #Put into long format
  Indiv_long <- separate(Indiv_long, col = XDate, c(NA, "Date"), sep = "X") #Get rid of the X
  
  # Count the NAs in Height
  NA_count <- sapply(Indiv_long, function(x) sum(is.na(x)))
  
  if (NA_count[10] > ((ncol(Model_data)-8)/4)) {
    Too_many_NA <- append(Too_many_NA, as.matrix(Indiv_long[1,1]))
  } else {
  
  #identify individual plot name
  Plot_Name <- as.matrix(Model_data[Plot,1])
  ################################# WHERE ADJUSTMENTS NEED TO BE MADE ###########
  # Run the model 
  Plot.lo <- loess(Height ~ Date, data = Indiv_long, model = T, span = 0.25)
  Predictions <- data.frame(seq(50, 2200, by = 50))
  Predictions$Height <- predict(Plot.lo, data.frame(Date = seq(50,2200, by = 50)))
 
  # Rename and reorganize file
  Predictions$GDD <- Predictions[,1]
  Predictions[,1] <- NULL
  Predictions <- Predictions[, c(2,1)]
  
  ###pdf(file = paste0(Path, Year, "/", Loc, "/", Field, "_", Year, "/Data_Analysis/Loess_Predictions_Plots_", Loc_Name, "_", Year, ".pdf"))
  
  ggplot(Predictions, aes(x = GDD, y = Height)) +
    geom_point() +
    geom_line()
  
  # Rename Height Column to the name of the plot
  colnames(Predictions)[which(colnames(Predictions)=="Height")] <- Plot_Name
  
  # Make a file with Plot predictions in order to keep row names and not export the GDD repeatedly
  Plot_Predictions <- Predictions
  Plot_Predictions$GDD <- NULL
  Plot_Predictions <- (t(Plot_Predictions))
  
  # Export Predictions
  if (Plot == 1) {
    write.table(t(Predictions), file = paste0(Path, Year[x],"/", Loc[x], "/", Field[x], "_", Year[x], "/Data_Analysis/Loess_Predictions_", Loc_Name[x], "_", Year[x], ".txt"), col.names = F, sep = "\t")
  } else {
    write.table(Plot_Predictions, file = paste0(Path, Year[x],"/", Loc[x], "/", Field[x], "_", Year[x], "/Data_Analysis/Loess_Predictions_", Loc_Name[x], "_", Year[x], ".txt"), append = T, row.names = T, col.names = F, sep = "\t")
  }
  }
  
}

Too_many_NA <- data.frame("col" = unlist(Too_many_NA))
write.table(Too_many_NA, file = paste0(Path, Year[x], "/", Loc[x], "/", Field[x], "_", Year[x], "/Data_Analysis/Loess_Predictions_MissingPlots_", Loc_Name[x], "_", Year[x], ".txt"), sep = "\n", col.names = "GDD", row.names = F )

}
```

## Loess Regression Window Analysis

  - Prints total Loess curves for each date before cutting down
  - Cuts Loess predictions down to window of interest (50-1450)
  - combines Loess curve data from all years of interest
  - Prints All_loess_curves_plot.pdf &
    All_environment_GDD_Loess_Predictions.txt &
    All_environment_Interval_Loess_Predictions.txt"

```{r}
## Clearing the global environment
rm(list=ls(all=TRUE))

library(caret)
library(mlbench)
library(tidyverse)
library(gtools)
library(car)
library(viridis)
library(MASS)

#####################################
# Set Path and Read in Information #
####################################
Path <- "/Users/dorothykirsch/Desktop/Projects_Data/Flights/"
Year <- c("2018", "2019", "2020", "2021")
Loc <- c("SaintPaul", "SaintPaul", "SaintPaul", "SaintPaul")
Field <- c("X3", "X5", "X3", "X2")
Loc_Name <- c("StPaul", "StPaul", "StPaul", "StPaul")
Prediction_list <- list()

# initialize data frames
All_env_data <- data.frame()
Final_Height_data <- data.frame()
Final_Genotypes_data <- data.frame()

####################################
# Read in Data and Make Some Files #
###################################
for (x in 1:length(Year)) {
  Date_year <- Year[x]
  
  # read in the files
  Loess_Predictions <- read.delim(paste0(Path, "/", Year[x], "/", Loc[x], "/", Field[x], "_", Year[x], "/Data_Analysis/Loess_Predictions_", Loc_Name[x], "_", Year[x], ".txt"), header = T)
  Loess_Missed <- read.delim(paste0(Path, "/", Year[x], "/", Loc[x], "/", Field[x], "_", Year[x], "/Data_Analysis/Loess_Predictions_MissingPlots_", Loc_Name[x], "_", Year[x], ".txt"), header = T)
  
  # bind the missing plots to the prediction plots
  Loess_Missed_NAs <- smartbind(Loess_Predictions, Loess_Missed)
  
  # rename and reorganize the file to prepare to add genotype data back 
  Loess_Missed_NAs$Plot <- Loess_Missed_NAs$GDD
  Loess_Missed_NAs$GDD <- NULL
  Loess_Missed_NAs <- Loess_Missed_NAs[,c(ncol(Loess_Missed_NAs),1:(ncol(Loess_Missed_NAs)-1))]
  
  # read in the genotype data
  Genotypes <- read.csv(paste0(Path, "/", Year[x], "/", Loc[x], "/", Field[x], "_", Year[x], "/Data_Analysis/Final_File_b4_ANOVA_", Loc_Name[x], "_", Year[x], ".csv"))
  
  # Merge the Genotype information with the Loess regressions and name the file based on the year
  temp_Geno_Pred <- merge(Genotypes[1:8], Loess_Missed_NAs, by = "Plot")
  
  # add a column for year
  temp_Geno_Pred$Year <- Year[x]
  
  # reorganize file
  temp_Geno_Pred <- temp_Geno_Pred[,c(1:8, ncol(temp_Geno_Pred), 9:(ncol(temp_Geno_Pred)-1))]
  
  # long form for loess curves
  temp_long <- temp_Geno_Pred %>%
  pivot_longer(cols = (10:ncol(temp_Geno_Pred)), names_to = "GDD", values_to = "Height") %>%
  separate(col = GDD, c(NA, "GDD"), sep = "X") %>%
  na.omit()
  
  pdf(paste0(Path, "/", Year[x], "/", Loc[x], "/", Field[x], "_", Year[x], "/Data_Analysis/Loess_curves_plot.pdf")) 
  
  temp_plot <- temp_long %>%
  ggplot(aes(x = as.numeric(GDD), y = Height, group = Plot, na.rm = T)) +
    geom_point(na.rm = T) +
    geom_line(na.rm = T) +
    geom_vline(xintercept = 1450, color = "red") +
    facet_wrap(~Year +Block+ Rep, scales = "free")
  
  print(temp_plot)
  
  dev.off()
  
  All_env_data <- rbind(All_env_data, temp_Geno_Pred)
  
  Genotypes$Final <- Genotypes[,ncol(Genotypes)]
  
  Final_Height_data <- rbind(Final_Height_data, Genotypes[,c(1,ncol(Genotypes))])
  
  Final_Genotypes_data <- rbind(Final_Genotypes_data, Genotypes[,(1:4)])

}

write.table(Final_Height_data, paste0(Path, "All/Final_Height_Data_Plot.txt"), sep = "\t", row.names = F)

# Filter All_env_data down to only the genotype info and the window we care about (150-1450)
All_env_data <- All_env_data[,c(1:9,12:38)]

#######################################################
# Deal with All Environments and Loess Predicted GDDs #
######################################################
#change the variables to characters
All_env_data[,1] <- as.character(All_env_data[,1])
All_env_data[,2] <- as.character(All_env_data[,2])
All_env_data[,3] <- as.character(All_env_data[,3])
All_env_data[,4] <- as.character(All_env_data[,4])
All_env_data[,5] <- as.character(All_env_data[,5])
All_env_data[,6] <- as.character(All_env_data[,6])
All_env_data[,7] <- as.character(All_env_data[,7])
All_env_data[,8] <- as.character(All_env_data[,8])
All_env_data[,9] <- as.character(All_env_data[,9])

# Make a long version of all environment data
All_long <- All_env_data %>%
  pivot_longer(cols = (10:ncol(All_env_data)), names_to = "GDD", values_to = "Height") %>%
  separate(col = GDD, c(NA, "GDD"), sep = "X") %>%
  na.omit()

###### Plot the Loess Predictions by year, block and rep. Ask Candy and Cory about 2020
pdf(paste0(Path, "/All/All_loess_curves_plot.pdf"), height = 10, width = 15)  

All_loess <- ggplot(All_long, aes(x = as.numeric(GDD), y = Height, group = Plot, na.rm = T)) +
  geom_point(na.rm = T) +
  geom_line(na.rm = T) +
  facet_wrap(~Year +Block+ Rep, scales = "free")

print(All_loess)

dev.off()

traits <- colnames(All_env_data)[10:ncol(All_env_data)]

# Export All_env_data so it can be used later
write.table(All_env_data, file = paste0(Path, "All/All_environment_GDD_Loess_Predictions.txt"), sep = "\t", row.names = T)

######################################################
# Use Loess Predicted GDD to Determine Growth Curves #
######################################################
# Make a matrix of the slopes between points
slope.matrix <- matrix(data = NA, nrow = nrow(All_env_data), ncol = (ncol(All_env_data)-10))
colnames(slope.matrix) <- paste0("X", seq(150,1400, by = 50), "_X", seq(200, 1450, by = 50))

# Make a list of X values to determine slope
columnNameList <- as.data.frame(colnames(All_env_data))
columnNameList_dates <- separate(columnNameList, col = `colnames(All_env_data)`, c(NA, "Date"), sep = "X")

for (i in 1:nrow(All_env_data)) {
  for (j in 10:(ncol(All_env_data)-1)) {
    if (is.na(All_env_data[i,(j+1)])){
      next()
    } else if (is.na(All_env_data[i,j])) {
      next()
    }
    slope.matrix[i,(j-9)] <- (All_env_data[i,(j+1)] - All_env_data[i,j])/(as.numeric(columnNameList_dates[(j+1),1]) - as.numeric(columnNameList_dates[j,1]))
  }
}

slope.matrix <- cbind(All_env_data[,c(1:9)],slope.matrix)

# Export slope.matrix so it can be used later
write.table(slope.matrix, file = paste0(Path, "All/All_environment_Interval_Loess_Predictions.txt"), sep = "\t", row.names = T)
```

## Percent Variance Explained

  - completes ANOVA for each GDD and slope between GDDs (interval) of Loess curves and collects percent variance explained
  - calculates heritability for each GDD and slope between GDDs (intervals) of Loess curves
  - plots percent variance explained and heritability over time

```{r}
## Clearing the global environment
rm(list=ls(all=TRUE))

library(caret)
library(mlbench)
library(tidyverse)
library(gtools)
library(car)
library(viridis)
library(MASS)
library("car")
library("lme4")
library("dfoptim")
library("lmerTest")

#####################################
# Set Path and Read in Information #
####################################
Path <- "/Users/dorothykirsch/Desktop/Projects_Data/Flights/"
Year <- c("2018", "2019", "2020", "2021")
Loc <- c("SaintPaul", "SaintPaul", "SaintPaul", "SaintPaul")
Field <- c("X3", "X5", "X3", "X2")
Loc_Name <- c("StPaul", "StPaul", "StPaul", "StPaul")
Prediction_list <- list()

####################
# Read in the Data #
###################
All_env_data <- read.delim(paste0(Path, "All/All_environment_GDD_Loess_Predictions.txt")) %>%
  mutate(Year = factor(Year),
           Rep = factor(Rep),
           Block = factor(Block)) 
slope.matrix <- read.delim(paste0(Path, "All/All_environment_Interval_Loess_Predictions.txt")) %>%
  mutate(Year = factor(Year),
           Rep = factor(Rep),
           Block = factor(Block)) 

############################################################################
# Run an ANOVA for each GDD and Create Master file with Variance Explained # 
############################################################################
# MAKE A LARGE FOR LOOP FOR ALL YEARS TOGETHER AND THEN EACH YEAR INDIVIDUALLY

#### ANOVA OF EACH GDD
df.out <- data.frame()
herit.out <- data.frame()

traits <- colnames(All_env_data)[10:ncol(All_env_data)]
for (trait in traits) {
    # make a model
model <- tryCatch(lm(get(trait) ~ Genotype + Year + Year/Rep + Year/Rep/Block + Genotype:Year, data=All_env_data)) #, error = function(x) NA)
#if(is.na(model)){ print(paste0("skipping ", trait)); next }
  
anova.model <- anova(model) #,type = 2))
  
    # run a model
anova.model.temp <- broom::tidy(anova(model)) %>% #,type = 2)) %>%
  mutate(total_ss = sum(sumsq),
          `R^2` = sumsq/total_ss)
  
df.temp <- data.frame("GDD" = trait, "Genotype" = anova.model.temp$`R^2`[1], "Year" = anova.model.temp$`R^2`[2], "Year.Rep" = anova.model.temp$`R^2`[3], "Genotype.Year" = anova.model.temp$`R^2`[4], "Year.Rep.Block" = anova.model.temp$`R^2`[5], "Residuals" = anova.model.temp$`R^2`[6])
  
# make a data frame with all data from the anovas
df.out <- rbind(df.out, df.temp)
# export anova data 
out <- capture.output(anova.model)
cat(out, file = paste0(Path, "/All/Anova/anova_",trait,".txt"), sep = "\n", append = F)
# random effects model
  model.1 <- lmer(get(trait) ~ (1|Genotype) + (1|Year) + (1:Year/Rep) + (1:Year/Rep/Block) + (1|Genotype:Year), data = All_env_data, REML = TRUE)
  # Decreasing stopping tolerances
strict_tol <- lmerControl(optCtrl=list(xtol_abs=1e-8, ftol_abs=1e-8))
if (all(model.1@optinfo$optimizer=="nloptwrap")) {
  model <- update(model.1, control=strict_tol)
}
  
  # Calculate hertiability 
model_variances <- as.data.frame(VarCorr(model))
  #h2 <- model_variances$vcov[2]/(model_variances$vcov[2]+(model_variances$vcov[1]/5)+(model_variances$vcov[8]/10)) #from Jonathan
  #  For that section [2] is genotype, [1] is environment, and [8] is residual. 5 in my case was the number of environments and 10 was the number of environments (5) multiplied by the number of reps (2). So for your case you would need to change the code to have genotype/(genotype+residual/(number of reps)) and the numbers would correspond to your model if that makes sense?
h2 <- model_variances$vcov[2]/(model_variances$vcov[2]+(model_variances$vcov[3]/4) + (model_variances$vcov[4]/8))
out <- capture.output(h2)
cat(out, file=paste0(Path,"/All/Heritability/heritability",trait,".txt"), sep="\n", append=TRUE)
  
herit.temp <- data.frame("GDD" = trait, "Heritability" = h2)
# make a data frame with all the heritabilities
herit.out <- rbind(herit.out,herit.temp)
}

# export variance explained by each GDD
write.table(df.out, paste0(Path, "All/GDD_Variance_Explained_file.txt"), sep = "\t", row.names = F)
# re-format file for plotting
df.out.1 <- df.out %>%
  pivot_longer(cols = 2:ncol(df.out), names_to = "variables", values_to = "R^2") %>%
  separate(GDD, into = c(NA,"GDD"),sep = "X")
# export heritability for each GDD
write.table(herit.out, paste0(Path,"All/GDD_Heritability_file.txt"), sep = "\t", row.names = F)
# re-format file for plotting
herit.out.1 <- herit.out %>%
  separate(GDD, into = c(NA,"GDD"), sep = "X")

# Plot the variance explained by each variable
pdf(paste0(Path, "All/GDD_Variation_Explained.pdf"), height = 2.6, width = 5)

plot <- ggplot() +
  geom_bar(data = df.out.1, aes(x = as.numeric(GDD), y =  `R^2`, fill = factor(variables, levels = c("Residuals", "Year.Rep.Block", "Year.Rep", "Year", "Genotype.Year", "Genotype"))), position = "stack", stat = "identity") +
  geom_point(data = herit.out.1, aes(x = as.numeric(GDD), y = Heritability), color = "black", fill = "red", size = 1, shape = 21) +
  scale_fill_viridis(discrete = T,name = "Variables", labels = c("Residuals", "Year/Rep/Block", "Year/Rep", "Year","Genotype/Year", "Genotype")) +
  theme_light() +
  theme(axis.text.x = element_text(angle = 90),
        legend.position = "none") +
  xlab("Growing Degree Days") +
  ylab("Variation explained") +
  scale_x_continuous(name = "Growing Degree Days", breaks = seq(from = 150, to = 1450, by = 50))

print(plot)

dev.off()

# Plot the heritability over time
# pdf(paste0(Path, "All/GDD_Heritability_Over_Time.pdf"))
# 
# plot <- herit.out %>%
#   ggplot(aes(x = GDD, y = Heritability)) +
#   geom_point() +
#   xlab("Growing Degree Days") +
#   ylab("Heritability") +
#   theme_classic() +
#   theme(axis.text.x = element_text(angle = 90))
# print(plot)
# dev.off()

#####################################################################################
# Run an ANOVA for each GDD Interval and Create Master file with Variance Explained #
#####################################################################################
#### ANOVA OF EACH GDD interval
intervals <- colnames(slope.matrix)[10:ncol(slope.matrix)]
df.slope.out <- data.frame()
herit.slope.out <- data.frame()
for (interval.h in intervals) {
    # make a model
  model <- tryCatch(lm(get(interval.h) ~ Genotype + Year + Year/Rep + Year/Rep/Block + Genotype:Year, data=slope.matrix)) #, error = function(x) NA)
  #if(is.na(model)){ print(paste0("skipping ", interval.h)); next }
  
    # run a model
  anova.model <- anova(model) #, type = 2)
  
  anova.model.temp <- broom::tidy(anova(model)) %>% #,type = 2)) %>%
    mutate(total_ss = sum(sumsq),
           `R^2` = sumsq/total_ss)
  
  df.temp <- data.frame("Interval" = interval.h, "Genotype" = anova.model.temp$`R^2`[1], "Year" = anova.model.temp$`R^2`[2], "Year.Rep" = anova.model.temp$`R^2`[3], "Genotype.Year" = anova.model.temp$`R^2`[4], "Year.Rep.Block" = anova.model.temp$`R^2`[5], "Residuals" = anova.model.temp$`R^2`[6])
  
  # make a data frame with all data from the anovas
  df.slope.out <- rbind(df.slope.out, df.temp)
  # export anova data 
  out <- capture.output(anova.model)
  cat(out, file = paste0(Path, "/All/Anova/anova_interval_",interval.h,".txt"), sep = "\n", append = F)
   # random effects model
   model.1 <- lmer(get(interval.h) ~ (1|Genotype) + (1|Year) + (1:Year/Rep) + (1:Year/Rep/Block) + (1|Genotype:Year), data = slope.matrix, REML = TRUE)
    # Decreasing stopping tolerances
  strict_tol <- lmerControl(optCtrl=list(xtol_abs=1e-8, ftol_abs=1e-8))
  if (all(model.1@optinfo$optimizer=="nloptwrap")) {
    model <- update(model.1, control=strict_tol)
  }
  
   # Calculate hertiability 
  model_variances <- as.data.frame(VarCorr(model))
  #h2 <- model_variances$vcov[2]/(model_variances$vcov[2]+(model_variances$vcov[1]/5)+(model_variances$vcov[8]/10)) #from Jonathan
  #  For that section [2] is genotype, [1] is environment, and [8] is residual. 5 in my case was the number of environments and 10 was the number of environments (5) multiplied by the number of reps (2). So for your case you would need to change the code to have genotype/(genotype+residual/(number of reps)) and the numbers would correspond to your model if that makes sense?
  h2 <- model_variances$vcov[2]/(model_variances$vcov[2]+(model_variances$vcov[3]/4) + (model_variances$vcov[4]/8))
  out <- capture.output(h2)
  cat(out, file=paste0(Path,"/All/Heritability/heritability",interval.h,".txt"), sep="\n", append=TRUE)
  
  herit.temp <- data.frame("Interval" = interval.h, "Heritability" = h2)
  # make a data frame with all the heritabilities
  herit.slope.out <- rbind(herit.slope.out,herit.temp)
}

# make a factor level for plot
level.order <- c(paste0(seq(150,1400,50)))

# Export variance explained by each GDD interval
write.table(df.slope.out, paste0(Path, "All/GDD_Interval_Variance_Explained_file.txt"), sep = "\t", row.names = F)
# re-format file for plotting
df.slope.out.1 <- df.slope.out %>%
  pivot_longer(cols = 2:ncol(df.slope.out), names_to = "variables", values_to = "R^2") %>%
  mutate(Interval = str_replace_all(Interval,"X","")) %>%
  separate(col = Interval, into = c("Interval", NA), sep = "_")

# export heritability by each GDD interval
write.table(herit.slope.out, paste0(Path, "All/GDD_Interval_Heritability_file.txt"), sep = "\t", row.names = F)
# re-format file for plotting
herit.slope.out.1 <- herit.slope.out %>%
  mutate(Interval = str_replace_all(Interval,"X","")) %>%
  separate(col = Interval, into = c("Interval", NA), sep = "_")

# Plot the variance explained by each variable
pdf(paste0(Path, "All/GDD_Interval_Variation_Explained.pdf"), height = 2.6, width = 5)

plot <- ggplot() +
  geom_bar(data = df.slope.out.1, aes(x = factor(Interval, level = level.order), y =  `R^2`, fill = factor(variables, levels = c("Residuals", "Year.Rep.Block", "Year.Rep", "Year", "Genotype.Year", "Genotype"))), position = "stack", stat = "identity") +
  geom_point(data = herit.slope.out.1, aes(x = factor(Interval, level = level.order), y = Heritability), color = "black", fill = "red", size = 1, shape = 21) +
  scale_fill_viridis(discrete = T,name = "Variables", labels = c("Residuals", "Year/Rep/Block", "Year/Rep", "Year","Genotype/Year", "Genotype")) +
  theme_light() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5),
        legend.position = "none",
        legend.title = element_blank()) +
  xlab("Growing Degree Days Slopes") +
  ylab("Variation explained") 
  

print(plot)

dev.off()

# PLOT THE HERITABILITY OVER TIME
# Plot the heritability over time
# pdf(paste0(Path, "All/GDD_Interval_Heritability_Over_Time.pdf"))
# 
# plot <- herit.slope.out %>%
#   ggplot(aes(x = Interval, y = as.numeric(as.character(Heritability)))) +
#   geom_point() +
#   xlab("Growing Degree Day Intervals") +
#   ylab("Heritability") +
#   theme_classic() +
#   theme(axis.text.x = element_text(angle = 90))
#   
# print(plot)
# dev.off()


```
## Percent Variance Explained for Canopy

  - completes ANOVA for each GDD and of canopy Loess curves and collects percent variance explained
  - calculates heritability for each GDD of canopy Loess curves
  - plots percent variance explained and heritability over time

```{r}
## Clearing the global environment
rm(list=ls(all=TRUE))

library(caret)
library(mlbench)
library(tidyverse)
library(gtools)
library(car)
library(viridis)
library(MASS)
library("car")
library("lme4")
library("dfoptim")
library("lmerTest")

#####################################
# Set Path and Read in Information #
####################################
Path <- "/Users/dorothykirsch/Desktop/Projects_Data/Flights/"
####################
# Read in the Data #
###################
All_env_data <- read.csv(paste0(Path, "All/All_Year_Canopy_Loess_Data.csv"))
# reorder data columns
All_env_data <- All_env_data[,c(1,30,31,32,33,34,2:29)]
############################################################################
# Run an ANOVA for each GDD and Create Master file with Variance Explained # 
############################################################################
#### ANOVA OF EACH GDD
df.out <- data.frame()
herit.out <- data.frame()

traits <- colnames(All_env_data)[7:ncol(All_env_data)]
for (trait in traits) {
  
  data.temp <- All_env_data %>%
    dplyr::select("Genotype", "Year", "Rep","Block",trait) %>%
    mutate(Year = factor(Year),
           Rep = factor(Rep),
           Block = factor(Block)) %>%
    na.omit()
  
    # make a model
model <- tryCatch(lm(get(trait) ~ Genotype + Year + Year/Rep + Year/Rep/Block + Genotype:Year, data= data.temp)) #, error = function(x) NA)
#if(is.na(model)){ print(paste0("skipping ", trait)); next }
  
anova.model <- anova(model)
#anova.model <- Anova(model,type = 2)
  
    # run a model
anova.model.temp <- broom::tidy(anova(model)) %>%  # Anova(model,type = 2)) %>%
  mutate(total_ss = sum(sumsq),
          `R^2` = sumsq/total_ss)
  
df.temp <- data.frame("GDD" = trait, "Genotype" = anova.model.temp$`R^2`[1], "Year" = anova.model.temp$`R^2`[2], "Year.Rep" = anova.model.temp$`R^2`[3], "Genotype.Year" = anova.model.temp$`R^2`[4], "Year.Rep.Block" = anova.model.temp$`R^2`[5], "Residuals" = anova.model.temp$`R^2`[6])
  
# make a data frame with all data from the anovas
df.out <- rbind(df.out, df.temp)
# export anova data 
out <- capture.output(anova.model)
cat(out, file = paste0(Path, "/All/Anova/anova_canopy_",trait,".txt"), sep = "\n", append = F)
# random effects model
  model.1 <- lmer(get(trait) ~ (1|Genotype) + (1|Year) + (1:Year/Rep) + (1:Year/Rep/Block) + (1|Genotype:Year), data = data.temp, REML = TRUE)
  # Decreasing stopping tolerances
strict_tol <- lmerControl(optCtrl=list(xtol_abs=1e-8, ftol_abs=1e-8))
if (all(model.1@optinfo$optimizer=="nloptwrap")) {
  model <- update(model.1, control=strict_tol)
}
  
  # Calculate hertiability 
model_variances <- as.data.frame(VarCorr(model))
  #h2 <- model_variances$vcov[2]/(model_variances$vcov[2]+(model_variances$vcov[1]/5)+(model_variances$vcov[8]/10)) #from Jonathan
  #  For that section [2] is genotype, [1] is environment, and [8] is residual. 5 in my case was the number of environments and 10 was the number of environments (5) multiplied by the number of reps (2). So for your case you would need to change the code to have genotype/(genotype+residual/(number of reps)) and the numbers would correspond to your model if that makes sense?
h2 <- model_variances$vcov[2]/(model_variances$vcov[2]+(model_variances$vcov[3]/4) + (model_variances$vcov[4]/8))
out <- capture.output(h2)
cat(out, file=paste0(Path,"/All/Heritability/heritability_canopy_",trait,".txt"), sep="\n", append=TRUE)
  
herit.temp <- data.frame("GDD" = trait, "Heritability" = h2)
# make a data frame with all the heritabilities
herit.out <- rbind(herit.out,herit.temp)
}

# export variance explained by each GDD
write.table(df.out, paste0(Path, "All/GDD_Variance_Explained_Canopy.txt"), sep = "\t", row.names = F)
# re-format file for plotting
df.out.1 <- df.out %>%
  pivot_longer(cols = 2:ncol(df.out), names_to = "variables", values_to = "R^2") %>%
  separate(GDD, into = c(NA,"GDD"),sep = "X")
# export heritability for each GDD
write.table(herit.out, paste0(Path,"All/GDD_Heritability_Canopy.txt"), sep = "\t", row.names = F)
# re-format file for plotting
herit.out.1 <- herit.out %>%
  separate(GDD, into = c(NA,"GDD"), sep = "X")

# Plot the variance explained by each variable
pdf(paste0(Path, "All/GDD_Variation_Explained_Canopy.pdf"), height = 3.5, width = 5)

plot <- ggplot() +
  geom_bar(data = df.out.1, aes(x = as.numeric(GDD), y =  `R^2`, fill = factor(variables, levels = c("Residuals", "Year.Rep.Block", "Year.Rep", "Year", "Genotype.Year", "Genotype"))), position = "stack", stat = "identity") +
  geom_point(data = herit.out.1, aes(x = as.numeric(GDD), y = Heritability), color = "black", fill = "red", size = 1, shape = 21) +
  scale_fill_viridis(discrete = T,name = "Variables", labels = c("Residuals", "Year/Rep/Block", "Year/Rep", "Year","Genotype/Year", "Genotype")) +
  theme_light() +
  theme(axis.text.x = element_text(angle = 90),
        legend.position = "bottom") +
  xlab("Growing Degree Days") +
  ylab("Variation explained") +
  scale_x_continuous(name = "Growing Degree Days", breaks = seq(from = 300, to = 1650, by = 50))

print(plot)

dev.off()
```

-   skipped 7. Identify the best model (and therefore the best GDD and
    intervals) and 6. Loess Regression PCA if necessary go back and
    reinclude or take information from

## Fréchet Distance and Fuzzy C-means Clustering

  - completes Fréchet distance analysis and fuzzy c-means clustering on plant height data, canopy cover data, and plant height and canopy cover data together

```{r}
# Fuzzy C-Means Clustering and Frechet Distance
# Dorothy Sweet
# May 5, 2022
###################################################################
###################################################################
### EVALUATION OF PLANT HEIGHT AND CANOPY COVERAGE INDIVIDUALLY ###
###################################################################
###################################################################
########### SET UP ############

# load libraries
library(ppclust)
library(factoextra)
library(dplyr)
library(cluster)
library(fclust)
library(psych)
library(clusterSim)
library(tidyverse)
library(gridExtra)
library(TSdist)
library(gplots)
library(RColorBrewer)
library(ggVennDiagram)
library(ggside)
library(janitor)

# set working directory
workingdir <- "/Users/dorothykirsch/Desktop/Projects_Data/Flights/All/"
# read in loess curve data
all_canopy <- read.csv(paste0(workingdir, "All_Year_Canopy_Loess_Data.csv"), header = T)
all_height <- read.delim(paste0(workingdir, "All_environment_GDD_Loess_Predictions.txt"))
# read in flowering data 
flowering <- read.csv(paste0(workingdir, "FloweringTimeHeteroticGroup_Widiv.csv"))
# manipulate loess curve files to match each other in format/order
all_canopy <- all_canopy[,c(1,32,30,33,31,34,2:29)] %>%
  mutate(Stand = NA,
         Row = NA, 
         Range = NA) %>%
  relocate(c(Stand,Row,Range), .before = Year)
# initialize a vector to put NA rows to remove in
bad.rows <- vector()
# remove rows with only NAs in height
for (Env.rows in 1:nrow(all_height)) {
  if (all(is.na(all_height[Env.rows,10:ncol(all_height)]))) {
    bad.rows <- append(bad.rows,Env.rows)
  }
}
all_height <- all_height[-bad.rows,]
### set up for loop for canopy and plant height ###
for (data.type in c("Canopy", "Height")) {
  ############################
  # Fuzzy C-means Clustering #
  ############################
  ########### SAME GENOTYPE WITHIN A YEAR ##########
  # read in loess curve data
  if (data.type == "Canopy"){
    df <- all_canopy
  } else if (data.type == "Height") {
    df <- all_height
  }
  # make the first column the row.name
  row.names(df) <- df[,1]
  # set up file for for loop through years
  data.year <- c("2018", "2019", "2020", "2021")
  # initiate crisp.clust.all
  crisp.clust.all <- as.data.frame(matrix(nrow = 0, ncol = 4))
  colnames(crisp.clust.all) <- c("Plot","Genotype","Rep","Cluster")
  # for loop through the years
  for (y in 1:length(data.year)) {
    year <- data.year[y]
    ## DATA PREP ##
    # subset the data frame based on the year
    df.year <- subset(df, Year %in% year)
    
    
    # average genotypes
    df.genotypes <- unique(df.year$Genotype)
    geno.average <- as.data.frame(matrix(nrow = 0, ncol = (ncol(df.year)- 8)))
    for (b in 1:length(df.genotypes)) {
      geno <- df.genotypes[b] # assign genotype
      temp.geno <- df.year %>%
        filter(Genotype == geno) %>% # filter to just that genotype
        dplyr::select(-c("Plot", "Block", "Rep", "Entry", "Stand", "Row", "Range", "Year")) %>% # remove all identifiers other than genotype
        add_row(Genotype = 'mean', !!! colMeans(.[-1], na.rm = T)) # calculate the average curve values
      # identify the genotype and its average
      geno.average[b,] <- tail(temp.geno, n = 1) # average values throughout the curve
      geno.average[b,1] <- temp.geno[1,1] # assign the genotype name
    }
    
    colnames(geno.average) <- c("Genotype", colnames(df.year)[10:ncol(df.year)]) # assign column names based on df.year
    rownames(geno.average) <- geno.average[,1] # assign genotype as the row name
    geno.average$Genotype <- NULL # remove genotype column
    geno.average <- geno.average %>%
      janitor::remove_empty(which = "cols") %>% # remove columns with all NAs
      na.omit() # remove remaining NAs

    # scale the data
    #df.year.scale <- scale(df.year)
    df.year.scale <- scale(geno.average)
    ## FUZZY C MEANS CLUSTERING ##
    for (clustnum in 2:5) {
      # fcm clustering - number of centers varying for testing
      temp_clust <- ppclust::fcm(df.year.scale, centers = clustnum, nstart = 5, fixmemb = T)
      # print the wellness of fit to each cluster
      write.csv(temp_clust$u, paste0(workingdir, "FCM_Cluster_Data/Averaged_FCM_",clustnum,"Clusters_FitEach_",data.type,"_",year,".csv"))
      # print the assigned clusters
      write.table(temp_clust$cluster, paste0(workingdir, "FCM_Cluster_Data/Averaged_FCM_",clustnum,"Clusters_CrispCluster_",data.type,"_",year,".txt"))
      # plot the data over all GDD colored based on clusters
      temp_plots <- geno.average %>%
        mutate(Cluster = temp_clust$cluster,
               Genotype = row.names(geno.average)) %>%
        pivot_longer(cols = 1:(ncol(geno.average)), names_to = "GDD", values_to = "Height") %>%
        separate(col = GDD, c(NA, "GDD"), sep = "X") %>%
        na.omit() %>%
        ggplot(aes(x = as.numeric(GDD), y = Height, group = Genotype, color = as.character(Cluster), na.rm = T)) +
        geom_point(na.rm = T, alpha = 0.5) +
        geom_line(na.rm = T, alpha = 0.5) +
        labs(color = "Cluster") +
        ylab(data.type)
      # plot the average value for each cluster with the ribbon showing the minimum and maximum values at each GDD
      temp_averages <- geno.average %>%
        mutate(Cluster = temp_clust$cluster,
               Plot = row.names(geno.average)) %>%
        pivot_longer(cols = 1:(ncol(geno.average)), names_to = "GDD", values_to = "Height") %>%
        separate(col = GDD, c(NA, "GDD"), sep = "X") %>%
        group_by(GDD, Cluster) %>%
        mutate(Max = max(Height),
               Min = min(Height)) %>%
        ungroup() %>%
        na.omit() %>%
        ggplot(aes(x = as.numeric(GDD), y = Height, group = Plot, color = as.character(Cluster), na.rm = T)) +
        geom_ribbon(aes(x = as.numeric(GDD), ymin = Min, ymax = Max, fill = as.character(Cluster)), alpha = 0.1) +
        stat_summary(fun=mean,geom="line",lwd=2,aes(group=Cluster)) +
        labs(fill = "Cluster") +
        guides(color = "none") +
        ylab(data.type)
      # export plots
      pdf(paste0(workingdir, "FCM_Cluster_Data/Averaged_loess_",data.type,"_", year, "_fcm_",clustnum,"clusters.pdf"), height = 5, width = 10)
      print(temp_plots)
      print(temp_averages)
      dev.off()
      ## MAKE PLOTS TO SEE HOW CLUMPED THE CLUSTERS ARE ##
      # Initiate data frame
      perc <- as.data.frame(temp_clust$u)
      perc$Largest.cluster <- NA
      perc$Largest.percent <- NA
      # for loop to go through all genotypes
      for (g in 1:nrow(perc)) {
        # find the largest cluster (largest percentage belongs to which cluster)
        perc[g,"Largest.cluster"] <- paste0("Averaged_Cluster_",as.character(which.max(perc[g,c(1:clustnum)])))
        # find the largest percentage
        perc[g,"Largest.percent"] <- max(perc[g,c(1:clustnum)])
      }
      # Make a long format
      long_perc <- perc
      long_perc$Plot <- row.names(long_perc) 
      long_perc <- long_perc %>%
        pivot_longer(cols = 1:clustnum,names_to = "Averaged_Cluster_Assignment", values_to = "Percentage")
      # Make a small subset for density plot
      long_perc.part <- long_perc[,c(2,3)]
      long_perc.part$Cluster.Number <- clustnum
      # assign perc for later use
      assign(paste0("perc.",clustnum),perc)
      # assign the long_perc.part to put all together with all cluster quantities
      assign(paste0("long_perc.part",clustnum),long_perc.part)
      # assign the clusters for later use
      assign(paste0("res.fcm.",clustnum), temp_clust)
    }
    # combine all values of clusters to make density plot
    long_perc.all <- rbind(long_perc.part2, long_perc.part3, long_perc.part4, long_perc.part5)
    # Make and print the density plot with all values of clusters
    pdf(paste0(workingdir, "FCM_Cluster_Data/Averaged_Cluster_Largest_Percentage_Density_fcm_",data.type,"_", year, ".pdf"), height = 4, width = 4) 
    temp_plot <- ggplot(long_perc.all, aes(x = Largest.percent, group = as.character(Cluster.Number), fill = as.character(Cluster.Number))) +
      geom_density(alpha = 0.4) +
      ggtitle(paste0("Density of Largest Percentages")) +
      xlab("Largest Percent") +
      ylab("Density") +
      labs(fill = "Cluster Number")
    print(temp_plot)
    dev.off()
    # Plots of all of the clusters in 3 grouped clusters with color representing how well the plot fits in the cluster
    colors <- c("red","blue","orange")
    for (u in 1:3) {
      # make the plot
      temp_plot <- geno.average %>%
        mutate(Cluster = perc.3[,u],
               Plot = row.names(geno.average)) %>%
        pivot_longer(cols = 1:(ncol(geno.average)), names_to = "GDD", values_to = "Height") %>%
        separate(col = GDD, c(NA, "GDD"), sep = "X") %>%
        na.omit() %>%
        mutate(Plot = fct_reorder(Plot, Cluster)) %>%
        ggplot(aes(x = as.numeric(GDD), y = Height, group = Plot, color = Cluster, na.rm = T)) +
        geom_line(na.rm = T, linewidth = 0.25) +
        scale_color_gradientn(colors = c(low = "grey", high = colors[u]), 
                              breaks = c(0, 0.25, 0.5, 0.75, 1), 
                              limits = c(0,1)) +
        ylab(data.type) +
        xlab("Growing Degree Days") +
        ggtitle(paste0("Cluster ",u)) +
        theme_light()
      # assign based on which cluster
      assign(paste0("clust",u),temp_plot)
    }
    # print all cluster plots in one file
    pdf(paste0(workingdir, "FCM_Cluster_Data/Averaged_Cluster_3fuzzy_visualization_fcm_",data.type,"_", year, ".pdf"), height = 5, width = 4)
    assign(paste0("fcm_",data.type,"_cluster_viz_", year, "_GDD"), grid.arrange(clust1, clust2, clust3, nrow = 3))
    dev.off()
    # create temp file for background data
    back.data <- merge(geno.data,flowering, by = "Genotype")
    # read in the crisp cluster assignment for the year
    clust.assign <- as.data.frame(read.delim(paste0(workingdir,"FCM_Cluster_Data/Averaged_FCM_3Clusters_CrispCluster_",data.type,"_",year,".txt"), sep = " "))
    # merge parts to get Plot, Genotype, cluster assignment
    crisp.clust.year <- as.data.frame(rownames(geno.average)) # make a data frame with rownames(df.year)
    crisp.clust.year$Genotype <- crisp.clust.year$`rownames(geno.average)` # rename column
    crisp.clust.year$`rownames(geno.average)` <- NULL # remove bad column
    #crisp.clust.year <- merge(crisp.clust.year,all_height[,c(1,2,4)], by = "Plot")
    # cbind crisp.clust.year and the cluster 
    crisp.clust.year <- cbind(crisp.clust.year,clust.assign)
    # rename cluster column
    crisp.clust.year$Cluster <- crisp.clust.year$x
    crisp.clust.year$x <- NULL
    crisp.clust.year$Year <- year
    # rbind all to crisp.clust.all
    crisp.clust.all <- rbind(crisp.clust.all, crisp.clust.year)
  }
  ########## UPSET PLOT OF THE GENOTYPES IN EACH CLUSTER ##############
  # for loop to find the number of intersections for pairs of each year and cluster
  for (a in 1:3) {
    for (b in 1:3) {
      for (c in c("2018","2019","2020","2021")) {
        first <- crisp.clust.all %>%
          filter(Year == c & Cluster == a) %>%
          dplyr::select(Genotype)
        for (d in c("2019","2020","2021")) {
          second <- crisp.clust.all %>%
            filter(Year == d & Cluster == b) %>%
            dplyr::select(Genotype)
          assign(paste0(c,"_clust_",a,"_",d,"_clust_",b), 
                 as.numeric(count(generics::intersect(first,second))))
        }
      }
    }
  }
  
  # for loop to find the number of intersections for triplets of each year and cluster
  for (a in 1:3) {
    for (b in 1:3) {
      for (c in 1:3) {
        for (d in c("2018","2019","2020","2021")) {
          first <- crisp.clust.all %>%
            filter(Year == d & Cluster == a) %>%
            dplyr::select(Genotype)
          for (e in c("2019","2020","2021")) {
            second <- crisp.clust.all %>%
              filter(Year == e & Cluster == b) %>%
              dplyr::select(Genotype)
            for (f in c("2020","2021")) {
              third <- crisp.clust.all %>%
                filter(Year == f & Cluster == c) %>%
                dplyr::select(Genotype)
              temp <- generics::intersect(first,second) # find genotypes between 1 & 2
              assign(paste0(d,"_clust_",a,"_",e,"_clust_",b,"_",f,"_clust_",c), 
                     as.numeric(count(generics::intersect(temp,third)))) # find # genotypes btwn 1&2&3
            }
          }
        }
      }
    }
  }
  
  # for loop to find the number of intersections for all four years and clusters
  for (a in 1:3) {
    for (b in 1:3) {
      for (c in 1:3) {
        for (d in 1:3) {
          for (e in c("2018","2019","2020","2021")) {
            first <- crisp.clust.all %>%
              filter(Year == e & Cluster == a) %>%
              dplyr::select(Genotype)
            for (f in c("2019","2020","2021")) {
              second <- crisp.clust.all %>%
                filter(Year == f & Cluster == b) %>%
                dplyr::select(Genotype)
              for (g in c("2020","2021")) {
                third <- crisp.clust.all %>%
                  filter(Year == g & Cluster == c) %>%
                  dplyr::select(Genotype) 
                for (h in "2021") {
                  fourth <- crisp.clust.all %>%
                    filter(Year == h & Cluster == d) %>%
                    dplyr::select(Genotype)
                  temp <- generics::intersect(first,second) # find genotypes btwn 1&2
                  temp1 <- generics::intersect(temp,third) # find genotypes btwn 1&2&3
                  assign(paste0(e,"_clust_",a,"_",f,"_clust_",b,"_",g,"_clust_",c,"_",h,"_clust_",d), 
                         as.numeric(count(generics::intersect(temp1,fourth)))) # find # genotypes btwn 1&2&3&4
                }
              }
            }
          }
        }
      }
    }
  }
  
  # create combined data frame for the upset plot
  upset_data <- c("2018.S.t.T&2019.S.t.T" = `2018_clust_1_2019_clust_3`,
                  "2018.S.t.S&2019.S.t.S" = `2018_clust_3_2019_clust_2`,
                  "2018.T.t.T&2019.T.t.T" = `2018_clust_2_2019_clust_1`,
                  "2018.S.t.T&2020.S.t.T" = `2018_clust_1_2020_clust_3`,
                  "2018.S.t.S&2020.S.t.S" = `2018_clust_3_2020_clust_1`,
                  "2018.T.t.T&2020.T.t.T" = `2018_clust_2_2020_clust_2`,
                  "2018.S.t.T&2021.S.t.T" = `2018_clust_1_2021_clust_3`,
                  "2018.S.t.S&2021.S.t.S" = `2018_clust_3_2021_clust_1`,
                  "2018.T.t.T&2021.T.t.T" = `2018_clust_2_2021_clust_2`,
                  "2019.S.t.T&2020.S.t.T" = `2019_clust_3_2020_clust_3`,
                  "2019.S.t.S&2020.S.t.S" = `2019_clust_2_2020_clust_1`,
                  "2019.T.t.T&2020.T.t.T" = `2019_clust_1_2020_clust_2`,
                  "2019.S.t.T&2021.S.t.T" = `2019_clust_3_2021_clust_3`,
                  "2019.S.t.S&2021.S.t.S" = `2019_clust_2_2021_clust_1`,
                  "2019.T.t.T&2021.T.t.T" = `2019_clust_1_2021_clust_2`,
                  "2020.S.t.T&2021.S.t.T" = `2020_clust_3_2021_clust_3`,
                  "2020.S.t.S&2021.S.t.S" = `2020_clust_1_2021_clust_1`,
                  "2020.T.t.T&2021.T.t.T" = `2020_clust_2_2021_clust_2`,
                  "2018.S.t.T&2019.S.t.T&2020.S.t.T&2021.S.t.T" = `2018_clust_1_2019_clust_3_2020_clust_3_2021_clust_3`,
                  "2018.S.t.S&2019.S.t.S&2020.S.t.S&2021.S.t.S" = `2018_clust_3_2019_clust_2_2020_clust_1_2021_clust_1`,
                  "2018.T.t.T&2019.T.t.T&2020.T.t.T&2021.T.t.T" = `2018_clust_2_2019_clust_1_2020_clust_2_2021_clust_2`
  )
  # create and export upset plot
  pdf(paste0(workingdir,"FCM_Cluster_Data/Averaged_ClusterToCluster_within_",data.type,"_Upset_AllYears.pdf"), height = 5, width = 6) #4
  UpSetR::upset(UpSetR::fromExpression(upset_data),
                sets = c("2018.S.t.S",
                         "2019.S.t.S",
                         "2020.S.t.S",
                         "2021.S.t.S",
                         "2018.T.t.T",
                         "2019.T.t.T",
                         "2020.T.t.T",
                         "2021.T.t.T",
                         "2018.S.t.T",
                         "2019.S.t.T",
                         "2020.S.t.T",
                         "2021.S.t.T"),
                mb.ratio = c(0.6, 0.4),
                number.angles = 0, 
                text.scale = 1.1, 
                point.size = 2.8, 
                line.size = 1,
                keep.order = T,
                sets.x.label = element_blank(), 
                mainbar.y.label = paste0("Intersection of ",data.type," Clusters"),
                sets.bar.color = c("orange","orange","orange","orange","blue","blue","blue","blue","red","red","red","red"),
                show.numbers = "no",
                set_size.show = F
                #main.bar.color = c("red","red","red","blue","orange","orange","orange","blue","blue","blue","blue","red","blue","orange","red","orange","red","orange","blue","orange","red")
  )
  dev.off()
  
  ########## SAME GENOTYPE DIFFERENT YEARS ##########
  df.year <- df
  # remove unnecessary columns
  df.year[,c("Plot", "Genotype", "Block", "Rep", "Entry", "Stand", "Row", "Range", "Year", "X300", "X350", "X400","X1500", "X1550", "X1600", "X1650")] <- NULL
  # remove NAs
  df.year <- df.year[ , colSums(is.na(df.year)) < nrow(df.year)]
  df.year <- na.omit(df.year)
  # scale the data
  df.year.scale <- scale(df.year)
  # run FCM with multiple starts
  res.fcm <- fcm(df.year.scale, centers=3, nstart=5, fixmemb=TRUE)
  # plot the data over all GDD colored based on clusters
  temp_plots <- df.year %>%
    mutate(Cluster = res.fcm$cluster, 
           Plot = row.names(df.year)) %>%
    pivot_longer(cols = 1:(ncol(df.year)), names_to = "GDD", values_to = "Height") %>%
    separate(col = GDD, c(NA, "GDD"), sep = "X") %>%
    na.omit() %>%
    ggplot(aes(x = as.numeric(GDD), y = Height, group = Plot, color = as.character(Cluster), na.rm = T)) +
    geom_point(na.rm = T, alpha = 0.5) +
    geom_line(na.rm = T, alpha = 0.5) +
    labs(color = "Cluster") +
    ylab(data.type)
  # Add cluster visualizations that show both the year and the clusters (facet on year and cluster) 
  temp_facet_plots <- df.year %>%
    mutate(Cluster = res.fcm$cluster, 
           Plot = row.names(df.year),
           Year = row.names(df.year)) %>%
    separate(col = Year, c("Year", NA), sep = ":") %>%
    pivot_longer(cols = 1:(ncol(df.year)), names_to = "GDD", values_to = "Height") %>%
    separate(col = GDD, c(NA, "GDD"), sep = "X") %>%
    na.omit() %>%
    ggplot(aes(x = as.numeric(GDD), y = Height, group = Plot, color = as.character(Cluster), na.rm = T)) +
    geom_point(na.rm = T, alpha = 0.5) +
    geom_line(na.rm = T, alpha = 0.5) +
    labs(color = "Cluster") +
    ylab(data.type) +
    facet_wrap(~ Year + Cluster, scales = "free_y", nrow = 4)
  # plot the average value for each cluster with the ribbon showing the minimum and maximum values at each GDD
  temp_averages <- df.year %>%
    mutate(Cluster = res.fcm$cluster,
           Plot = row.names(df.year)) %>%
    pivot_longer(cols = 1:(ncol(df.year)), names_to = "GDD", values_to = "Height") %>%
    separate(col = GDD, c(NA, "GDD"), sep = "X") %>%
    group_by(GDD, Cluster) %>%
    mutate(Max = max(Height),
           Min = min(Height)) %>%
    ungroup() %>%
    na.omit() %>%
    ggplot(aes(x = as.numeric(GDD), y = Height, group = Plot, color = as.character(Cluster), na.rm = T)) +
    geom_ribbon(aes(x = as.numeric(GDD), ymin = Min, ymax = Max, fill = as.character(Cluster)), alpha = 0.1) +
    stat_summary(fun=mean,geom="line",lwd=2,aes(group=Cluster)) +
    labs(fill = "Cluster") +
    ylab(data.type) +
    guides(color = "none")
  # export crisp clusters for all years
  df.year.cluster <- df.year %>%
    mutate(Cluster = res.fcm$cluster,
           Plot = row.names(df.year))
  df.year.cluster <- df.year.cluster[,c(ncol(df.year.cluster),(ncol(df.year.cluster)-1))]
  write.table(df.year.cluster,paste0(workingdir,"FCM_Cluster_Assignment_all_year_",data.type,".txt"), row.names = F)
  # export plots
  pdf(paste0(workingdir, "loess_all_years_fcm_clusters_",data.type,".pdf"), height = 5, width = 10)
  print(temp_plots)
  print(temp_facet_plots)
  print(temp_averages)
  dev.off()
  # Assign the percentage outcomes from the fcm of all years
  perc.all <- as.data.frame(res.fcm$u)
  # make a plot for all 3 clusters
  colors <- c("red","blue","orange")
  for (u in 1:3) {
    temp_plot <- df.year %>%
      mutate(Cluster = perc.all[,u], 
             Plot = row.names(df.year)) %>%
      pivot_longer(cols = 1:(ncol(df.year)), names_to = "GDD", values_to = "Height") %>%
      separate(col = GDD, c(NA, "GDD"), sep = "X") %>%
      na.omit() %>%
      mutate(Plot = fct_reorder(Plot, Cluster)) %>%
      ggplot(aes(x = as.numeric(GDD), y = Height, group = Plot, color = Cluster, na.rm = T)) +
      geom_line(na.rm = T, size = 0.25) +
      scale_color_gradientn(colors = c(low = "grey", high = colors[u]), 
                            breaks = c(0, 0.25, 0.5, 0.75, 1), 
                            limits = c(0,1)) +
      ylab(data.type)
    assign(paste0("clust",u), temp_plot)
  }
  # Print the clusters
  pdf(paste0(workingdir, "Cluster_visualization_",data.type,"_all_year_GDD.pdf"))
  assign(paste0("fcm_",data.type,"_cluster_viz_", year, "_GDD"), grid.arrange(clust1, clust2, clust3, nrow = 3))
  dev.off()
  ####################
  # Fréchet Distance #
  ####################  
  ########## SET UP ##########
  # isolate the unique genotypes
  genotypes <- unique(df$Genotype)
  # isolate the loess curve data without the extra information 
  if (data.type == "Canopy") {
    df.1 <- df[, c("Genotype","Rep","Year","X450","X500","X550","X600","X650","X700","X750","X800","X850","X900","X950","X1000","X1050","X1100","X1150","X1200","X1250","X1300","X1350","X1400","X1450")] 
    # reorder
    df.1 <- df.1[,c("Genotype","Rep","Year","X450","X500","X550","X600","X650","X700","X750","X800","X850","X900","X950","X1000","X1050","X1100","X1150","X1200","X1250","X1300","X1350","X1400","X1450")]
  } else if (data.type == "Height") {
    df.1 <- df[,c("Genotype","Rep","Year","X150","X200","X250","X300","X350","X400","X450","X500","X550","X600","X650","X700","X750","X800","X850","X900","X950","X1000","X1050","X1100","X1150","X1200","X1250","X1300","X1350","X1400","X1450")]
    # reorder
    df.1 <- df.1[,c("Genotype","Rep","Year","X150","X200","X250","X300","X350","X400","X450","X500","X550","X600","X650","X700","X750","X800","X850","X900","X950","X1000","X1050","X1100","X1150","X1200","X1250","X1300","X1350","X1400","X1450")]
  }
  # calculate z scores so canopy and plant height are comparable later
  # get mean and sd values of the data
  mean <- mean(as.matrix(df.1[,c(4:ncol(df.1))]), na.rm = T)
  sd <- sd(as.matrix(df.1[,c(4:ncol(df.1))]), na.rm = T)
  # normalize data
  df.1[,c(4:ncol(df.1))]<- data.frame(apply(df.1[,c(4:ncol(df.1))], MARGIN = c(1,2), function(x) (x - mean)/sd))
  # print the normalized data for use later on
  write.table(df.1, paste0(workingdir, "AllYear_zScore_Data_",data.type,".txt"))
  # Make df the same as df.1
  df <- df.1
  ########## SAME GENOTYPE WITHIN A YEAR ##########
  # initiate distance information file
  genotype_distance <- as.data.frame(matrix(nrow = length(genotypes), ncol = 5))
  colnames(genotype_distance) <- c("Genotype", "X2018", "X2019", "X2020", "X2021")
  genotype_distance$Genotype <- genotypes
  # for loop to go through each year
  for (y in 1:length(data.year)) {
    year <- data.year[y]
    # filter df by year
    df_subset <- filter(df, df$Year == year)
    # Make a data frame containing the Genotypes that have too many reps
    bad_genotype <- data.frame(matrix(nrow = 3, ncol = 1))
    colnames(bad_genotype) <- "Genotype"
    bad_genotype$Genotype <- c("B73", "PH207", "ND259")
    # dendrograms of data frame by year
    dendro_df <- filter(df.1, df.1$Year == year) # filter the dataframe by year
    dendro_df.1 <- merge(dendro_df,flowering, by = "Genotype") # merge the data with flowering info
    dendro_df.1 <- dendro_df.1[,c(1,ncol(dendro_df.1)-1,ncol(dendro_df.1),2:(ncol(dendro_df.1)-2))] # reorder the file
    dendro_df.1$Keep <- 0 # Make a keep/discard column to filter on later
    # filter dendro_df.1 to only keep two representatives of B73, PH207, and ND259
    dendro_df.1 <-  dendro_df.1 %>%
      mutate(Keep = Genotype %in% bad_genotype$Genotype) %>%
      group_by(Genotype, Rep) %>%
      mutate(Keep = +(row_number() > 1 & Keep))
    # Remove the extra checks for B73, PH207, and ND259
    dendro_df.1 <- dendro_df.1 %>%
      filter(Keep == 0)
    # make a new column with Genotype and Rep included
    dendro_df.1$newGenotype <- paste(dendro_df.1$Genotype,dendro_df.1$Rep) 
    # Make it a data frame
    dendro_df.1 <- as.data.frame(dendro_df.1) 
    # make newGenotype the rownames
    row.names(dendro_df.1) <- dendro_df.1$newGenotype
    # Make days to flowering a factor
    dendro_df.1$DaysToFlowering <- as.factor(dendro_df.1$DaysToFlowering) 
    # Compute Euclidean distance between samples
    dist=dist(dendro_df.1[ , c(6:(ncol(dendro_df.1)-2))] , diag=TRUE)
    # Perform clustering with hclust
    hc <- hclust(dist)
    dhc <- as.dendrogram(hc)
    # Change the leaf color attributes
    i=0
    colLab<<-function(n){
      if(is.leaf(n)){
        #I take the current attributes
        a=attributes(n)
        #I deduce the line in the original data, and so the treatment and the specie.
        ligne=match(attributes(n)$label,dendro_df.1$newGenotype)
        Heterotic=dendro_df.1[ligne,3];
        if(is.na(Heterotic)){col_treatment="red"}else {if(Heterotic=="flint"){col_treatment="#5D2A42"}; if(Heterotic=="iodent"){col_treatment="#FB4D3D"}; if(Heterotic=="mixed"){col_treatment="#6A994E"}; if(Heterotic=="non_stiff_stalk"){col_treatment="#3E78B2"}; if(Heterotic=="popcorn"){col_treatment="#F694C1"}; if(Heterotic=="stiff_stalk"){col_treatment="#92817A"}; if(Heterotic=="sweet_corn"){col_treatment="#B1F8F2"}; if(Heterotic=="tropical"){col_treatment="#D7F75B"}; if(Heterotic=="unknown"){col_treatment="#122C34"}}
        d2f=dendro_df.1[ligne,2];
        if(is.na(d2f)){col_d2f="blue"}else {if(d2f==71){col_d2f="#1E0501"}; if(d2f==72){col_d2f="#3D0B02"}; if(d2f==73){col_d2f="#5B1003"}; if(d2f==74){col_d2f="#791503"}; if(d2f==75){col_d2f="#981A04"}; if(d2f==76){col_d2f="#B61F05"}; if(d2f==77){col_d2f="#D52506"}; if(d2f==78){col_d2f="#F32A07"}; if(d2f==79){col_d2f="#F44526"}; if(d2f==80){col_d2f="#F65F45"}; if(d2f==81){col_d2f="#F77A64"}; if(d2f==82){col_d2f="#F99583"}; if(d2f==83){col_d2f="#FAAFA2"}; if(d2f==84){col_d2f="#FCCAC1"}; if(d2f==85){col_d2f="#FDE4E0"}; if(d2f==86){col_d2f="#FFFAF9"}; if(d2f==87){col_d2f="#FFFFFF"}}
        #Modification of leaf attribute
        attr(n,"nodePar")<-c(a$nodePar,list(cex=0.5,lab.cex=0.5,pch=20,col=col_treatment,lab.col=col_d2f,lab.font=1,lab.cex=1))
      }
      return(n)
    }
    # Finally I just have to apply this to my dendrogram
    dL <- dendrapply(dhc, colLab)
    # set the legend data
    legend_name <- c("Flint" , "Iodent" , "Mixed" , "Non stiff stalk" , "Popcorn", "Stiff stalk", "Sweet Corn", "Tropical", "Unknown", "71", "72","73","74","75","76","77","78","79","80","81","82","83","84","85","86","87")
    legend_color <- c("#5D2A42","#FB4D3D","#6A994E", "#3E78B2","#F694C1","#92817A", "#B1F8F2","#D7F75B","#122C34","#1E0501","#3D0B02","#5B1003","#791503","#981A04","#B61F05","#D52506","#F32A07","#F44526","#F65F45","#F77A64","#F99583","#FAAFA2","#FCCAC1","#FDE4E0","#FFFAF9","#FFFFFF")
    # And the plot
    pdf(paste0(workingdir,"Dendrograms/RelationshipForLoessData_",data.type,"_",year,".pdf"), height = 11, width = 18)
    
    plot(dL , main="Relationship between Plot Growth Curves and Genotypes")
    legend("topright", 
           legend = legend_name, 
           col = legend_color,
           pch = c(20,20,20,20,20,20,20,20,20,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4), bty = "n",  pt.cex = 0.4, cex = 0.3 , 
           text.col = "black", horiz = F, inset = c(-0.1, 0))
    
    plot(dL[[1]] , main="Relationship between Plot Growth Curves and Genotypes Group 1")
    legend("topright", 
           legend = legend_name, 
           col = legend_color,
           pch = c(20,20,20,20,20,20,20,20,20,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4), bty = "n",  pt.cex = 0.4, cex = 0.3 , 
           text.col = "black", horiz = F, inset = c(-0.1, 0))
    
    plot(dL[[2]], main="Relationship between Plot Growth Curves and Genotypes Group 2")
    legend("topright", 
           legend = legend_name, 
           col = legend_color,
           pch = c(20,20,20,20,20,20,20,20,20,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4), bty = "n",  pt.cex = 0.4, cex = 0.3 , 
           text.col = "black", horiz = F, inset = c(-0.1, 0))
    
    dev.off()
    # for loop to go through each genotype
    for (g in 1:length(genotypes)) {
      geno <- genotypes[g]
      # filter df_subset by genotype
      df_subset_g <- filter(df_subset, df_subset$Genotype == as.character(geno))
      # remove first two columns from df_subset_g
      df_subset_g <- df_subset_g[, -c(1:6)]
      # make two dataframes of the data from each genotype
      plot1_g <- data.frame(df_subset_g[1,])
      plot2_g <- data.frame(df_subset_g[2,])
      # filter genotype_distance to just the genotype of interest
      geno_temp <- genotype_distance %>%
        filter(Genotype == genotypes[g])
      # calculate Fréchet distance
      geno_temp[1,(y+1)] <- FrechetDistance(plot1_g, plot2_g)
      # replace value in genotype_distance with the values from geno_temp
      genotype_distance[g,] <- geno_temp
    }
  }
  # Print genotype_distance
  write.table(genotype_distance, file = paste0(workingdir, "Frechets/FrechetDistance_btwn_geno_wthn_year_",data.type,".txt"))
  # make a visualization
  geno_dist_long <- pivot_longer(genotype_distance, 2:5, names_to = "Year", values_to = "Distance")
  # merge with flowering time and heterotic group data
  FlowerDistance <- merge(geno_dist_long, flowering, by = "Genotype")
  # merge with cluster data
  cluster.assign <- read.delim(paste0(paste0(workingdir,"FCM_Cluster_Assignment_all_year_",data.type,".txt")), sep = " ")
  Plots2Geno <- read.csv(paste0(workingdir, "Plots2GenotypeAllYears.csv"))
  cluster.assign <- merge(cluster.assign, Plots2Geno, by = "Plot")
  cluster.assign <- merge(geno_dist_long, cluster.assign[,c(2,3)], by = "Genotype")
  
  FlowerDistance$Geno.Max <- NA
  for (s in 1:nrow(FlowerDistance)) {
    if (FlowerDistance[s,2] == "X2021") {
      FlowerDistance[s,6] <- FlowerDistance[s,3]
    }
  }
  pdf(paste0(workingdir, "Frechets/FrechetDistance_btwn_geno_wthn_year_all_scatter_",data.type, ".pdf"), height = 6, width = 9)
  temp_plot <- FlowerDistance %>%
    group_by(Genotype) %>%
    mutate(Geno.Max.All = max(Geno.Max, na.rm = T)) %>%
    ungroup() %>%
    ggplot(aes(x = reorder(Genotype, - Geno.Max.All), y = Distance, color = Year)) +
             geom_point() +
    theme_classic() +
    theme(
      axis.text.x = element_text(angle = 90, size = 2)
    ) +
    ggtitle(paste0("Fréchet Distance Between Replications of "), data.type) +
    xlab("Genotype") +
    scale_color_discrete(name = "Year", labels = c("2018", "2019", "2020", "2021")) +
    geom_ysidedensity(aes(x = stat(density), fill = Year, alpha = 0.5)) +
    guides(fill = F, alpha = F)
  print(temp_plot)
  dev.off()
  for (year in c("X2018", "X2019", "X2020", "X2021")) {
    # scatterplot by heterotic group
    het.group.scat <- FlowerDistance %>%
      filter(Year == year) %>%
      ggplot(aes(x = reorder(Genotype,-Distance), y = Distance, color = Heterotic.Group)) +
      geom_point() +
      theme(
        axis.text.x = element_text(angle = 90, size = 2)
      ) +
      xlab("Genotype") +
      ggtitle("Distance by Heterotic Groups")
    # scatterplot by flowering date
    flower.date.scat <- FlowerDistance %>%
      filter(Year == year) %>%
      ggplot(aes(x = reorder(Genotype,-Distance), y = Distance, color = DaysToFlowering)) +
      geom_point() +
      theme(
        axis.text.x = element_text(angle = 90, size = 2)  
      ) +
      xlab("Genotype") +
      ggtitle("Distance by Flowering Date")
    # scatterplot by clusters
    cluster.assign.scat <- cluster.assign %>%
      filter(Year == year) %>%
      ggplot(aes(x = reorder(Genotype,-Distance), y = Distance, color = as.character(Cluster))) +
      geom_point() +
      theme(
        axis.text.x = element_text(angle = 90, size = 2)  
      ) +
      xlab("Genotype") +
      ggtitle("Distance by Fuzzy C Mean Cluster")
    # Print Scatterplots
    pdf(file = paste0(workingdir, "Frechets/Scatterplot_FrechetDistance_btwn_geno_wthn_year_",year,"_",data.type,".pdf"))
    print(het.group.scat)
    print(flower.date.scat)
    print(cluster.assign.scat)
    dev.off()
  }
  # print pdf of heatmap of frechet distance
  pdf(file = paste0(workingdir, "Frechets/Heatmap_FrechetDistance_btwn_geno_wthn_year_",data.type,".pdf"), height = 11, width = 18)
  temp_plot <- ggplot(geno_dist_long, aes(x = Genotype, y = Year, fill = Distance)) +
    geom_tile() + 
    theme(
      axis.text.x=element_text(angle=-90))
  print(temp_plot)
  dev.off()
  # summarize average distance for each year and non NA count
  avg_dist <- summary(genotype_distance)
  write.table(avg_dist, file = paste0(workingdir, "Frechets/Summary_FrechetDistance_btwn_geno_wthn_year_",data.type,".txt"))
  ########## SAME GENOTYPE DIFFERENT YEAR ##########
  # initiate mean Frechet distance over years for genotype file
  frechet_mean <- as.data.frame(matrix(nrow = 0, ncol = 2))
  colnames(frechet_mean) <- c("Genotype", "Mean.Frechet.Distance")
  frechet_pairwise <- as.data.frame(matrix(nrow = 0, ncol = 7))
  colnames(frechet_pairwise) <- c("Genotype","X2018_2019", "X2018_2020", "X2018_2021", "X2019_2020", "X2019_2021", "X2020_2021")
  for (g in 1:length(genotypes)) {
    geno <- genotypes[g]
    # filter df by genotype
    geno_sub <- filter(df, df$Genotype == as.character(geno))
    # initiate data frame
    geno_year_mean <- as.data.frame(matrix(ncol = 21, nrow = 0))
    # for loop to average the curves within year
    for(z in c(2018, 2019, 2020, 2021)) {
      gen <- filter(geno_sub, geno_sub$Year == z) # filter out the individual year
      gen <- gen[,-c(1:6)] # remove genotype and year
      temp <- as.data.frame(t(colMeans(gen))) # create a temporary file with the curves means
      temp$Year <- z # add year back in 
      geno_year_mean <- rbind(geno_year_mean,temp) # rbind the average for the year back in 
    }
    # Make data frame to hold the Frechat distance results
    geno_distance <- as.data.frame(matrix(nrow = 1, ncol = 6))
    colnames(geno_distance) <- c("X2018_2019", "X2018_2020", "X2018_2021", "X2019_2020", "X2019_2021", "X2020_2021")
    # for loop to complete all of the possible combinations
    rep = 1 # create a counter to put the frechet distances in the right place
    for(g1 in 2018:2020) {
      sub_1 <- geno_year_mean %>%
        filter(Year == g1)
      sub_1$Year <- NULL # remove Year
      for (g2 in (g1+1):2021) {
        sub_2 <- geno_year_mean %>%
          filter(Year == g2)
        sub_2$Year <- NULL # remove Year
        # calculate Frechet distance and put in geno_distance
        geno_distance[1,rep] <- FrechetDistance(sub_1, sub_2)
        rep <- rep +1 # add one to the rep
      }
    }
    # print the geno_distance file for the genotype
    output_filename <- gsub(pattern = "/", replacement = "-", x = geno)
    write.table(geno_distance, file = paste0(workingdir, "Frechets/FrechetDistance_btwn_geno_all_year_", output_filename, "_",data.type,".txt"))
    # add the frechet distances to the frechet_pairwise file
    frechet_pairwise[g,1] <- as.character(geno)
    frechet_pairwise[g,c(2:7)] <- geno_distance
    # add the mean frechet distance for the genotype to the frechet_mean file
    frechet_mean[g,1] <- as.character(geno)
    frechet_mean[g,2] <- mean(unlist(geno_distance), na.rm = T)
  }
  write.table(frechet_mean, file = paste0(workingdir, "Frechets/FrechetDistance_btwn_geno_all_year_mean_",data.type,".txt"))
  # make a heatmap of the data 
  heatmap_pair <- frechet_pairwise # make a copy
  heatmap_pair <- merge(heatmap_pair,flowering, by = "Genotype") # add the flowering data in
  row.names(heatmap_pair) <- heatmap_pair$Genotype # make the Genotype the rownames
  heatmap_pair <- na.omit(heatmap_pair) # remove NAs
  my_group_het <- as.factor(substr(heatmap_pair$Heterotic.Group,1,1)) # Make file with heterotic group for the row side colors
  my_group_d2f <- (as.factor(substr(heatmap_pair$DaysToFlowering,1,2))) # Make a file with Days2Flowering for row side colors
  heatmap_pair <- heatmap_pair[,-c(1,8,9)] # remove genotype, heterotic group, and days to flowering
  heatmap_pair <- as.matrix(heatmap_pair) # make heatmap_pair a matrix
  my_palette <- colorRampPalette(c("#05C4CB", "#CB05C4", "#C4CB05")) # set the color palette
  colSideHet <- brewer.pal(9, "Set1")[my_group_het] # set row side colors based on heterotic group
  colSideD2f <- colorRampPalette(c("red", "orange", "yellow","white"))(12)[my_group_d2f]# set row side colors based on Days to flowering
  rowSideAvg <- brewer.pal(9, "YlOrRd")[as.factor(as.numeric(colMeans(heatmap_pair)))]
  # print heatmap

pdf(paste0(workingdir, "Frechets/Heatmap_FrechetDistance_Btwn_Same_Geno_Different_Years_all_",data.type,".pdf"), height = 4, width = 10)

heatmap.2(t(heatmap_pair), # plotting the transposed version of the data frame
          trace = "none", # remove tracing
          scale = "none", # don't scale the data
          dendrogram = "none", # remove dendrograms
          col = my_palette, # color using my defined pallete above
          labRow = c("2018 & 2019","2018 & 2020","2018 & 2021","2019 & 2020","2019 & 2021","2020 & 2021"), # assign row text labels
          cexRow = 1.25, # set Row text size
          labCol = F, # remove column text
          xlab = "Genotypes", # set Row title
          ylab = "Year Pairs", # set Column title
          margins = c(2,8.5), # change margins around heatmap
          #srtRow = 90, # adjust the angle of row text
          adjRow = c(NA, 0.5),# adjust location of row text
          Rowv = F,
          key.title = NA, # remove title for key
          key.xlab = NA, # rename xlabel on key
          keysize = 1, # change the size of the key
          lmat = rbind(c(0,4),c(2,1),c(0,3)), # put the key on the top (key = 4, rowden = 2, heatmap = 1, colden = 3)
          lwid = c(0.15,4), # sets the widths of columns of the matrix for assigning locations
          lhei = c(1,4,.25) # sets the heights of rows of the matrix for assigning locations
          ) 

dev.off()
  # Make Just a Dendrogram 
  Flower_pair <- merge(frechet_pairwise,flowering, by = "Genotype") # add flowering data
  Flower_pair <- na.omit(Flower_pair) # remove NAs
  row.names(Flower_pair) <- Flower_pair$Genotype # change row names to the Genotype
  distance <- dist(Flower_pair[,c(2:7)], diag = TRUE) # euclidian distance for dendrogram
  hc <- hclust(distance) # clustering for dendrogram
  dhc <- as.dendrogram(hc) # dendrogram
  # Change the leaf color attributes
  i=0
  colLab<<-function(n){
    if(is.leaf(n)){
      #I take the current attributes
      a=attributes(n)
      #I deduce the line in the original data, and so the treatment and the specie.
      ligne=match(attributes(n)$label,Flower_pair[,1])
      Heterotic=Flower_pair[ligne,9];
      if(is.na(Heterotic)){col_treatment="red"}else {if(Heterotic=="flint"){col_treatment="#5D2A42"}; if(Heterotic=="iodent"){col_treatment="#FB4D3D"}; if(Heterotic=="mixed"){col_treatment="#6A994E"}; if(Heterotic=="non_stiff_stalk"){col_treatment="#3E78B2"}; if(Heterotic=="popcorn"){col_treatment="#F694C1"}; if(Heterotic=="stiff_stalk"){col_treatment="#92817A"}; if(Heterotic=="sweet_corn"){col_treatment="#B1F8F2"}; if(Heterotic=="tropical"){col_treatment="#D7F75B"}; if(Heterotic=="unknown"){col_treatment="#122C34"}}
      d2f=Flower_pair[ligne,8];
      if(is.na(d2f)){col_d2f="blue"}else {if(d2f==71){col_d2f="#1E0501"}; if(d2f==72){col_d2f="#3D0B02"}; if(d2f==73){col_d2f="#5B1003"}; if(d2f==74){col_d2f="#791503"}; if(d2f==75){col_d2f="#981A04"}; if(d2f==76){col_d2f="#B61F05"}; if(d2f==77){col_d2f="#D52506"}; if(d2f==78){col_d2f="#F32A07"}; if(d2f==79){col_d2f="#F44526"}; if(d2f==80){col_d2f="#F65F45"}; if(d2f==81){col_d2f="#F77A64"}; if(d2f==82){col_d2f="#F99583"}; if(d2f==83){col_d2f="#FAAFA2"}; if(d2f==84){col_d2f="#FCCAC1"}; if(d2f==85){col_d2f="#FDE4E0"}; if(d2f==86){col_d2f="#FFFAF9"}; if(d2f==87){col_d2f="#FFFFFF"}}
      #Modification of leaf attribute
      attr(n,"nodePar")<-c(a$nodePar,list(cex=1,lab.cex=1,pch=20,col=col_treatment,lab.col=col_d2f,lab.font=1,lab.cex=1))
    }
    return(n)
  }
  # Finally I just have to apply this to my dendrogram
  dL <- dendrapply(dhc, colLab)
  # And the plot
  pdf(paste0(workingdir,"Dendrograms/RelationshipForLoessData_same_genotype_different_years_",data.type,".pdf"), height = 11, width = 18)
  plot(dL , main="Relationship between Plot Growth Curves and Genotypes within a Year")
  legend("topright", 
         legend = legend_name, 
         col = legend_color,
         pch = c(20,20,20,20,20,20,20,20,20,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4), bty = "n",  pt.cex = 0.4, cex = 0.3 , 
         text.col = "black", horiz = F, inset = c(-0.1, 0))
  plot(dL[[1]] , main="Relationship between Plot Growth Curves and Genotypes within a Year Group 1")
  legend("topright", 
         legend = legend_name, 
         col = legend_color,
         pch = c(20,20,20,20,20,20,20,20,20,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4), bty = "n",  pt.cex = 0.4, cex = 0.3 , 
         text.col = "black", horiz = F, inset = c(-0.1, 0))
  plot(dL[[2]], main="Relationship between Plot Growth Curves and Genotypes within a Year Group 2")
  legend("topright", 
         legend = legend_name, 
         col = legend_color,
         pch = c(20,20,20,20,20,20,20,20,20,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4,4), bty = "n",  pt.cex = 0.4, cex = 0.3 , 
         text.col = "black", horiz = F, inset = c(-0.1, 0))
  dev.off()
  # Scatterplots 
  # read in heterotic group and flowering date data
  FlowerDistance <- merge(frechet_mean, flowering, by = "Genotype")
  
  FlowerDistance <- merge(frechet_pairwise, flowering, by = "Genotype")
  FlowerDistance$Max <- apply(FlowerDistance[,c(2:7)], 1, max, na.rm = T) #as.numeric(pmax(FlowerDistance[,c(2:7)], na.rm = T))
  Flower_dist_long <- FlowerDistance %>%
    pivot_longer(cols = 2:7, names_to = "Year_Group", values_to = "Distance") %>%
    group_by("Gentoype") %>%
    ggplot(aes(x = reorder(Genotype, -Max), y = Distance, color = Heterotic.Group)) +
    geom_point() +
    theme(
      axis.text.x = element_text(angle = 90, size = 2)
    )
  pdf(paste0(workingdir,"Frechets/Scatterplot_FrechetDistance_btwn_geno_btwn_years_",data.type,".pdf"))
  # scatterplot by heterotic group
  het_scat <- FlowerDistance %>%
    pivot_longer(cols = 2:7, names_to = "Year_Group", values_to = "Distance") %>%
    group_by("Gentoype") %>%
    ggplot(aes(x = reorder(Genotype, -Max), y = Distance, color = Heterotic.Group)) +
    geom_point() +
    theme(
      axis.text.x = element_text(angle = 90, size = 2)
    ) +
    xlab("Genotype") +
    ggtitle("Scatterplot of Frechet Distance colored by Heterotic Group")
  # scatterplot by flowering date 
  flower_scat <- FlowerDistance %>%
    pivot_longer(cols = 2:7, names_to = "Year_Group", values_to = "Distance") %>%
    group_by("Gentoype") %>%
    ggplot(aes(x = reorder(Genotype, -Max), y = Distance, color = DaysToFlowering)) +
    geom_point() +
    theme(
      axis.text.x = element_text(angle = 90, size = 2)
    ) +
    xlab("Genotype") +
    ggtitle("Scatterplot of Frechet Distance colored by Days to Flowering")
  # scatterplot by Year Group
  year_scat <- FlowerDistance %>%
    pivot_longer(cols = 2:7, names_to = "Year_Group", values_to = "Distance") %>%
    group_by("Gentoype") %>%
    ggplot(aes(x = reorder(Genotype, -Max), y = Distance, color = Year_Group)) +
    geom_point() +
    theme(
      axis.text.x = element_text(angle = 90, size = 2)
    ) +
    xlab("Genotype") +
    ggtitle("Scatterplot of Frechet Distance colored by Year Pairing")
  print(het_scat)
  print(flower_scat)
  print(year_scat)
  dev.off()
  # manipulation to make the matrix for the heatmap
  frechet_mean.2 <- na.omit(frechet_mean[order(frechet_mean$Mean.Frechet.Distance),]) # copy frechet_mean for analysis
  frechet_mean.3 <- frechet_mean.2 # make a copy
  frechet_mean.2$Mean.2 <- frechet_mean.2$Mean.Frechet.Distance # duplicate the mean data
  Genotypes_frechet_mean <- frechet_mean.2$Genotype #Make a file with the genotype data
  frechet_mean.2 <- frechet_mean.2[,-1] # remove the genotype column
  frechet_mean.2 <- as.matrix(frechet_mean.2) # make it a numeric matrix
  rownames(frechet_mean.2) <- Genotypes_frechet_mean # make the genotypes the row names
  # make files for descriptive information for the heatmap
  frechet_mean.3 <- merge(frechet_mean.3, flowering, by = "Genotype") # add the flowering data in
  row.names(frechet_mean.3) <- frechet_mean.3$Genotype # make the Genotype the rowname
  frechet_mean_grp_het <- as.factor(substr(frechet_mean.3$Heterotic.Group,1,1)) # make file with heterotic group 
  frechet_mean_grp_d2f <- as.factor(substr(frechet_mean.3$DaysToFlowering,1,2)) # make file with d2f
  mean.palette <- colorRampPalette(c("Yellow", "Orange", "Red"))
  meanSideHet <- brewer.pal(9, "Set1")[frechet_mean_grp_het] # set row side colors based on heterotic group
  meanSideD2F <- colorRampPalette(c("red", "orange", "yellow","white"))(12)[frechet_mean_grp_d2f]# set row side colors based on Days to flowering
  # print pdf of heatmap
  pdf(file = paste0(workingdir, "Frechets/Heatmap_FrechetDistance_btwn_geno_all_year_mean_",data.type,".pdf"), height = 11, width = 18)
  temp_plot <- heatmap.2(t(frechet_mean.2), trace = "none", Rowv = NA, Colv = NA, scale = "none", col = mean.palette, ColSideColors = meanSideHet, dendrogram = "none", labRow = F, cexCol = 0.6)
  print(temp_plot)
  temp_plot <- heatmap.2(t(frechet_mean.2), trace = "none", Rowv = NA, Colv = NA, scale = "none", col = mean.palette, ColSideColors = meanSideD2F, dendrogram = "none", labRow = F, cexCol = 0.6)
  print(temp_plot)
  dev.off()
}
############################################################
############################################################
#### Analysis of Plant Height and Canopy Cover Together ####
############################################################
############################################################
### SEPARATE VALUES BROUGHT TOGETHER ###
###############################################
# Plant Height VS Canopy Cover Fuzzy Clusters #
###############################################
# UPSET AND VENN DIAGRAM OF PLANT HEIGHT AND CANOPY COVER CLUSTERS #
library(UpSetR)
# reload the crisp fuzzy c means clusters
fcm_comp_height <- read.delim(paste0(workingdir,"FCM_Cluster_Assignment_all_year_Height.txt"), sep = " ")
fcm_comp_canopy <- read.delim(paste0(workingdir,"FCM_Cluster_Assignment_all_year_Canopy.txt"), sep = " ")
# for loop to find the number of intersections
for (cl in 1:3){
  for (cl.1 in 1:3) {
    ph <- fcm_comp_height %>%
      filter(Cluster == cl) %>%
      dplyr::select(Plot)
    cc <- fcm_comp_canopy %>%
      filter(Cluster == cl.1) %>%
      dplyr::select(Plot)
    assign(paste0("clust",cl,"clust",cl.1), as.numeric(count(generics::intersect(ph,cc))))
  }
}
# create combined data frame for upset plot
upset_data <- c(#Plant.Height.Cluster.1 = sum(fcm_comp_height$Cluster == 1),
                #Plant.Height.Cluster.2 = sum(fcm_comp_height$Cluster == 2),
                #Plant.Height.Cluster.3 = sum(fcm_comp_height$Cluster == 3),
                #Canopy.Cover.Cluster.1 = sum(fcm_comp_canopy$Cluster == 1),
                #Canopy.Cover.Cluster.2 = sum(fcm_comp_canopy$Cluster == 2),
                #Canopy.Cover.Cluster.3 = sum(fcm_comp_canopy$Cluster == 3),
                "Plant.Height.Cluster.1&Plant.Height.Cluster.2" = 0,
                "Plant.Height.Cluster.1&Plant.Height.Cluster.3" = 0,
                "Plant.Height.Cluster.2&Plant.Height.Cluster.3" = 0,
                "Canopy.Closure.Cluster.1&Canopy.Closure.Cluster.2" = 0,
                "Canopy.Closure.Cluster.1&Canopy.Closure.Cluster.3" = 0,
                "Canopy.Closure.Cluster.2&Canopy.Closure.Cluster.3" = 0,
                "Plant.Height.Cluster.1&Canopy.Closure.Cluster.1" = clust1clust1,
                "Plant.Height.Cluster.1&Canopy.Closure.Cluster.2" = clust1clust2,
                "Plant.Height.Cluster.1&Canopy.Closure.Cluster.3" = clust1clust3,
                "Plant.Height.Cluster.2&Canopy.Closure.Cluster.1" = clust2clust1,
                "Plant.Height.Cluster.2&Canopy.Closure.Cluster.2" = clust2clust2,
                "Plant.Height.Cluster.2&Canopy.Closure.Cluster.3" = clust2clust3,
                "Plant.Height.Cluster.3&Canopy.Closure.Cluster.1" = clust3clust1,
                "Plant.Height.Cluster.3&Canopy.Closure.Cluster.2" = clust3clust2,
                "Plant.Height.Cluster.3&Canopy.Closure.Cluster.3" = clust3clust3)
# create and export upset plot
pdf(paste0(workingdir,"FCM_Cluster_Data/ClusterToCluster_PlantVCanopy_Upset_AllYear.pdf"), height = 5, width = 11)
upset(fromExpression(upset_data), 
      sets = c("Canopy.Closure.Cluster.3", "Canopy.Closure.Cluster.2", "Canopy.Closure.Cluster.1", "Plant.Height.Cluster.3", "Plant.Height.Cluster.2", "Plant.Height.Cluster.1"),
        #"Plant.Height.Cluster.1", "Plant.Height.Cluster.2", "Plant.Height.Cluster.3","Canopy.Cover.Cluster.1","Canopy.Cover.Cluster.2","Canopy.Cover.Cluster.3"),
      #nintersects = 15, 
      #nsets = 6, 
      #order.by = "freq", 
      #decreasing = T, 
      mb.ratio = c(0.6, 0.4),
      number.angles = 0, 
      text.scale = 1.1, 
      point.size = 2.8, 
      line.size = 1,
      keep.order = T,
      sets.x.label = " Plots Per Cluster", 
      mainbar.y.label = "Height Vs Closure Cluster Intersection",
      sets.bar.color = c("blue", "blue","blue","red","red","red")
)
dev.off()
# combine canopy and plant height
fcm_comp_canopy$Type <- "Canopy"
fcm_comp_height$Type <- "Height"
fcm_comp <- rbind(fcm_comp_canopy,fcm_comp_height)
#################################################
# Plant Height VS Canopy Cover Frechet Distance #
#################################################
# load distance matrices for canopy cover and plant height distance
canopy_dist <- read.delim(paste0(workingdir,"Frechets/FrechetDistance_btwn_geno_wthn_year_Canopy.txt"), sep = " ")
height_dist <- read.delim(paste0(workingdir,"Frechets/FrechetDistance_btwn_geno_wthn_year_Height.txt"), sep = " ")
# SCATTERPLOT OF PLANT HEIGHT VS CANOPY COVER #
# pivot longer and add columns for data type
canopy_dist <- canopy_dist %>%
  pivot_longer(cols = 2:5, names_to = "Year", values_to = "Canopy")
height_dist <- height_dist %>%
  pivot_longer(cols = 2:5, names_to = "Year", values_to = "Height")
# combine data types
comb_dist <- merge(canopy_dist,height_dist, by = c("Genotype","Year"))
# scatterplot of plant height vs canopy cover
pdf(paste0(workingdir,"Frechets/Scatterplot_FrechetDistance_GenoBtwnYears_PlantVCanopy.pdf"), height = 4, width = 4)
temp_plot <- ggplot(comb_dist, aes(x = Height, y = Canopy, group = Genotype, color = Year)) +
  geom_point(size = 0.75) +
  geom_abline(slope = 1, intercept = 0) +
  xlim(0,2) +
  ylim(0,2) +
  theme_light() +
  geom_ysidedensity(aes(x = stat(density), group = Year, fill = Year, alpha = 0.3)) +
  geom_xsidedensity(aes(y = stat(density), group = Year, fill = Year, alpha = 0.3)) +
  guides(alpha = "none") +
  theme(legend.position = "top",
        legend.title = element_blank()) +
  scale_xsidey_continuous(limits = c(0,5), breaks = c(0,4)) +
  scale_ysidex_continuous(limits = c(0,5), breaks = c(0,4)) +
  scale_fill_manual(labels = c("2018", "2019","2020","2021"), values = c("#F8766D","#7CAE00","#00BFC4","#C77CFF")) +
  scale_color_manual(labels = c("2018", "2019","2020","2021"),values = c("#F8766D","#7CAE00","#00BFC4","#C77CFF"))
print(temp_plot)
dev.off()
# CORRELATION HEATMAP OF PLANT HEIGHT VS CANOPY COVER #
# Read in z-score canopy cover and plant height data
canopy.z <- read.delim(paste0(workingdir, "AllYear_zScore_Data_Canopy.txt"), sep = " ")
canopy.z$Plot <- rownames(canopy.z) # make a plot column
height.z <- read.delim(paste0(workingdir, "AllYear_zScore_Data_Height.txt"), sep = " ")
height.z$Plot <- rownames(height.z) # make a plot column
# initialize file to hold all correlations for all GDDs and all years
corr.file <- as.data.frame(matrix(ncol = 22, nrow = 0))
colnames(corr.file) <- c("Year","X450","X500","X550","X600","X650","X700","X750","X800","X850","X900","X950",
                         "X1000","X1050","X1100","X1150","X1200","X1250","X1300","X1350","X1400","X1450")
# set up for the loop
All.years <- c(2018, 2019, 2020, 2021)
# for loop to isolate years
for (ay in 1:length(All.years)) {
  year = All.years[ay]
  # initialize file to hold correlations for the year
  corr.temp <- as.data.frame(matrix(ncol = 22, nrow = 0))
  colnames(corr.temp) <- c("Year","X450","X500","X550","X600","X650","X700","X750","X800","X850","X900","X950",
                           "X1000","X1050","X1100","X1150","X1200","X1250","X1300","X1350","X1400","X1450")
  corr.temp[1,"Year"] <- year
  # initialize a file with the listed GDDs
  all.gdds <- c("X450","X500","X550","X600","X650","X700","X750","X800","X850","X900",
                "X950","X1000","X1050","X1100","X1150","X1200","X1250","X1300","X1350","X1400","X1450")
  # filter height.z by year
  height.z.year <- height.z %>%
    filter(Year == year)
  # filter canopy.z by year
  canopy.z.year <- canopy.z %>%
    filter(Year == year)
  #initialize temporary files
  temp.hei <- height.z.year %>%
    dplyr::select(Plot)
  temp.can <- canopy.z.year %>%
    dplyr::select(Plot)
  # nested for loop to isolate same GDD interval from both plant height and canopy cover
  for (gdd in 1:21) {
    # identify the gdd of interest
    gdd.ite <- all.gdds[gdd]
    # identify individual GDD column and plot names from plant height 
    temp.hei$Height <- height.z.year[,gdd.ite]
    # identify individual GDD column and plot names from canopy cover
    temp.can$Canopy <- canopy.z.year[,gdd.ite]
    # merge together based on plot
    temp.both <- merge(temp.hei, temp.can, by = "Plot")
    # remove NAs (if necessary - may be taken care of by merge)
    temp.both <- na.omit(temp.both)
    # calculate correlation between cc and ph and add to temporary file
    corr.temp[1,gdd.ite] <- cor(temp.both[,"Height"], temp.both[,"Canopy"])
  }
  corr.file <- rbind(corr.file, corr.temp)
}
# Make the rownames the year and remove year column
rownames(corr.file) <- corr.file$Year
corr.file$Year <- NULL
# Make the color pallete for the heatmap
colors = c(seq(-0.9,-0.31,length=15),seq(-0.3,0.3,length=15),seq(0.31,0.9,length=15))
my_palette <- colorRampPalette(c("blue", "white", "red"))(n = 44)
# Plot a heatmap of the correlations with x axis the GDDs and y axis the years
pdf(paste0(workingdir, "Correlation_PlantVCanopy_Heatmap.pdf"), height = 2.5, width = 4)
heatmap.2(as.matrix(corr.file),
          trace = "none", 
          scale = "none", 
          dendrogram = "none",
          col = my_palette,
          cexRow = 1.25,
          labCol = c("450", "500", "550","600","650","700","750","800","850","900","950","1000","1050","1100","1150","1200","1250","1300","1350","1400","1450"), 
          ylab = "Years", 
          xlab = "Growing Degree Days", 
          symm = F, 
          symkey = F, 
          symbreaks = T,
          Rowv = F, 
          Colv = F,
          breaks = colors, 
          keysize = 1, 
          key.title = NA,
          key.xlab = NA,
          denscol = "black", 
          offsetRow = 0,
          margins = c(5,5), # change margins around heatmap
          lmat = rbind(c(0,4),c(2,1),c(0,3)), # put the key on the top (key = 4, rowden = 2, heatmap = 1, colden = 3)
          lwid = c(0.15,4), # sets the widths of columns of the matrix for assigning locations
          lhei = c(1.25,4,.25) # sets the heights of rows of the matrix for assigning locations
          )
dev.off()
####### TOGETHER ########
# Bring down all data and rescale
# get mean and sd values of the data
mean <- mean(as.matrix(all_canopy[,c(7:ncol(all_canopy))]), na.rm = T)
sd <- sd(as.matrix(all_canopy[,c(7:ncol(all_canopy))]), na.rm = T)
# normalize data
all_canopy.z <- all_canopy
all_canopy.z[,c(10:ncol(all_canopy.z))]<- data.frame(apply(all_canopy.z[,c(10:ncol(all_canopy.z))], MARGIN = c(1,2), function(x) (x - mean)/sd))

all_canopy.z <- all_canopy.z %>%
  dplyr::select(-c(Row,Range,Stand))

# get mean and sd values of the data
mean <- mean(as.matrix(all_height[,c(10:ncol(all_height))]), na.rm = T)
sd <- sd(as.matrix(all_height[,c(10:ncol(all_height))]), na.rm = T)
# normalize data
all_height.z <- all_height
all_height.z[,c(10:ncol(all_height.z))]<- data.frame(apply(all_height.z[,c(10:ncol(all_height.z))], MARGIN = c(1,2), function(x) (x - mean)/sd))
# initiate file for all years of Frechet Distance
PlantVCanopyDistAll <- as.data.frame(matrix(nrow = 0, ncol = 7))
colnames(PlantVCanopyDistAll) <- c("Genotype", "Plot", "Distance", "DaysToFlowering", "Heterotic.Group", "Year", "Geno.Max")
# set up for loop
year_list <- c("2018","2019", "2020","2021")
for (year in year_list) {
  # subset the files to just that year and make a vector to contain only plots in both
  temp_can <- filter(all_canopy.z, Year == year)
  temp_hei <- filter(all_height.z, Year == year)
  # merge plant height and canopy cover to get Plots that are present in both
  temp_both <- merge(temp_hei,temp_can, by = "Plot")
  # set all plots to plot values present in both 
  All_plots <- as.character(temp_both$Plot)
  # Change to long format and change GDD column titles to include canopy 
  temp_can <- temp_can %>%
    filter(Plot %in% All_plots) %>%
    pivot_longer(cols =  7:34, names_to = "GDD", values_to = "Coverage") %>%
    mutate(GDD_Canopy = paste0(GDD,"_Canopy")) %>%
    dplyr::select(-GDD) %>%
    pivot_wider(names_from = "GDD_Canopy", values_from = "Coverage")
  # Make Plot the rownames and cut down to just the right GDDs
  rownames(temp_can) <- temp_can$Plot
  temp_can <- as.data.frame(temp_can)
  if (year == "2018") {
    temp_can <- temp_can[,-c(2:9,31:34)]
  } else {
    temp_can <- temp_can[,-c(2:6,31:34)]
  }
  # Change to long format and change GDD column titles to include height
  temp_hei <- temp_hei %>%
    filter(Plot %in% All_plots) %>%
    pivot_longer(cols =  10:36, names_to = "GDD", values_to = "Height") %>%
    mutate(GDD_Height = paste0(GDD,"_Height")) %>%
    dplyr::select(-GDD) %>%
    pivot_wider(names_from = "GDD_Height", values_from = "Height")  
  # Make Plot the rownames and cut down to just the right GDDs
  temp_hei <- as.data.frame(temp_hei)
  rownames(temp_hei) <- temp_hei$Plot
  if (year == "2018") {
    temp_hei <- temp_hei[,-c(2:15)]
  } else {
    temp_hei <- temp_hei[,-c(2:12)]
  }
  # remove plot
  temp_hei$Plot <- NULL
  temp_can$Plot <- NULL
  ####################
  # Frechet Distance #
  ####################
  # initiate file for frechet distance
  PlantVCanopyDist <- as.data.frame(matrix(nrow = 0, ncol=2))
  colnames(PlantVCanopyDist) <- c("Plot", "Distance")
  # for loop to go through all plots
  for (r in 1:nrow(temp_hei)) {
    # isolate a single curve from both height and canopy cover
    temp.hei <- temp_hei[r, ]
    temp.can <- temp_can[r, ]
    # add Plot and the distance
    PlantVCanopyDist[r,1] <- rownames(temp_hei[r,])
    PlantVCanopyDist[r,2] <- FrechetDistance(temp.hei,temp.can)
  }
  PlantVCanopyDist <- merge(PlantVCanopyDist, all_height[,c(1,2,4)], by = "Plot") # add genotype data back in
  PlantVCanopyDist <- merge(PlantVCanopyDist, flowering, by = "Genotype") # add Heterotic group and flowering data
  PlantVCanopyDist$Year <- year # add a column for year
  # add a column with the max for that genotype for just 2018
  if (year == "2021") {
    PlantVCanopyDist <- PlantVCanopyDist %>%
      group_by(Genotype) %>%
      mutate(Geno.Max = max(Distance)) %>%
      ungroup() 
  } else {
    PlantVCanopyDist$Geno.Max <- 0
  }
  # add to file containing every year
  PlantVCanopyDistAll <- rbind(PlantVCanopyDistAll,PlantVCanopyDist)
}


PlantVCanopyDist.sum <- PlantVCanopyDistAll %>%
  dplyr::select(-c(Plot,Rep,DaysToFlowering,Heterotic.Group,Geno.Max)) %>%
  group_by(Genotype,Year) %>%
  mutate(Average = mean(Distance)) %>%
  dplyr::select(-Distance) %>%
  mutate(Year = paste0("PlantHeightvsCanopyCover.",Year)) %>%
  unique() %>%
  pivot_wider(names_from = Year, values_from = Average)
  
write.csv(PlantVCanopyDist.sum, paste0(workingdir,"Frechets/FrechetDistance_within_geno_btwn_PlantHeightvsCanopyCover.csv"))

# Make a new column to organize all facets based on 2021
PlantVCanopyDistAll <- PlantVCanopyDistAll %>%
  group_by(Genotype) %>%
  mutate(Geno.Max.All = max(Geno.Max))
PlantVCanopyDistAll <- as.data.frame(PlantVCanopyDistAll) # make it a data frame
upper <- max(PlantVCanopyDistAll$Geno.Max.All) # set the upper limit of the ribbon
lower <- min(PlantVCanopyDistAll[,8][which(PlantVCanopyDistAll[,8]>0)]) # set the lower limit of the ribbon
# Count values in each year 
for (l in year_list) {
  assign(paste0("quant",l), count(PlantVCanopyDistAll %>%
                                    filter(Year == l)))
}
f_labels <- data.frame(Year = c("2018", "2019", "2020", "2021"), label = c(as.character(quant2018), as.character(quant2019), as.character(quant2020), as.character(quant2021)))
# plot the distance of Plant vs canopy scatterplot and histogram #
# pdf(paste0(workingdir, "Frechets/Scatterplot&Histogram_FrechetDistance_Btwn_Plant_Height&Canopy_Cover.pdf"), height = 8, width = 13)
# # temp_scat <- ggplot(PlantVCanopyDistAll, aes(x = reorder(Genotype, - Geno.Max.All))) +
# #            geom_point(aes(y = Distance, color = Heterotic.Group)) +
# #   geom_ribbon(aes(x = reorder(Genotype, -Geno.Max.All), ymin = lower, ymax = upper, group = Year), alpha = 0.4) +
# #   geom_line(aes(x = reorder(Genotype, -Geno.Max.All), y = Geno.Max.All, group = Year)) +
# #   theme_classic() +
# #            theme (
# #              axis.text.x = element_text(angle = 90, size = 2),
# #              ) +
# #            facet_wrap(~ fct_relevel(Year, levels = c("2021", "2020", "2019", "2018")), nrow = 4) +
# #   ggtitle("Frechet Distance Between Plant Height and Canopy Coverage") +
# #   xlab("Genotype")
# # print(temp_scat)
# 
# # temp_scat <- PlantVCanopyDistAll %>%
# #   group_by(Year,Genotype) %>%
# #   mutate(Geno.Mean = mean(Distance)) %>%
# #   ggplot(aes(x = reorder(Genotype,-Geno.Max.All), y = Geno.Mean, color = Year)) +
# #   geom_point() +
# #   theme_classic() +
# #   theme(
# #     axis.text.x = element_text(angle = 90, size = 2),
# #     axis.ticks.x = element_blank()
# #   ) +
# #   ggtitle("Frechet Distance Between Plant Height and Canopy Coverage") +
# #   xlab("Genotype") +
# #   ylab("Average Distance") +
# #   geom_ysidedensity(aes(x=stat(density), yfill = Year, alpha = 0.3)) +
# #   guides(alpha = "none")
# # print(temp_scat)
# 
# # temp_hist <- ggplot(PlantVCanopyDistAll, aes(x = Distance)) +
# #   geom_histogram() +
# #   ggtitle("Distribution of Frechet Distance Between Plant Height and Canopy Coverage"
# #   )
# # print(temp_hist)
# # temp_hist <- ggplot(PlantVCanopyDistAll, aes(x = Distance)) +
# #   geom_histogram() +
# #   geom_text(x = 0.125, y = 125, aes(label = label), data = f_labels) +
# #   facet_wrap(~ Year) +
# #   ggtitle("Distribution of Frechet Distance Between Plant Height and Canopy Coverage over each year") +
# #   xlab("Genotype")
# #   
# # print(temp_hist)
# 
# dev.off()
### HEATMAP ###
# reduce B73, PH207, and ND259 to only two representative replications for each year #
# initialize new file to hold the PlantVCanopy histogram data
PlantVCanopyHist <- as.data.frame(matrix(ncol = 4, nrow = 0))
colnames(PlantVCanopyHist) <- c("Genotype", "Rep", "Distance", "Year")
# for loop to go through each year to remove extra reps
for (year in year_list) {
  # remove extra replications of B73, PH207, and ND259
  pvc.temp <- PlantVCanopyDistAll %>%
    unique() %>%
    dplyr::select("Genotype", "Rep", "Distance", "Year") %>% # select columns of interest
    filter(Year == year) %>% # filter out only the year
    mutate(Keep = 0,#) %>%#,
           Keep = Genotype %in% bad_genotype$Genotype) %>% # create column keep with all 0s initially
    group_by(Genotype,Rep) %>%
    mutate(Keep = +(row_number() > 1 & Keep)) %>% # change all reps not the first 1 or 2 to 1 in Keep column
    filter(Keep == 0) %>%# remove all rows with Keep = 1
    mutate(Keep = NULL)
  # add to PlantVCanopyHist
  PlantVCanopyHist <- rbind(PlantVCanopyHist,as.data.frame(pvc.temp))
}
# Remove duplicate replicates within a year
PlantVCanopyHist <- PlantVCanopyHist %>%
  pivot_wider(names_from = "Rep", values_from = "Distance") %>%
  mutate(Distance.1 = case_when(
    !is.na(`1`) ~ `1`),
    Distance.2 = case_when(
      is.na(Distance.1) ~ `2`))
# make column (Distance) keeping rep1 if it exists and rep2 if it doesn't
PlantVCanopyHist$Distance <- paste(ifelse(is.na(PlantVCanopyHist$Distance.1),"",PlantVCanopyHist$Distance.1), ifelse(is.na(PlantVCanopyHist$Distance.2), "",PlantVCanopyHist$Distance.2)) 
# reset Distance as numeric
PlantVCanopyHist$Distance <- as.numeric(PlantVCanopyHist$Distance)
# select the necessary columns and pivot wider
PlantVCanopyHist <- PlantVCanopyHist %>%
  dplyr::select("Genotype", "Year", "Distance") %>%
  distinct() %>%
  pivot_wider(names_from = "Year", values_from = "Distance")
# make the rownames the newGenotype column and remove newGenotype
geno_names <- PlantVCanopyHist$Genotype
PlantVCanopyHist$Genotype <- NULL
PlantVCanopyHist <- as.matrix(PlantVCanopyHist) # make it a matrix
rownames(PlantVCanopyHist) <- geno_names
# set colors for heatmap
colors = c(seq(from=min(PlantVCanopyHist, na.rm = T), to=max(PlantVCanopyHist, na.rm = T), length.out=25))
my_palette <- colorRampPalette(c("#05C4CB", "#CB05C4", "#C4CB05"))
#my_palette <- c(colorRampPalette(c("white","yellow","red"))(n = 24))
# Heatmap
pdf(paste0(workingdir,"Frechets/Heatmap_FrechetDistance_Btwn_Height&CanopyCover.pdf"), height = 3, width = 4)
heatmap.2(t(na.omit(PlantVCanopyHist)),
          trace = "none",
          scale = "none",
          Rowv = F,
          Colv = T,
          na.color = "white",
          breaks = colors,
          col = my_palette,
          cexCol = 0.25,
          dendrogram = "none",
          denscol = 'black',
          labCol = F, 
          keysize = 1.5,
          key.title = NA,
          key.xlab = NA,
          key.ylab = NA,
          xlab = "Genotypes",
          ylab = "Year",
          margins = c(2,6.5),
          lmat = rbind(c(0,4),c(2,1),c(0,3)), # put the key on the top (key = 4, rowden = 2, heatmap = 1, colden = 3)
          lwid = c(0.15,4), # sets the widths of columns of the matrix for assigning locations
          lhei = c(1,4,.10) # sets the heights of rows of the matrix for assigning locations
          
          )
dev.off()

```
## Fréchet distance vs Terminal Height variance

 - determine which genotypes have low variance in terminal height and high average Fréchet distance values or vice versa

```{r}
library(tidyverse)

# read in average frechet distance values
frechet.average <- read.table("/Users/dorothykirsch/Desktop/Projects_Data/Flights/All/Frechets/FrechetDistance_btwn_geno_all_year_mean_Height.txt")
# determine 
frechet.average %>%
  mutate(average = mean(Mean.Frechet.Distance, na.rm = T),
         std.dev = sd(Mean.Frechet.Distance, na.rm = T))

terminal.height <- read.table("/Users/dorothykirsch/Desktop/Projects_Data/Flights/All/Terminal_Height_Data_Descript.txt") %>%
  dplyr::select(Genotype,Rep,Year,Final) %>%
  filter(!is.na(Final)) %>%
  group_by(Genotype) %>%
  mutate(variance = var(Final)) %>%
  dplyr::select(Genotype, variance) %>%
  unique() 


ave.fre.term.ph <- merge(frechet.average,terminal.height, by = "Genotype")
ave.fre.term.ph$Group <- NA

 for (a in 1:nrow(ave.fre.term.ph)) {
  if(is.na(ave.fre.term.ph[a,3])) {
    next
  } else if (is.na(ave.fre.term.ph[a,2])) {
    next
  } else if(ave.fre.term.ph[a,3]<2729 & ave.fre.term.ph[a,2]<0.42) {
    ave.fre.term.ph[a,4] <- "low.fre.low.var"
  } else if(ave.fre.term.ph[a,3]>2729 & ave.fre.term.ph[a,2]<0.42) {
    ave.fre.term.ph[a,4] <- "low.fre.hi.var"
  } else if(ave.fre.term.ph[a,3]<2729 & ave.fre.term.ph[a,2]>0.42) {
    ave.fre.term.ph[a,4] <- "hi.fre.low.var"
  } else if(ave.fre.term.ph[a,3]>2729 & ave.fre.term.ph[a,2]>0.42) {
    ave.fre.term.ph[a,4] <- "hi.fre.hi.var"
  }
}

ave.fre.term.ph %>%
  group_by(Group) %>%
  mutate(ave.var = mean(variance, na.rm = T),
         ave.fre = mean(Mean.Frechet.Distance, na.rm = T),
         count = n()) %>%
  dplyr::select(Group, ave.var, ave.fre, count) %>%
  unique()
```

## Make visual of Largest and Smallest Fréchet distance genotypes

  - plots the genotypes with the lowest average Frechet distance and the genotypes with the highest average Frechet distance over the growing season 
  
```{r}
library(tidyverse)

########## INPUTS ##########
data.type = "Height" # options: "Height", "Canopy"
geno = c("PHN66","YING-55","PHG47") # options: c("PHN66","YING-55","PHG47") for height, or c("EAST 028","N542","ND259","790) for canopy
###########################
# read in data
  if (data.type == "Canopy") {
    all_height <- read.csv("/Users/dorothykirsch/Desktop/Projects_Data/Flights/All/All_Year_Canopy_Loess_Data.csv")
    all_height <- all_height[,-c(29)]
  } else if (data.type == "Height") {
    all_height <- read.csv("/Users/dorothykirsch/Desktop/Projects_Data/Flights/All/All_environment_GDD_Loess_Predictions.txt", sep = "\t")
    all_height <- all_height[,c(1,10:36,3,5,2,4,9)]
  }
# for loop to go through all of the genotypes
for (a in (1:length(geno))) {
  # assign genotype
  genotype <- geno[a]
  # print plot for each genotype
  # export plot
pdf(paste0("/Users/dorothykirsch/Desktop/Projects_Data/Flights/All/Best&Worst_FrechetDistance_",data.type,"_Individual_Genotypes_",genotype,".pdf"), height = 2.5, width = 3.3)

# plot growth over all
tester <- all_height %>%
  filter(Genotype == genotype) %>%
  pivot_longer(cols = 2:28, names_to = "GDD", values_to = "Height") %>%
  separate(col = GDD, into = c(NA, "GDD"), sep = "X", remove = T) %>%
  ggplot(aes(x = as.numeric(GDD), y = Height, group = Plot, color = as.character(Year))) +
  geom_point() +
  geom_line() +
  theme_light() +
  xlab("Growing Degree Days") +
  ylab(data.type) +
  guides(color = guide_legend(title = "Year"))
print(tester)
dev.off()
  
}

```
## Precipitation Data

  - Takes Precipitation Data from its original source, adds GDD and makes it better to work with in GWAS analysis later

```{r}
library(tidyverse)

#################################################### VARIABLES TO CHANGE #######################################
planting_date <- "05062021" ### StPaul2018: 05142018, StPaul2019: 05302019, StPaul2020: 05072020, Waseca2020: 05122020, StPaul2021: 05062021
year <- "2021"
field <- "X2"
location <- "SaintPaul"
################################################################################################################
# read in the file
raw_weather <- read.csv(paste0("/Users/dorothykirsch/Desktop/Projects_Data/Flights/", year,"/", location, "/", field, "_", year, "/Weather_", year,".csv"))

# split the Date column and only keep the date and not the time
raw_weather$Date <- sapply(as.character(raw_weather$Date), function(x) strsplit(x, split=" ", fixed = T)[[1]][1])

# reformat date to match normal format
raw_weather$Date <- as.Date(raw_weather$Date, "%m/%d/%y")
raw_weather$Date <- format(raw_weather$Date, "%m%d%y")

# add a rainfall_total column as well as max and min temperature for the total rainfall from that day
weather_rain <- raw_weather %>%
  group_by(Date) %>%
  mutate(RainfallTotal = sum(Rainfall),
         DailyMaxT = max(AirTMax.F.),
         DailyMinT = min(AirTMin.F.)) %>%
  ungroup()
  
# Set up a file to calculate the GDD from the temperature data
GDD_calculation_file <- cbind(weather_rain[,"Date"], weather_rain[, "DailyMaxT"], weather_rain[, "DailyMinT"])
GDD_calculation_file <- unique(GDD_calculation_file)
GDD_calculation_file$GDD <- 0

# calculate the GDD for each day of the growing season
for (wrow in 1:nrow(GDD_calculation_file)){
  if (((GDD_calculation_file[wrow,2] + GDD_calculation_file[wrow,3])/2)> 50) {
    GDD_calculation_file[wrow,4] = (((GDD_calculation_file[wrow,2] + GDD_calculation_file[wrow,3])/2) - 50)
  } else {
    GDD_calculation_file[wrow,4] = 0
  }
}

# calculate cumulative gdd after the planting date
weather_dap <- GDD_calculation_file[!(GDD_calculation_file$Date < planting_date), ] # remove all the days prior to planting
weather_dap$cum.GDD <- weather_dap[2,4] # cumulative starts the day after planting 

#for loop to calculate cumulative GDD
for (wdaprow in 3:nrow(weather_dap)) {
  weather_dap[wdaprow,5] <- (weather_dap[(wdaprow -1),5] + weather_dap[wdaprow,4])
}

#add removed days back on
GDD_calculation_file$cum.GDD <- 0
weather_temp <- GDD_calculation_file[!(GDD_calculation_file$Date >= planting_date), ]
weather_tot <- rbind(weather_temp,weather_dap)

# calculate cumulative rainfall for growing season
weatherWRain <- unique(merge(weather_tot, weather_rain[, c("Date", "RainfallTotal")], by = "Date"))
temp_rain <- weatherWRain[!(weatherWRain$Date < planting_date), ]
temp_rain$cum.rain <- temp_rain[2,6]

# calculate cumulative rainfall
for (c in 3:nrow(temp_rain)) {
  temp_rain[c,7] <- (temp_rain[(c - 1), 7] + temp_rain[c,6])
}

weatherWRainTemp <- weatherWRain[!(weatherWRain$Date >= planting_date), ]
weatherWRainTemp$cum.rain <- 0
temp_rain <- rbind(weatherWRainTemp, temp_rain)

# CHANGE THE FLIGHT DAY TO GROWING DEGREE DAYS (THE )
Full_temp_GDD <- data.frame()
Full_temp_GDD <- merge(weather_rain, temp_rain[,c("Date", "cum.GDD", "cum.rain")], by = "Date")
Full_GDD <- Full_temp_GDD 

# subset the file to what we actually want
Weather_Final <- unique(Full_GDD[, c("RainfallTotal", "cum.GDD", "cum.rain")])
Weather_Final$Date <- Weather_Final$cum.GDD
Weather_Final$cum.GDD <- NULL

# write out the file to be used in later script
write.csv(Weather_Final, file = paste0("/Users/dorothykirsch/Desktop/Projects_Data/Flights/", year,"/", location, "/", field, "_", year, "/Precipitation_", year,".csv"), row.names = F, col.names = T)
```
## envirotypeR environmental data Compared to Frechet distance

  - compares the correlations between year environmental data to the average frechet distances between the same years
  - reads in:
    - FrechetDistance_within_geno_btwn_years_pairwise_PlantHeight.csv
  - exports:
    - processed_weather_envirotypeR.csv
    - envirotypeR_yearpair_correlations_vs_frechet_yearpair_average.pdf

Variable:     Definition:
T2M           Temperature at 2 Meters
T2M_MAX       Maximum Temperature at 2 Meters
T2M_MIN       Minimum Temperature at 2 Meters
PRECTOT       Precipitation Corrected (mm/day)
WS2M          Wind Speed at 2 Meters
RH2M          Relative Humidity at 2 Meters
T2MDEW        Dew/Frost Point at 2 Meters
n             Actual duration of sunshine (hour)
N             Daylight hours (hour)
RTA           Extraterrestrial radiation (MJ/m^2/day)
SRAD          Solar radiation (MJ/m^2/day)
SPV           Slope of saturation vapour pressure curve (kPa.Celsius)
VPD           Vapour pressure deficit (kPa)
ETP           Potential Evapotranspiration (mm.day)
PETP          Deficit by Precipitation (mm.day)
GDD           Growing Degree Day (oC/day)
FRUE          Effect of temperature on radiation use efficiency (from 0 to 1)
T2M_RANGE     Daily Temperature Range (oC day)

```{r weather data}
library(data.table)
library(EnvRtype)
library(ggplot2)
library(tidyr)
library(tibble)
library(dplyr)
#remotes::install_github("gcostaneto/envirotypeR",force=TRUE)
# Location information
locations <- data.frame(env.id = c('2018','2019','2020','2021'),
                       lat = c(44.993053,44.994108,44.993053,44.992647),
                       lon = c(-93.175521,-93.175521,-93.175521,-93.175533),
                       plant.date = c("2018-05-14","2019-05-30","2020-05-07","2021-05-06"),
                       harv.date = c("2018-10-15","2019-10-31","2020-10-08","2021-10-07"))

# Climate Data
weather = envirotypeR::get_weather(env.id = locations$env.id,
                                   lat = locations$lat,
                                   lon = locations$lon,
                                   start.day = locations$plant.date,
                                   end.day = locations$harv.date)

# Elevation Data
weather <- envirotypeR::get_spatial(digital.raster = terra::rast("https://raw.githubusercontent.com/gcostaneto/envirotypeR/main/inst/extdata/wc2.1_2.5m_elev.tif"),
                                    env.dataframe = weather,
                                    lat = 'LAT',
                                    lng = 'LON',
                                    env.id = 'env',
                                    name.feature = 'Elevation',
                                    merge = T ) # combine it with the original weather table

weather = unique(weather) # Reduce to unique lines (multiplied each line 170 times)

# Identify useful variables
weather_vars = c("T2M",
                 "T2M_MAX",
                 "T2M_MIN",
                 "PRECTOT",
                 "WS2M",
                 "RH2M",
                 "T2MDEW",
                 "n",
                 "N",
                 "RTA",
                 "SPV",
                 "VPD",
                 "ETP",
                 "PETP",
                 "GDD",
                 "FRUE",
                 "T2M_RANGE")

# process the weather data
proc_weather = EnvRtype::processWTH(weather) %>%
  dplyr::select(env, LON, LAT, YYYYMMDD, daysFromStart, weather_vars) %>%
  left_join(weather %>%
              dplyr::select(env, Elevation) %>%
              unique(),
            by = 'env') %>%
  filter(daysFromStart <= 90)

proc_weather %>%
   write_csv('/Users/dorothykirsch/Desktop/Projects_Data/Flights/All/processed_weather_envirotypeR.csv')

weat.2018 <- proc_weather %>%
  filter(env == "2018") %>%
  dplyr::select(-c(LON,LAT,YYYYMMDD,env)) %>%
  cluster::daisy()

weat.2019 <- proc_weather %>%
  filter(env == "2019") %>%
  dplyr::select(-c(LON,LAT,YYYYMMDD,env)) %>%
  cluster::daisy()

weat.2020 <- proc_weather %>%
  filter(env == "2020") %>%
  dplyr::select(-c(LON,LAT,YYYYMMDD,env)) %>%
  cluster::daisy()

weat.2021 <- proc_weather %>%
  filter(env == "2021") %>%
  dplyr::select(-c(LON,LAT,YYYYMMDD,env)) %>%
  cluster::daisy()

r1 <- ade4::mantel.rtest(weat.2018,weat.2019, nrepet = 99) # 0.9411235
r2 <- ade4::mantel.rtest(weat.2018,weat.2020, nrepet = 99) # 0.9261137
r3 <- ade4::mantel.rtest(weat.2018,weat.2021, nrepet = 99) # 0.9395039
r4 <- ade4::mantel.rtest(weat.2020,weat.2019, nrepet = 99) #0.9398609
r5 <- ade4::mantel.rtest(weat.2021,weat.2019, nrepet = 99) # 0.9452872
r6 <- ade4::mantel.rtest(weat.2020,weat.2021, nrepet = 99) # 0.9459862

# compile results into data frame
mantel.results <- data.frame(Year.pair = c("X2018_2019","X2018_2020","X2018_2021","X2019_2020","X2019_2021","X2020_2021"),
                             r = c(r1$obs, r2$obs, r3$obs, r4$obs, r5$obs,r6$obs))

## FRECHET CORRELATION ##

# compare to average Frechet distance across year pairings
frechet.results <- (read.csv("/Users/dorothykirsch/Desktop/Projects_Data/Flights/All/Frechets/FrechetDistance_within_geno_btwn_years_pairwise_PlantHeight.csv", row.names = 1)) %>%
  #dplyr::select(-Genotype) %>%
  #(average = colMeans(., na.rm = T)) 
  add_row(Genotype = 'mean', !!! colMeans(.[-1], na.rm = T)) %>%
  filter(Genotype == "mean") %>%
  pivot_longer(cols = 2:7, names_to = "Year.pair", values_to = "Frechet") %>%
  dplyr::select(-Genotype) %>%
  left_join(mantel.results, by = "Year.pair")
  
pdf("/Users/dorothykirsch/Desktop/Projects_Data/Flights/All/envirotypeR_yearpair_correlations_vs_frechet_yearpair_average.pdf", width = 4, height = 3)
frechet.results %>%
  separate(Year.pair, into = c(NA, "Year.pair"), sep = "X") %>%
ggplot(aes(x = r, y = Frechet)) +
  geom_point(aes(color = Year.pair)) +
  geom_smooth(method = "lm", se = F) +
  theme_light() +
  xlab("Environmental Correlation") +
  ylab("Average Fréchet Distance") +
  guides(color=guide_legend(title="Year Pairing")) +
  ggtitle(paste0("r = ",round(cor(frechet.results$r,frechet.results$Frechet, method = "pearson"),2)))
dev.off()

## FCM CLUSTER CORRELATION ##
# FCM.results <- read.csv("/Users/dorothykirsch/Desktop/Projects_Data/Flights/All/FCM_Cluster_Data/Averaged_FCM_3Clusters_FitEach_All_DataTypes_Years.csv", row.names = 1) %>%
#   dplyr::select(- contains("Canopy")) %>%
#   pivot_longer(cols = 2:13, names_to = "Cluster", values_to = "fitness") %>%
#   separate(Cluster, into = c(NA,"Year",NA,"Cluster"), sep = "\\.") %>%
#   group_by(Genotypes,Year) %>%
#   slice(which.max(fitness)) %>%
#   ungroup() %>%
#   mutate(Group = fifelse(((Year==2018&Cluster==1)|(Year==2019&Cluster==3)|(Year==2020&Cluster==3)|(Year==2021&Cluster==3)),"S.t.T",
#                          fifelse(((Year==2018&Cluster==2)|(Year==2019&Cluster==1)|(Year==2020&Cluster==2)|(Year==2021&Cluster==2)),"T.t.T","S.t.S"))) %>%
#   dplyr::select(Genotypes,Year,Group) %>%
# 
#   #mutate(Geno.id = 1:nrow(.)) %>%
#   pivot_wider(names_from = Year, values_from = Group) %>%
#   summarise(X2018.2019 = sum(`2018`==`2019`), X2018.2020 = sum(`2018`==`2020`))
#   #group_by(Genotypes) %>%
#   mutate(X2018.2019 = n(`2018`==`2019`))
#   
#   FCM.overlap <- data.frame(Year.pair = c("X2018_2019","X2018_2020","X2018_2021","X2019_2020","X2019_2021","X2020_2021"),
#                             FCM.overlap = c(FCM.results %>%filter(`2018` == `2019`) %>% summarise(count = n()) %>% pull(),
#                                             FCM.results %>%filter(`2018` == `2020`) %>% summarise(count = n()) %>% pull(),
#                                             FCM.results %>%filter(`2018` == `2021`) %>% summarise(count = n()) %>% pull(),
#                                             FCM.results %>%filter(`2019` == `2020`) %>% summarise(count = n()) %>% pull(),
#                                             FCM.results %>%filter(`2019` == `2021`) %>% summarise(count = n()) %>% pull(),
#                                             FCM.results %>%filter(`2020` == `2021`) %>% summarise(count = n()) %>% pull())) %>%
#     left_join(mantel.results, by = "Year.pair")
#   
#   
# ggplot(FCM.overlap, aes(x = r, y = FCM.overlap, color = Year.pair)) +
#   geom_point() +
#   geom_smooth() +
#   xlab("Environmental Correlation") +
#   ylab("Number of Genotypes in the Same Cluster") +
#   ggtitle(cor(FCM.overlap$r,FCM.overlap$FCM.overlap, method = "pearson"))
```
## Make visual of Dates to GDD

   - plots gdd accumulation for each year
   
```{r}
library(tidyverse)
############################### CHANGE THESE ################################
Path <- "/Users/dorothykirsch/Desktop/Projects_Data/Flights/"
Year <- c("2018","2019","2020","2021")
Location <- c("X3","X5","X3","X2") 
#############################################################################
All_year <- data.frame()
for( y in c(1:4)) {
  # read in the date to gdd file
  data <- read.csv(paste0(Path,Year[y],"/SaintPaul/",Location[y],"_",Year[y],"/GDD_Accumulation_",Year[y],".csv"), sep = ",", colClasses = 'character')
  # add a year column
  data$Year <- Year[y]
  # reformat date column
  data$Date <- format(as.Date(data$Date, format = "%m%d%Y"), "%m/%d/%Y")
  # cut to May - August
  data <- subset(data, Date> paste0("04/30/",Year[y]) & Date < paste0("09/01/",Year[y]))
  # remove Year from date
  data$Date <- format(as.Date(data$Date, format = "%m/%d/%Y"), "%m/%d")
  # add to All_year
  All_year <- rbind(All_year,data)
}

pdf("/Users/dorothykirsch/Desktop/Projects_Data/Flights/All/Date_to_GDD_StPaul_All.pdf", height = 3, width = 6)

plot <- ggplot(All_year, aes(x = Date, y = as.numeric(cum.GDD), group = as.character(Year) )) +
  geom_point(aes(color = as.character(Year))) +
  geom_line(aes(color = as.character(Year))) + 
  # geom_hline(aes(yintercept = 100))+
  # geom_text(aes(118,100,label = 100, vjust = -1)) +
  # geom_hline(aes(yintercept = 500))+
  # geom_text(aes(118,500,label = 500, vjust = -1)) +
  # geom_hline(aes(yintercept = 1000))+
  # geom_text(aes(118,1000,label = 1000, vjust = -1)) +
  # geom_hline(aes(yintercept = 1450))+
  # geom_text(aes(118,1450,label = 1450, vjust = -1)) +
  theme_classic() +
  xlab("Date") +
  ylab("Accumulation of Growing Degree Days") +
  scale_x_discrete(labels = c("May","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","","", "September")) +
  theme(
    axis.ticks.x = element_blank()
  ) +
  guides(color = guide_legend(title = "Year"))

print(plot)
dev.off()
```

## Make visual of Plant Height Ranges

  - plots the minimum and maximum plant height values for each gdd and each year

```{r}
library(tidyverse)

# read in file
plant.data <- read.delim("/Users/dorothykirsch/Desktop/Projects_Data/Flights/All/All_environment_GDD_Loess_Predictions.txt")

 range.all <- data.frame()

# All years
data.years <- c("2018","2019","2020","2021")
# for loop to go through years
for (y in data.years) {
  # filter out individual year
 plant.temp <- plant.data %>%
  filter(Year == y)
 # for loop to go through all GDD
 for (g in 10:ncol(plant.temp)) {
   # find the minimum and maximum
   range.temp <- data.frame("GDD" = colnames(plant.data)[g],
                            "Minimum" = min(plant.temp[,g], na.rm = T),
                            "Maximum" = max(plant.temp[,g], na.rm = T),
                            "YEAR" = y)
   # add to all
   range.all <- rbind(range.all, range.temp)
 }
}
# Plot and export the Ranges 
pdf("/Users/dorothykirsch/Desktop/Projects_Data/Flights/All/PlantHeight_Range_AllYears.pdf", height = 4, width = 6.5)

plot <- range.all %>%
  separate(GDD, c("X", "GDD"), sep = "X") %>%
  pivot_longer(cols = c(3:4), names_to = "Min.Max", values_to = "Height") %>%
  ggplot(aes(x = as.numeric(GDD), y = Height, color = YEAR, shape = Min.Max)) +
  theme_classic() +
  geom_point() +
  geom_line() +
  xlab("Growing Degree Days") +
  labs(color = "Year", shape = "Extrema")
print(plot)

dev.off()

```

## Make visual of Precipitation Accumulation

  - plots accumulation of precipitation for each year over the growing season
  
```{r}
library(tidyverse)
######### INPUTS ##########
Path <- "/Users/dorothykirsch/Desktop/Projects_Data/Flights/"
Year <- c("2018","2019","2020","2021")
Field <- c("X3","X5","X3","X2")
###########################
# initiate data.frame to put all precipitation data
all.precip <- data.frame()
# set up for loop to go through years
for(y in 1:length(Year)) {
  year <- Year[y]
  # read in precipitation data
  indiv.precip <- read.csv(paste0(Path, year, "/SaintPaul/",Field[y],"_",year,"/Precipitation_",year,".csv"))
  # add a column with the year
  indiv.precip$Year <- year
  # add to all.precip
  all.precip <- rbind(all.precip,indiv.precip)
}
# plot the accumulation 
pdf(paste0(Path,"All/Precipiation_Accumulation_Over_All_Years.pdf"))
temp_plot <- ggplot(all.precip, aes(x = as.numeric(Date), y = cum.rain, group = Year, color = Year)) +
  theme_light() +
  geom_line() +
  ylim(0,11) +
  xlab("Growing Degree Days") +
  ylab("Cumulative Rainfall (inches)") +
  scale_x_continuous(limits = c(0,1500), breaks = seq(0,1500,by = 100))
print(temp_plot)
dev.off()
```

## Make visual of all growth curves with growth phases

  - plots the loess growth curves over growing season for each year with exponential growth phase, lag phase, and terminal height marked. 
  
```{r}
library(tidyverse)
# Set the path
Path <- "/Users/dorothykirsch/Desktop/Projects_Data/Flights/"
# Read in GDD Loess curve data
Heights <- read.delim(paste0(Path,"All/All_environment_GDD_Loess_Predictions.txt"))

# Make a long format and reformat the GDD column
Heights <- Heights %>%
  pivot_longer(cols = 10:36, names_to = "GDD", values_to = "Height") %>%
  separate(col = GDD, into = c(NA, "GDD"), sep = "X", remove = T)

# Make a dataframe with information about the phases of growth
Phases <- data.frame(Start = c(150,500,1400,150,500,1400,150,650,1400,150,800,1400),
                     End = c(500, 1400, 1450,500, 1400, 1450,650, 1400, 1450,800, 1400, 1450),
                     Phase = c("Lag Phase", "Exponential Phase", "Terminal Height","Lag Phase", "Exponential Phase", "Terminal Height","Lag Phase", "Exponential Phase", "Terminal Height","Lag Phase", "Exponential Phase", "Terminal Height"),
                     Year = c("2018","2018","2018","2019","2019","2019","2020","2020","2020","2021","2021","2021"))

# Relevel the factors
Phases$Phase <- factor(Phases$Phase, levels = c("Lag Phase", "Exponential Phase","Terminal Height"))

# Plot and export
pdf(paste0(Path, "All/All_loess_curves_w_growth_phases.pdf"), height = 6, width = 5)

temp.plot <- ggplot() +
  geom_line(data = Heights, aes(x = as.numeric(GDD), y = Height, group = Plot)) +
  geom_rect(data = Phases, aes(xmin = Start, xmax = End, fill = Phase, ymin = -Inf, ymax = Inf, alpha = 1)) +
  scale_fill_manual(values = c("#E4AC05","#05E4AC","#AC05E4")) +
  theme_light() +
  scale_x_continuous(name = "Growing Degree Days", breaks = seq(from = 150, to = 1450, by = 100)) +
  theme(
    axis.text.x = element_text(angle = 90)
  ) +
  guides(alpha = "none") +
  facet_wrap(~ Year, nrow = 4) +
  theme(legend.position = "bottom",
        legend.title = element_blank())

print(temp.plot)

dev.off()
              
##### Canopy Cover #####
# Set the path
Path <- "/Users/dorothykirsch/Desktop/Projects_Data/Flights/"
# Read in GDD Loess curve data
Canopy <- read.csv(paste0(Path,"All/All_Year_Canopy_Loess_Data.csv"))

# Make a long format and reformat the GDD column
Canopy <- Canopy %>%
  pivot_longer(cols = 2:29, names_to = "GDD", values_to = "Cover") %>%
  separate(col = GDD, into = c(NA, "GDD"), sep = "X", remove = T) %>%
  filter(as.numeric(GDD) < 1500)

# Make a dataframe with information about the phases of growth
Phases <- data.frame(Start = c(150,450,1400,150,700,1400,150,400,1400,150,450,1400),
                     End = c(450, 1400, 1450,700, 1400, 1450,400, 1400, 1450,450, 1400, 1450),
                     Phase = c("Lag Phase", "Exponential Phase", "Terminal Height","Lag Phase", "Exponential Phase", "Terminal Height","Lag Phase", "Exponential Phase", "Terminal Height","Lag Phase", "Exponential Phase", "Terminal Height"),
                     Year = c("2018","2018","2018","2019","2019","2019","2020","2020","2020","2021","2021","2021"))

# Relevel the factors
Phases$Phase <- factor(Phases$Phase, levels = c("Lag Phase", "Exponential Phase","Terminal Height"))

# Plot and export
pdf(paste0(Path, "All/All_loess_curves_w_growth_phases_canopy.pdf"), height = 6, width = 5)

temp.plot <- ggplot() +
  geom_line(data = Canopy, aes(x = as.numeric(GDD), y = Cover, group = Plot)) +
  geom_rect(data = Phases, aes(xmin = Start, xmax = End, fill = Phase, ymin = -Inf, ymax = Inf, alpha = 1)) +
  scale_fill_manual(values = c("#E4AC05","#05E4AC","#AC05E4")) +
  theme_light() +
  scale_x_continuous(name = "Growing Degree Days", breaks = seq(from = 150, to = 1450, by = 100)) +
  theme(
    axis.text.x = element_text(angle = 90)
  ) +
  guides(alpha = "none") +
  facet_wrap(~ Year, nrow = 4) +
  theme(legend.position = "bottom",
        legend.title = element_blank())  

print(temp.plot)

dev.off()
```

## Local Graph of plant height for a single plot

  - plots the growth curve of a single plot over time

```{r}
library(tidyverse)
# set path
Path <- "/Users/dorothykirsch/Desktop/Projects_Data/Flights/"
# read in data
data_2021 <- read.delim(paste0(Path,"2021/SaintPaul/X2_2021/Data_Analysis/Normalized_Genotype_Plant_Height_Data_StPaul_2021.txt"), sep = "")
# read in loess curve data
loess_2021 <- read.delim(paste0(Path,"All/All_environment_GDD_Loess_Predictions.txt"))
# read in date to gdd
date.gdd <- read.csv(paste0(Path,"2021/SaintPaul/X2_2021/Data_Analysis/Date_to_GDD_StPaul_2021.csv"))
# change the date to match
date.gdd$Date <- paste0("0",date.gdd$Date)
# filter out to only plot YC21:1025
loess_2021 <- loess_2021 %>%
  filter(Plot == "YC21:1025") %>%
  dplyr::select(-Plot,-Block,-Entry,-Rep,-Stand,-Range,-Row,-Year) %>%
  pivot_longer(cols = 2:28, names_to = "Date", values_to = "Loess") %>%
  separate(Date,into = c("Letter","Date"), sep = "X") %>%
  dplyr::select(-Letter) 

# filter out to only Plot YC21:1025 - PHN66
data_2021 <- data_2021 %>%
  filter(Plot == "YC21:1025") %>%
  dplyr::select(-Plot,-Block,-Entry,-Rep) %>%
  pivot_longer(cols = 2:12, names_to = "Date", values_to = "Raw") %>%
  separate(Date,into = c("Letter","Date"), sep = "X") %>%
  dplyr::select(-Letter)

# merge the data with the gdd data
data_2021 <- merge(data_2021, date.gdd, by = "Date")
data_2021$Date <- NULL # remove old date
data_2021$Date <- data_2021$cum.GDD # rename gdd data to date
data_2021$cum.GDD <- NULL # remove old gdd column

# combine the normalized height data and loess curve data
temp.2021 <- rbind(data_2021[,c(1,3)], loess_2021[,c(1,2)]) #rbind the Date and Genotype columns from the loess and raw 
temp.2021 <- merge(temp.2021, data_2021[,c(2,3)], all.x = T)# merge the raw height data 
temp.2021 <- merge(temp.2021, loess_2021[,c(2,3)], all.x = T)# merge the loess height data

# for loop to get rid of NAs in loess curve column
for (b in 1:nrow(temp.2021)) {
  if(is.na(temp.2021[b,4])) {
    temp.2021[b,4] <- temp.2021[b,3]
  }
}
# graph the plot growth over time
pdf(paste0(Path, "All/Growth_PHN66_2021_for_article.pdf"), height = 2, width = 4)
ggplot(temp.2021) +
  geom_point(aes(x = as.numeric(Date), y = Raw)) + 
  geom_line(aes(x = as.numeric(Date), y = Loess)) +
  theme_classic() +
  xlab("Cumulative Growing Degree Days") +
  ylab("Normalized Height") +
  scale_x_continuous(breaks=c(floor(unlist(as.numeric(data_2021$Date))))) +
  theme(
    axis.text.x = element_text(angle = 45, hjust = 0.95, size = 7),
    axis.text.y = element_text(size = 7)
  )
dev.off()

```
## Plot Manual Measurements vs Normalized Extracted
  
  - exports "Normalized_Extract2Manual",date,".pdf"

```{r}
## Clearing the global environment
rm(list=ls(all=TRUE))

library(ggplot2)
library(RColorBrewer)
######## CHANGE INPUTS ##########
Year <-c("2018","2018","2018","2018","2018","2018","2018","2018","2019","2019","2019","2019","2019","2019","2019","2019","2019","2020","2020","2020","2020","2020","2020","2020","2021","2021","2021","2021","2021","2021","2021","2021")
Location <- c("X3","X3","X3","X3","X3","X3","X3","X3","X5","X5","X5","X5","X5","X5","X5","X5","X5","X3","X3","X3","X3","X3","X3","X3","X2","X2","X2","X2","X2","X2","X2","X2")
Path = "/Users/dorothykirsch/Desktop/Projects_Data/Flights/"
date = c("06052018","06132018","06282018","07092018","07172018","07242018","07312018","08092018","06132019","06172019","06252019","07022019","07082019","07152019","07222019","07312019","08082019","06032020","06082020","06162020","06232020","07012020","07222020","07292020","06072021","06152021","06232021","06282021","07082021","07132021","07232021","08062021")
#################################
# initiate combined file
combined.all <- data.frame(matrix(nrow = 0, ncol = 4))
colnames(combined.all) <- c("Plot","Mean","Extracted","Date")
# for loop to go through each iterations
for (a in 1:length(Year)) {
  # reading in the normalized extracted heights from the flights 
  if (Year[a] == "2020" | Year[a] == "2021") {
    data.ex <- read.delim(paste0(Path,Year[a],"/SaintPaul/",Location[a],"_",Year[a],"/Data_Analysis/Normalized_Genotype_Plant_Height_Data_StPaul_",Year[a],".txt"), sep = " ")
  } else if (Year[a] == "2018" | Year[a] == "2019") {
    data.ex <- read.csv(paste0(Path,Year[a],"/SaintPaul/",Location[a],"_",Year[a],"/Data_Analysis/UAVHeight_",Year[a],".csv"))
  }
  # select the normalized data for only the date of interest
  data.ex <- data.ex %>%
    dplyr::select(Plot, paste0("X",date[a]))
  # read in the manual height measurements
  data.man <-read.csv(paste0(Path,Year[a],"/SaintPaul/",Location[a],"_",Year[a],"/manual_measurements/",date[a],".csv"), header=TRUE)
  # calculate the average plant height per plot
  data.man$Mean <- (data.man$Height1 + data.man$Height2 + data.man$Height3 + data.man$Height4 + data.man$Height5)/5 
  # combine the normalized extracted heights and the manual measurements
  data.all <- merge(data.ex, data.man[,c(1, 7)], by = "Plot")
  # rename the normalized extracted to extracted for generic use in plotting
  data.all$Extracted <- data.all[,2]
  # plot the normalized vs the manual
  plot <- ggplot(data.all, aes(x = Extracted, y = Mean)) +
    geom_point() +
    theme_classic() +
    xlab(paste0("Normalized Extracted Plant Height ",date[a])) +
    ylab("Manual Plant Height Measurements ") +
    geom_smooth(method = "lm", se = F) +
    ggpubr::stat_cor(label.y = max(data.all$Mean), color = 'red',size = 6, na.rm = T,aes(label = ..r.label..)) 
  # export the plot
  pdf(file = paste0(Path,Year[a],"/SaintPaul/",Location[a],"_",Year[a],"/Extracted_Manual_Plots/Normalized_Extract2Manual_",date[a],".pdf"), height = 3, width = 4)
    print(plot)
  dev.off()
    
  ## ADD ALL VALUES TO COMBINED FILE ##
  # add a date column and remove the column with date name
  data.all <- data.all %>%
    mutate(Date = paste0("X", date[a])) %>%
    dplyr::select(-paste0("X",date[a]))
  # add to larger file containing all
  combined.all <- rbind(combined.all,data.all)
}
## PLOT EXTRACTED VS HAND FOR ALL DAYS ##
plot <- combined.all %>%
  filter(grepl('2021',Date)) %>%
  separate(col=Date,into=c(NA,"Date"),sep="X") %>%
  ggplot(aes(x = Extracted, y = Mean)) +
  geom_point(aes(color = Date)) +
  theme_classic() +
  xlab("Normalized Extracted Plant Height") +
  ylab("Manual Plant Height Measurements") +
  geom_smooth(method = "lm", se = F) +
  ggpubr::stat_cor(label.y = max(data.all$Mean), color = 'red',size = 6, na.rm = T,aes(label = ..r.label..))
# export the plot
pdf(paste0(Path,Year[a],"/SaintPaul/",Location[a],"_",Year[a],"/Extracted_Manual_Plots/Normalized_Extract2Manual_all_dates_",Year[a],".pdf"), height = 3, width = 4)
  print(plot)
dev.off()
```

## Create file with terminal height information for GWAS

  - read in Final_Height_Data_Plot.txt & All_environment_GDD_Loess_Predictions.txt
  - export Terminal_Height_Data_Descript.txt
  
```{r}
library(tidyverse)

# read in the terminal height
fin.height <- read.delim("/Users/dorothykirsch/Desktop/Projects_Data/Flights/All/Final_Height_Data_Plot.txt")
# read in the plot descriptive data (all loess curve data which has the genotypes in it)
descript <- read.delim("/Users/dorothykirsch/Desktop/Projects_Data/Flights/All/All_environment_GDD_Loess_Predictions.txt")
# filter descript to just the descriptive data and no loess data
descript <- descript %>%
  dplyr::select(Plot,Genotype,Block,Rep,Entry,Stand,Range,Row,Year)
# merge descriptive data with terminal plant height data
desc.height <- merge(descript,fin.height, by = "Plot")
# print the final height file with all descriptive information
write.table(desc.height, file = "/Users/dorothykirsch/Desktop/Projects_Data/Flights/All/Terminal_Height_Data_Descript.txt", sep = "\t")

```

# Scripts to run GWAS

  - Written to run on MSI
  - All R scripts in this section used R v 4.0.4
  - Most scripts have a bash script associated with them to submit the script
  
## Running_BLUP_Extraction_Steps.sh

  - bash script to submit BLUP_Extraction_Steps.R

```{bash}
#!/bin/bash
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=1
#SBATCH --cpus-per-task=1
#SBATCH --mem=100gb
#SBATCH --time=10:00:00
#SBATCH --mail-type=ALL
#SBATCH --mail-user=kirsc168@umn.edu
#SBATCH -o /home/hirschc1/kirsc168/GWAS_project/%j.out
#SBATCH -e /home/hirschc1/kirsc168/GWAS_project/%j.err

cd /home/hirschc1/kirsc168/GWAS_project/

# Load R
module load R/3.6.3

# Running Script
Rscript --max-ppsize=500000 BLUPS_Extraction_Steps.R
```

### BLUPS_Extraction_Steps.R

  - calculates BLUPs for each GDD and each slope between GDDs in each year separately for use in GWAS
  
```{r}
# BLUPS_Extraction_Steps.R
library(lme4)
library(tidyverse)
library(dplyr)
library(data.table)

Years <- c("2018", "2019", "2020", "2021")
## Set working directory ##
setwd("/Users/dorothykirsch/Desktop/Projects_Data/Flights/All/")

## Read in files ##
GDD <- fread("All_environment_GDD_Loess_Predictions.txt")
GDD <- as.data.frame(GDD)
GDD$V1 <- NULL
# change things to factors
GDD$Genotype <- as.factor(GDD$Genotype)
GDD$Block <- as.factor(GDD$Genotype)
GDD$Rep <- as.factor(GDD$Rep)
GDD$Entry <- as.factor(GDD$Entry)
GDD$Range <- as.factor(GDD$Range)
GDD$Row <- as.factor(GDD$Row)
GDD$Year <- as.factor(GDD$Year)

interval <- fread("All_environment_Interval_Loess_Predictions.txt")
interval <- as.data.frame(interval, rownames = 1)
interval$V1 <- NULL
# change things to factors
interval$Genotype <- as.factor(interval$Genotype)
interval$Block <- as.factor(interval$Genotype)
interval$Rep <- as.factor(interval$Rep)
interval$Entry <- as.factor(interval$Entry)
interval$Range <- as.factor(interval$Range)
interval$Row <- as.factor(interval$Row)
interval$Year <- as.factor(interval$Year)

terminal <- fread("Terminal_Height_Data_Descript.txt")
terminal <- as.data.frame(terminal)
terminal$V1 <- NULL
# change things to factors
terminal$Genotype <- as.factor(terminal$Genotype)
terminal$Block <- as.factor(terminal$Genotype)
terminal$Rep <- as.factor(terminal$Rep)
terminal$Entry <- as.factor(terminal$Entry)
terminal$Range <- as.factor(terminal$Range)
terminal$Row <- as.factor(terminal$Row)
terminal$Year <- as.factor(terminal$Year)

## Find blups for the GDD ##

for(a in Years){
  GDD.temp <- GDD %>%
    filter(Year == a)

  # initialize a vector to put rows to remove in
  All.env.bad.rows <- vector()

  # remove any rows with only NAs
  for (Env.rows in 1:nrow(GDD.temp)) {
    if (all(is.na(GDD.temp[Env.rows,11:(ncol(GDD.temp)-1)]))) {
      All.env.bad.rows <- append(All.env.bad.rows,Env.rows)
    }
  }

  GDD.temp <- GDD.temp[-All.env.bad.rows,]

  Grow.days <- colnames(GDD.temp[11:ncol(GDD.temp)])

  for (Grow.day in Grow.days) {
    Model <- lmer(data = GDD.temp, get(Grow.day) ~ (1|Genotype) + (1|Rep/Block), REML = TRUE)
    BLUPS <- ranef(Model)$Genotype
    Usable_BLUPs_GDD <- tibble(taxa = rownames(BLUPS), BLUPS = BLUPS[[1]])
    write.table(Usable_BLUPs_GDD, file = paste0("BLUPS/BLUPs_GDD_",a,"_", Grow.day, ".txt"), sep = "\t", row.names = FALSE)
  }

}

print("Done with GDD")

for(a in Years){
  interval.temp <- interval %>%
    filter(Year == a)

  # initialize a vector to put rows to remove in
  All.env.bad.rows <- vector()

  # remove any rows with only NAs
  for (Env.rows in 1:nrow(interval.temp)) {
    if (all(is.na(interval.temp[Env.rows,11:(ncol(interval.temp)-1)]))) {
      All.env.bad.rows <- append(All.env.bad.rows,Env.rows)
    }
  }

  interval.temp <- interval.temp[-All.env.bad.rows,]

  Grow.days <- colnames(interval.temp[11:ncol(interval.temp)])

  for (Grow.day in Grow.days) {
    Model <- lmer(data = interval.temp, get(Grow.day) ~ (1|Genotype) + (1|Rep/Block), REML = TRUE)
    BLUPS <- ranef(Model)$Genotype
    Usable_BLUPs_interval <- tibble(taxa = rownames(BLUPS), BLUPS = BLUPS[[1]])
    write.table(Usable_BLUPs_interval, file = paste0("BLUPS/BLUPs_interval_",a,"_", Grow.day, ".txt"), sep = "\t", row.names = FALSE)
  }

}

print("Done with Interval")

# Find BLUPS for the terminal height for each year
# filter for just year
for(a in Years){
  terminal.temp <- terminal %>%
    filter(Year == a)
  # initialize a vector to put rows to remove in
  All.env.bad.rows <- vector()
  # find plots that need to be removed
  for (b in 1:nrow(terminal.temp)) {
    if (is.na(terminal.temp[b,ncol(terminal.temp)])) {
      All.env.bad.rows <- append(All.env.bad.rows,b)
    }
  }
  # remove the NA plots
  terminal.temp <- terminal.temp[-All.env.bad.rows,]
  # complete the model and write the BLUPS
  Model <- lmer(data = terminal.temp, Final ~ (1|Genotype) + (1|Rep/Block), REML = TRUE)
  BLUPS <- ranef(Model)$Genotype
  Usable_BLUPs_GDD <- tibble(taxa = rownames(BLUPS), BLUPS = BLUPS[[1]])
  write.table(Usable_BLUPs_GDD, file = paste0("BLUPS/BLUPs_Terminal_",a,".txt"), sep = "\t", row.names = FALSE)
  
  print(paste0("Done with ", a))
}

```
## Running_Pruning_SNPs_by_LD.sh

  - if tassel is not installed, install it first: git clone https://bitbucket.org/tasseladmin/tassel-5-standalone.git
  - uses plink to cut down hapmap file
  - Cuts down christine_common_final.hmp.txt (found in  Qiu, et al. 2021. “Whole-Genome Variation of Transposable Element Insertions in a Maize Diversity Panel.” G3  11 (10). https://doi.org/10.1093/g3journal/jkab238.) to Pruned_final_hapmap
  - Pruned... contains only SNPs not in LD to be used in all future GWAS analysis

```{bash}
#!/bin/bash
#SBATCH --time=2:00:00
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=4
#SBATCH --mem=40gb
#SBATCH -J prune_marker_data_ld
#SBATCH -o /home/hirschc1/kirsc168/GWAS_project/%j.out
#SBATCH -e /home/hirschc1/kirsc168/GWAS_project/%j.err
#SBATCH --mail-type=ALL
#SBATCH --mail-user=kirsc168@umn.edu
#SBATCH --no-requeue

### load the module
module load plink/1.90b6.10

# go to project folder
cd /home/hirschc1/kirsc168/GWAS_project/

# transform hmp to plk
run_pipeline.pl -Xmx40g -importGuess christine_common_final.hmp.txt \
                -export Hapmap_2_plink \
                -exportType Plink

# prune plink file by ld
plink --file Hapmap_2_plink.plk \
      --indep-pairwise 10 kb 1 0.9 \
      --geno 0.25 \
      --out Plink_intermediate_hapmap \
      --allow-extra-chr \
      --make-founders

# filter hapmap
run_pipeline.pl -Xmx40g -importGuess christine_common_final.hmp.txt \
                -includeSiteNamesInFile Plink_intermediate_hapmap.prune.in \
                -export Pruned_final_hapmap \
                -exportType HapmapDiploid
```

## Running_Numerical_GAPIT.sh

  - bash script to submit Numerical_GAPIT.R
  
```{bash}
#!/bin/bash
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=1
#SBATCH --cpus-per-task=1
#SBATCH --mem=100gb
#SBATCH --time=40:00:00
#SBATCH --mail-type=ALL
#SBATCH --mail-user=kirsc168@umn.edu
#SBATCH -o /home/hirschc1/kirsc168/GWAS_project/%j.out
#SBATCH -e /home/hirschc1/kirsc168/GWAS_project/%j.err

cd /home/hirschc1/kirsc168/GWAS_project/

# Load R
module load R/4.0.4

# Running Script
Rscript --max-ppsize=500000 Numerical_GAPIT.R
```

### Numerical_GAPIT.R

  - converts hapmap file into numerical format

```{r}
### Numerical GAPIT 
### Michael Burns
### 11/12/2020

#####################
# Loading Libraries #
#####################
library("multtest")
library("snpStats")
library("gplots")
library("LDheatmap")
library("genetics")
library("ape")
library("EMMREML")
library("compiler")
library("scatterplot3d")
library("bigmemory")
library("biganalytics")
library("tidyverse")
library("data.table")

##############################
# Sourcing GAPIT and FarmCPU #
##############################
source("http://zzlab.net/GAPIT/emma.txt")
source("http://zzlab.net/GAPIT/gapit_functions.txt")
source("http://zzlab.net/FarmCPU/FarmCPU_functions.txt")

#########################
# Set Working Directory #
#########################
setwd("/home/hirschc1/kirsc168/GWAS_project/")

################
# Loading Data #
################
myG <- read.table("Pruned_final_hapmap.hmp.txt", sep = "\t", header = F) # was originally christine_common_final.hmp.txt but changed after pruned

############################################################
# Running Gapit for Conversion of HapMap to Numeric Format #
############################################################
print("Starting GAPIT")

myGAPIT <- GAPIT(
  G = myG,
  output.numerical=T,
  PCA.total = 5
)

print("Finished GAPIT")

print("El Fin")

```

## Creating GD_Taxa_List.txt

  - bash code written to create a taxa list from the output of previous script (GAPIT.Genotype.Numerical.txt)

```{bash}
cut -f1 GAPIT.Genotype.Numerical.txt > GD_Taxa_List.txt
```

## List all BLUPs in folder

  - bash code to create a list of BLUPs in the BLUPS folder
  
```{bash}
ls BLUPS/ > listmyfolderBLUPS.txt
```
## Running_Splitting_BLUPs_by_Env.sh

  - bash script to run Splitting_BLUPs_by_Env.R
  - 216 iterations for all GDD and slopes between GDDs in all years
  - how to submit jobs for each iteration in parallel: for chr in {1..216}; do sbatch --export=BLUPIT=${chr} Running_Splitting_BLUPs_by_Env.sh; done

```{bash}
#!/bin/bash
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=1
#SBATCH --cpus-per-task=1
#SBATCH --mem=100gb
#SBATCH --time=10:00:00
#SBATCH --mail-type=ALL
#SBATCH --mail-user=kirsc168@umn.edu
#SBATCH -o /home/hirschc1/kirsc168/GWAS_project/%j.out
#SBATCH -e /home/hirschc1/kirsc168/GWAS_project/%j.err

cd /home/hirschc1/kirsc168/GWAS_project/

# Load R
module load R/4.0.4

# Running Script
echo ${BLUPIT}
Rscript --max-ppsize=500000 Splitting_BLUPs_by_Env.R ${BLUPIT}
```

### Splitting_BLUPs_by_Env.R

  - match BLUPs and genomic data

```{r}
## Splitting_BLUPs_by_Env.R
## From Michael
cli_arg <- commandArgs(trailingOnly = TRUE)
BLUPIT <- as.numeric(cli_arg[1])
print(BLUPIT)

##################
# Load Libraries #
##################
library(tidyverse)
library(lme4)
library(magrittr)

########################
# Setting memory limit #
########################
memory.limit()
memory.limit(size = 35000)

#########################
# Set Working Directory #
#########################
setwd("/home/hirschc1/kirsc168/GWAS_project/") # Only for use in MSI

################
# Loading Data #
################
blupFilesList <- read.delim("listmyfolderBLUPS.txt",head=F)

myG <- read.table("GD_Taxa_List.txt", head = T)
Env_1_BLUPs <-read.table(paste0("BLUPS/", blupFilesList[BLUPIT,1]),head=T)

#######################################
# List of Genotypes with Genomic Data #
#######################################
genotypes <- myG$taxa

#########################################
# Matching BLUP and Genomic Information #
#########################################
Env_1_Y <- Env_1_BLUPs %>%
  filter(taxa %in% genotypes)

dim(Env_1_Y)

genos_1 <- tibble(taxa = genotypes) %>%
  filter(taxa %in% Env_1_Y$taxa)

dim(genos_1)

Env_1_Y_Match <- genos_1 %>%
  full_join(Env_1_Y)

dim(Env_1_Y_Match)
sum(Env_1_Y_Match$taxa == genos_1$taxa)

#########################
# Write Out Subset Data #
#########################\
newFile = strsplit(blupFilesList[BLUPIT,1], split = ".", fixed = T)[[1]][1]

### Phenotypic BLUP Data ###
write_csv(Env_1_Y_Match,paste0("MyY_Splitting_Env/MyY_Env_", newFile, ".csv"))

```

## Running_GWAS_Env_Splitter.sh

  - bash script that is submitted to run GWAS_Env_Splitter.pl

```{bash}
#!/bin/bash
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=1
#SBATCH --cpus-per-task=1
#SBATCH --mem=100gb
#SBATCH --time=48:00:00
#SBATCH --mail-type=ALL
#SBATCH --mail-user=kirsc168@umn.edu
#SBATCH -o /home/hirschc1/kirsc168/GWAS_project/%j.out
#SBATCH -e /home/hirschc1/kirsc168/GWAS_project/%j.err

cd /home/hirschc1/kirsc168/GWAS_project/

# Running Script
perl GWAS_Env_Splitter.pl
```

### GWAS_Env_Splitter.pl

  - extracts the genotypes found in the taxa list for each iteration from the numerical hapmap data

```{bash}
#!/bin/perl
### GWAS_Env_Splitter.pl
### Michael Burns
### 2/22/21

# The purpose of this code is to read in two files (environment taxa and numericalized hapmap dataset) and extract from the hapmap data the genotypes found in the environment taxa list

#################
# Starter Stuff #
#################
use strict;
use warnings;
use Getopt::Std;
use Data::Dumper;

opendir my $directfiles, "MyY_Splitting_Env" or die "Cannot open directory: $!";
my @files = readdir $directfiles;


foreach(@files) {
print;
print "\n";
}

my $i; # Initialize iteration variable
foreach my $file (@files) {

        if($file eq ".")
        {
                next;
        }elsif($file eq "..")
        {
                next;
        }else{
	print "Currently Working on Environment $file\n"; # Tell me which environment is in progress

        ######################
        # Opening File Paths #
        ######################
        open(my $Env_in_fh, '<',"MyY_Splitting_Env/${file}") or die("Env Dataset Couldn't be Found\n"); # Open path to environment taxa
        open(my $GD_in_fh, '<', "GAPIT.Genotype.Numerical.txt") or die("GD Dataset Couldn't be Found\n"); # Open path to genotypic dataset -- Change this later!


        my ($sc1, $sc2, $sc3, $sc4, $sc5, $sc6) = split('_', $file); # split the name of the MyY file so parts can be reused


        open(my $Env_out_fh, '>', "GD_Env/GD_Env_${sc4}_${sc5}_${sc6}.txt") or die("Output Dataset Couldn't be Found\n"); # Open path to output file

        ###################
        # Populating Hash #
        ###################
        my %env_hash; # Initialize hash
        while(my $line = <$GD_in_fh>){ # While there are lines in the genotype dataset...
                chomp $line; # Remove tailing \n
                my @fields = split('\t', $line); # Split first line into separate elements by tabs
                my $taxa = $fields[0]; # Extract the first column (taxa) from the genotypic dataset
                my $num_cols = scalar @fields-1; # This should count the number of columns present, and subtract one. This is important since the indexing in perl starts at 0.
                my $geno = join "\t", @fields[1..$num_cols]; # Rejoin all of the genotypic data columns -- This will not throw an error if the wrong number of fields are selected, so be careful!
                $env_hash{$taxa} = $geno; # Add taxa key and genotypic values to hash -- all genotypic data needs to be in one element
        }
print Dumper(\%env_hash);

        ########################################################
        # Matching Keys and Values in Environment Taxa Dataset #
        ########################################################
        while(my $key = <$Env_in_fh>){ # While there are lines in environment taxa dataset...
                chomp $key; # Remove tailing \n
                my @fields = split(',', $key);
                my $taxa_key = $fields[0];
                print $Env_out_fh $taxa_key, "\t", $env_hash{$taxa_key}, "\n"; # Print matches in order of Key    Numerical Data \n
        }

	####################
        # Close File Paths #
        ####################
        close $Env_in_fh;
        close $GD_in_fh;
        close $Env_out_fh;
}
}

closedir $directfiles;
```

## Running_p_value_GAPIT.sh

  - bash script to run p_value_GAPIT.R
  - Has to run for each iteration, uses IterationsForPValue.txt to submit the scripts in parallel
  - How to submit all iterations, copy and paste into command line:
  
```{bash}
# remove trailing \r
sed -i 's/\r$//' IterationsForPValue.txt 
while read -r line; do sbatch --export=iter=${line} Running_p_value_GAPIT.sh;done < "IterationsForPValue.txt"
```

  - actual bash script that will submit p_value_GAPIT.R for each iteration

```{bash}
#!/bin/bash
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=1
#SBATCH --cpus-per-task=1
#SBATCH --mem=100gb
#SBATCH --time=6:30:00
#SBATCH --mail-type=ALL
#SBATCH --mail-user=kirsc168@umn.edu
#SBATCH -o /home/hirschc1/kirsc168/GWAS_project/%j.out
#SBATCH -e /home/hirschc1/kirsc168/GWAS_project/%j.err

cd /home/hirschc1/kirsc168/GWAS_project/

# Load R
module load R/4.0.4

# Running Script
Rscript --max-ppsize=500000 p_value_GAPIT.R ${iter}
```

### p_value_GAPIT.R

  - calculates the p value for each iteration

```{r}
### p_value_GAPIT.R
### Michael Burns
### 11/12/2020

#####################
# Loading Libraries #
#####################
library("multtest")
library("snpStats")
library("gplots")
library("LDheatmap")
library("genetics")
library("ape")
library("EMMREML")
library("compiler")
library("scatterplot3d")
library("bigmemory")
library("biganalytics")

##############################
# Sourcing GAPIT and FarmCPU #
##############################
source("http://zzlab.net/GAPIT/emma.txt")
source("http://zzlab.net/GAPIT/gapit_functions.txt")
source("http://zzlab.net/FarmCPU/FarmCPU_functions.txt")

########################
# Setting memory limit #
########################
memory.limit()
memory.limit(size = 35000)

###############################
# Getting Shell Script Inputs #
###############################
cli_arg <- commandArgs(trailingOnly = TRUE) # takes a variable following the script
iter <- as.character(cli_arg[1])
print(iter)

#########################
# Set Working Directory #
#########################
setwd("/home/hirschc1/kirsc168/GWAS_project/")

################
# Loading Data #
################
myY <- read.csv(paste0("MyY_Splitting_Env/MyY_Env_BLUPs_", iter, ".csv"), header = T) # phenotypic data (split by year)
myGD <- read.big.matrix(paste0("GD_Env/GD_Env_", iter,".txt"), type = "char", sep = "\t", head = T)#paste0("GD_Env_", iter, ".txt"), sep = "\t",type = "char", head=T) #name needs to be GAPIT.Genotype.Numerical.txt
myGM <- read.table("GAPIT.Genotype.map.txt", head=T) # also output from Numerical_GAPIT.R

#################################
# Determining P-Value Threshold #
#################################
FarmCPU.P.Threshold(
  Y=myY[,c(1,2)], #blup dataset    #only two columns allowed, the first column is taxa name and the second is phenotype value
  GD=myGD,
  GM=myGM, #genetic map dataset
  trait=paste0("FarmCPU_p_threshold", iter), #name of the trait, only used for the output file name #iter here is the environment
  theRep=100 #number of permutation times
)

```

## GWAS_FarmCPU.sh

  - bash script to run GWAS_FarmCPU.R
  - how to submit each iteration of the script
  
```{bash}
while read -r line; do sbatch --export=iter=${line} GWAS_FarmCPU.sh; done < "IterationsForPValue.txt"
```

  - actual bash script GWAS_FarmCPU.sh to submit GWAS_FarmCPU.R

```{bash}
#!/bin/bash
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=8
#SBATCH --cpus-per-task=1
#SBATCH --mem=100gb
#SBATCH --time=1:00:00
#SBATCH --mail-type=ALL
#SBATCH --mail-user=kirsc168@umn.edu
#SBATCH -o /home/hirschc1/kirsc168/GWAS_project/%j.out
#SBATCH -e /home/hirschc1/kirsc168/GWAS_project/%j.err

echo ${iter}

# Load R
module load R/4.0.4
cd /home/hirschc1/kirsc168/GWAS_project/

Rscript --max-ppsize=500000 GWAS_FarmCPU.R ${iter}
```

### GWAS_FarmCPU.R

  - completes the GWAS analysis for each iteration

```{r}
### GWAS_FarmCPU.R
### Dorothy Sweet
### 12/15/2022

#############
# Libraries #
#############
library("tidyverse")
library("multtest")
library("snpStats")
library("BiocManager")
library("gplots")
library("LDheatmap")
library("genetics")
library("ape")
library("EMMREML")
library("compiler")
library("scatterplot3d")
library("bigmemory")
library("biganalytics")

###############################
# Getting Shell Script Inputs #
###############################
cli_arg <- commandArgs(trailingOnly = TRUE)
iter <- as.character(cli_arg[1])
print(iter)

#########################
# Set Working Directory #
#########################
setwd(paste0("/home/hirschc1/kirsc168/GWAS_project/"))

##############################
# Sourcing GAPIT and FarmCPU #
##############################
source("http://zzlab.net/GAPIT/gapit_functions.txt")
source("http://zzlab.net/FarmCPU/FarmCPU_functions.txt")

################
# Loading Data #
################
myY <- read.csv(paste0("MyY_Splitting_Env/MyY_Env_BLUPs_", iter, ".csv"), header = T)
myGD <- read.big.matrix(paste0("GD_Env/GD_Env_", iter, ".csv.txt"), sep = "\t", type = "char", head=T)
myGM <- read.delim("GAPIT.Genotype.map.txt", sep = "\t", header=T)
myPCA <- read.csv("GAPIT.PCA.csv", header=T)
myPValue <- read.delim(paste0("FarmCPU_p_threshold/FarmCPU.p.threshold.optimize.FarmCPU_p_threshold",iter,".txt"), header = F, sep = "\t")

pval <- quantile(myPValue$V1, 0.05)
print(pval)

#########################
# Filtering PCA Dataset #
#########################
newPCA <- myPCA %>%
  filter(taxa %in% myY$taxa) %>%
  as.tibble()

######
# QC #
######
length(myY$taxa)
length(newPCA$taxa)
sum(myY$taxa == newPCA$taxa)

#################
# FarmCPU Model #
#################
MyFarmCPU <- FarmCPU(
  Y=myY, #phenotype
  GD=myGD, #Genotype matrix
  GM=myGM, #Genotypic map
  CV=newPCA[,-1], #Covariate variables (First 5 PCAs from GAPIT); taxa should NOT be included.
  threshold.output=1, #P value smaller than threshold.output will be output in GWAS table
  p.threshold=pval,
  MAF.calculate=TRUE, #Calculate minor allele frequency (MAF) or not, if set to TRUE, the SNPs with a lower MAF (<maf.threshold) will be deleted
  method.bin="optimum",
  maf.threshold=0.05, #When MAF.calculate=TRUE, the SNPs with a lower MAF (<maf.threshold) will be deleted
  maxLoop=50, #Maximum number of iterations allowed
  memo = paste("Env_", iter, sep = "") #Add extension to file name for parallel runs
)

```

# Analyzing GWAS Outputs

  - written to run on MSI
  - analyzes output for GWAS and makes outputs small enough to be evaluated locally

## Running_FarmCPU_significant_snp_identifier.sh

  - bash script to submit FarmCPU_significant_snp_identifier.R

```{bash}
#!/bin/bash
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=1
#SBATCH --cpus-per-task=1
#SBATCH --mem=100gb
#SBATCH --time=5:00:00
#SBATCH --mail-type=ALL
#SBATCH --mail-user=kirsc168@umn.edu
#SBATCH -o /home/hirschc1/kirsc168/GWAS_project/%j.out
#SBATCH -e /home/hirschc1/kirsc168/GWAS_project/%j.err

# Load R
module load R/4.0.4
cd /home/hirschc1/kirsc168/GWAS_project/

# Running Script
Rscript --max-ppsize=500000 FarmCPU_significant_snp_identifier.R
```

### FarmCPU_significant_snp_identifier.R

  - R script to look through results of each iteration of GWAS and the p values for those iterations to identify significant SNPs

```{r}
### FarmCPU Significant SNP Identifier
### Dorothy Sweet
### 8/12/21

# Loading Libraries
library(tidyr)

# read in file with variables
variables <- read.delim("IterationsForPValue.txt", header = F)

for (rows in 1:nrow(variables)) {
  iteration = variables[rows,1]

  ###################
  # Find threshold #
  ##################
  # use if statement to skip missing iterations
   if (file.exists(paste0("FarmCPU_p_threshold/FarmCPU.p.threshold.optimize.FarmCPU_p_threshold",iteration,".txt"))) {
        pthresfile <- read.delim(paste0("FarmCPU_p_threshold/FarmCPU.p.threshold.optimize.FarmCPU_p_threshold",iteration,".txt")) # open file

        # identify the p threshold from the p threshold file
        pthreshold <- pthresfile[nrow(pthresfile),1]
        # identify the negative log 10 of the pthreshold
        logthreshold <- (-log10(pthreshold))

        #########################
        # Find Significant SNPs #
        #########################
        # open farmCPU output file for the iteration
        if (file.exists(paste0("FarmCPU_output/FarmCPU.Env_", iteration, ".BLUPS.GWAS.Results.csv"))) {
                 farmoutfile <- read.csv(paste0("FarmCPU_output/FarmCPU.Env_", iteration, ".BLUPS.GWAS.Results.csv"))

                # make a column with the -log10 of the p value
                farmoutfile$logP <- (-log10(farmoutfile$P.value))
                
                # remove NAs
                farmoutfile <- farmoutfile %>%
                  na.omit()

                # filter based on -log(p)
                sigsnps <- farmoutfile[farmoutfile$logP >= logthreshold,]

                #print file with signficant snps
                write.table(sigsnps, file = paste0("FarmCPU_significant_snps/FarmCPU_significant_snps_", iteration, ".txt"), sep = "\t", row.names = F, col.names = T)
                } else {
                  next
                }
      } else {
	next
      }
}
```

## Running_IdentifySigSNPsLD.sh

  - bash script to submit IdentifySigSNPsLD.R

```{bash}
#!/bin/bash
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=1
#SBATCH --cpus-per-task=1
#SBATCH --mem=10gb
#SBATCH --time=10:00:00
#SBATCH --mail-type=ALL
#SBATCH --mail-user=kirsc168@umn.edu
#SBATCH -o /home/hirschc1/kirsc168/GWAS_project/%j.out
#SBATCH -e /home/hirschc1/kirsc168/GWAS_project/%j.err

# Load R
module load R/4.0.4
cd /home/hirschc1/kirsc168/GWAS_project/

# Running Script
Rscript --max-ppsize=500000 IdentifySigSNPsLD.R
```
### IdentifySigSNPsLD.R

  - Isolates each significant SNP to only one iteration no matter the number of occurrences of significance (Unique_Significant_SNPs.txt)

```{r}
# named: IdentifySigSNPsLD.R
# Dorothy Sweet
library(data.table)
library(stringr)
# read in file with variables
variables <- read.delim("IterationsForPValue.txt", header = F)

# initiate the significant SNP file
AllsigSNPs <- matrix(data = NA, nrow = 0, ncol = 6)
colnames(AllsigSNPs) <- c("SNP", "Chromosome", "Position", "P.value", "maf", "effect")

AllsigSNPs <- as.data.frame(AllsigSNPs)
AllsigSNPsDesc <- data.frame()

for (rows in 1:nrow(variables)) {
  iteration = variables[rows,1]

  if (file.exists(paste0("FarmCPU_significant_snps/FarmCPU_significant_snps_",iteration,".txt"))) {
        sigSNPsfile <- read.delim(paste0("FarmCPU_significant_snps/FarmCPU_significant_snps_",iteration,".txt")) # open file
        # Make a file with the significant snp, the year, data type, and GDD
        iteration <- as.character(iteration) # make iteration a character
        descriptive.data <- as.data.frame(str_split(iteration, pattern = "_")) # make a data frame with the descriptive info
        if (nrow(sigSNPsfile) == 0) {
          next
        } else {
          sigSNPsfileDesc <- sigSNPsfile # make a new data frame for this information
          sigSNPsfileDesc$Year <- descriptive.data[2,1] # add Year column
          sigSNPsfileDesc$Data.Type <- descriptive.data[1,1] # add data type column
          sigSNPsfileDesc$GDD <- descriptive.data[3,1] # add GDD column
          # rbind to overall file (needs to be initialized earlier)
          AllsigSNPsDesc <- rbind(AllsigSNPsDesc, sigSNPsfileDesc)
        }

        # make a file of all significant snps
        AllsigSNPs <- rbind(AllsigSNPs, sigSNPsfile)
      } else {
	next
      }
}

# filter out unique SNPs
onlySigSNP <- unique(AllsigSNPs$SNP)

# write file of all significant SNPs with descriptive information
write.table(AllsigSNPsDesc, file = "All_Significant_SNPs_and_Descriptive_Info.txt", row.names = F, col.names = T)

# write Unique significant SNPs file
write.table(onlySigSNP, file = "Unique_Significant_SNPs.txt", row.names = F, col.names = F)
```
## Running_Making_Master_GWAS_Results.sh

  - bash file to submit Making_Master_GWAS_Results.R

```{bash}
#!/bin/bash
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=1
#SBATCH --cpus-per-task=1
#SBATCH --mem=100gb
#SBATCH --time=10:00:00
#SBATCH --mail-type=ALL
#SBATCH --mail-user=kirsc168@umn.edu
#SBATCH -o /home/hirschc1/kirsc168/GWAS_project/%j.out
#SBATCH -e /home/hirschc1/kirsc168/GWAS_project/%j.err

# Load R
module load R/4.0.4
cd /home/hirschc1/kirsc168/GWAS_project/

# Running Script
Rscript --max-ppsize=500000 Making_Master_GWAS_Results.R
```
### Making_Master_GWAS_Results.R

  - compiles all data for each significant SNP in each year and data type separately

```{r}
# Loading Libraries
library(tidyr)
library(dplyr)

# read in file with variables
variables <- read.delim("IterationsForPValue.txt", header = F)

data.type <- c("GDD", "interval", "Terminal")
data.year <- c("2018", "2019", "2020", "2021")

for (a in data.type) {
  # filter the iterations file based on GDD or interval
  variables.a <- subset(variables, grepl(a, V1))
  # filter the iterations file based on year
  for (b in data.year) {
    variables.b <- subset(variables.a, grepl(b,V1))

    # initialize empty temp file
    temporary_gwas <- data.frame(matrix(ncol = 7, nrow = 0))
    colnames(temporary_gwas) <- c("SNP", "Chromosome", "Position", "P.value", "maf", "effect", "GWAS")

    for (rows in 1:nrow(variables.b)) {
      iteration = variables.b[rows,1]

      #########################
      # Find GWAS Results file #
      #########################
      if (file.exists(paste0("FarmCPU_output/FarmCPU.Env_", iteration, ".BLUPS.GWAS.Results.csv"))) {
         farmoutfile <- read.csv(paste0("FarmCPU_output/FarmCPU.Env_", iteration, ".BLUPS.GWAS.Results.csv")) # open file

         # add column to farmoutfile with the GDD or interval for that file
         farmoutfile$GWAS <- iteration

         # add results to temp file
         temporary_gwas <- rbind(temporary_gwas, farmoutfile)
      } else {
	next
      }
    }
    write.table(temporary_gwas, file = paste0("FarmCPU_Master_GWAS_Results/Master_GWAS_Results_", a, "_", b, ".txt"), sep = "\t", row.names = F, col.names = T)
  }
}

```
## Running_Graphing_SNP_Effect_over_time_v2.sh

  - bash script to submit Graphing_SNP_Effect_over_time_v2.R

```{bash}
#!/bin/bash
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=1
#SBATCH --cpus-per-task=1
#SBATCH --mem=250gb
#SBATCH --time=40:00:00
#SBATCH --mail-type=ALL
#SBATCH --mail-user=kirsc168@umn.edu
#SBATCH -o /home/hirschc1/kirsc168/GWAS_project/%j.out
#SBATCH -e /home/hirschc1/kirsc168/GWAS_project/%j.err

# Load R
module load R/4.0.4
cd /home/hirschc1/kirsc168/GWAS_project/

# Running Script
Rscript --max-ppsize=500000 Graphing_SNP_Effect_over_time_v2.R
```

### Graphing_SNP_Effect_over_time_v2.R

  - Roughly graphs SNP effect over time for all data types and years - just for initial look, better, more finalized version later on for local computer
  - also outputs "Master_GWAS_Results_all_with_sig.txt" for later use in that script

```{r}
# named: Graphing_SNP_Effect_over_time_v2.R
# Dorothy Sweet
library(dplyr)
library(tidyverse)
library(data.table)
library(patchwork)
library(ggplot2)

# create variables for "for loops" to open master GWAS results files
data.type <- c("GDD", "interval", "Terminal")
data.year <- c("2018", "2019", "2020", "2021")

###################################
## Make a p.threshold master file #
###################################
# read in iterations file
Iterations_file <- read.delim("IterationsForPValue.txt", header = F)

# initialize Master p.threshold file
Master.p <- data.frame(matrix(ncol = 2, nrow = 0))
colnames(Master.p) <- c("GWAS", "P.threshold")

for (ite in 1:nrow(Iterations_file)) {
 iteration <- Iterations_file[ite,1]

 # Use if statement to skip missing iterations
 if (file.exists(paste0("FarmCPU_p_threshold/FarmCPU.p.threshold.optimize.FarmCPU_p_threshold",iteration,".txt"))) {
   tempthres <- read.delim(paste0("FarmCPU_p_threshold/FarmCPU.p.threshold.optimize.FarmCPU_p_threshold",iteration,".txt")) # open file

   # identify the p threshold
   pthreshold <- tempthres[nrow(tempthres),1]

   # make a temporary file with iteration and pthreshold
   temp <- data.frame(matrix(nrow = 1, ncol = 2))
   temp[1,1] <- iteration
   temp[1,2] <- pthreshold

   # add to Master p threshold file
   Master.p <- rbind(Master.p, temp)
 }
}

# rename and remove columns
Master.p$GWAS <- Master.p$X1
Master.p$P.threshold <- Master.p$X2
Master.p$X1 <- NULL
Master.p$X2 <- NULL

write.table(Master.p, file = paste0("FarmCPU_p_threshold/FarmCPU.p.threshold.optimize.FarmCPU_p_threshold_all.txt"), sep = "\t", row.names = F, col.names = T)

#############################################
## Make a GWAS Results and Significance File #
#############################################
# read in Master.p file
 Master.p <- fread("FarmCPU_p_threshold/FarmCPU.p.threshold.optimize.FarmCPU_p_threshold_all.txt", header = T)

# initialize Master results file
All.Results.GWAS <- data.frame(matrix(ncol = 7, nrow = 0))
colnames(All.Results.GWAS) <- c("SNP", "P.value", "effect", "significance","data.type", "data.year", "GDD.Interval")

for (t in data.type) {
  for (y in data.year) {
    # read in the GWAS results master file
    GWAS_Results <- fread(paste0("FarmCPU_Master_GWAS_Results/Master_GWAS_Results_", t, "_", y, ".txt"), header = T)

    # join gwas results and p thresholds
    GWAS_Results.1 <- merge(GWAS_Results, Master.p, by = "GWAS")

    # initialize Significance column
    GWAS_Results.1$significance <- NA

    # determine if each snp is significant
    GWAS_Results.1$logp <- (-log10(GWAS_Results.1$P.value))
    GWAS_Results.1$logpthres <- (-log10(GWAS_Results.1$P.threshold))

    # make a column with significance
    GWAS_Results.1$significance <- fifelse(GWAS_Results.1$logp >= GWAS_Results.1$logpthres, "sig", "ns")

    # make a column with the data type and data year in another
    GWAS_Results.1$data.type <- t # data.type
    GWAS_Results.1$data.year <- y # data.year

    # make a new column with the GDD or interval from the GWAS column
    GWAS_Results.1$GDD.Interval <- as.numeric(sapply(as.character(GWAS_Results.1$GWAS), function(x) strsplit(x, split="X", fixed = T)[[1]][2]))

    # get rid of unnecessary columns
    GWAS_Results.1$Chromosome <- NULL
    GWAS_Results.1$ Position <- NULL
    GWAS_Results.1$maf <- NULL
    GWAS_Results.1$P.threshold <- NULL
    GWAS_Results.1$logp <- NULL
    GWAS_Results.1$logpthres <- NULL

    # Add to master GWAS Results file
    All.Results.GWAS <- rbind(All.Results.GWAS, GWAS_Results.1)
  }
}

# Print all GWAS RESULTS FILE
write.table(All.Results.GWAS, file = paste0("FarmCPU_Master_GWAS_Results/Master_GWAS_Results_all_with_sig.txt"), sep = "\t", row.names = F, col.names = T)

##########################
# Add precipitation data #
##########################
# initialize full precipitation datat file
All.weather <- data.frame(matrix(ncol = 4, nrow = 0))
colnames(All.weather) <- c("RainfallTotal", "cum.rain", "GDD.Interval", "data.year")

# for loop to read in all of the precipitation data and add to a master file with year as a column
for(prec in 1:length(data.year)) {
  weather <- data.year[prec]

  # read in the file
  temp_w <- read.csv(paste0("Precipitation_Data/Precipitation_", weather,".csv"))
  
  # remove precipitation data after 1500 GDD
  temp_w <- temp_w %>%
    filter(Date < 1500)
  
  # renoame Date column
  temp_w$GDD.Interval <- temp_w$Date
  temp_w$Date <- NULL

  # add a year column
  temp_w$data.year <- weather

  # add to master file
  All.weather <- rbind(All.weather, temp_w)
}

#########################
# Plot Significant SNPs #
#########################
# read in the ld file filtered with only the unique SNPs
ld_file <- read.table("ld_file_christine_data_filter.ld", header = TRUE)

# read in unique snp list
unique.snp.list <- read.delim("Unique_Significant_SNPs.txt", header = F, sep = "\n")
colnames(unique.snp.list) <- "SNP"

# filter out the SNPs in significant LD
filtered_ld <- filter(ld_file, R2 >= 0.95)

# while loop to make a plot for each significant snp
while (nrow(unique.snp.list) > 0) {
  # make inisnp the first value from the column
  inisnp <- unique.snp.list[1,1]

  # filter snps in sig ld with the unique significant snp from the GWAS
  sig.snp.ld <- filter(filtered_ld, SNP_A == inisnp)

  # list all snps here - the unique snp and snps in ld with it
  snp.list <- c(inisnp, sig.snp.ld$SNP_B)

  # remove all values in snp.list from unique.snp.list
  unique.snp.list <- subset(unique.snp.list, !(SNP %in% snp.list))

  # filter for only the snps of interest
  snp_results <- subset(All.Results.GWAS, SNP %in% snp.list)

  # make a -log10(p.value) column
  snp_results$logp <- -log10(snp_results$P.value)

 # initiate snp_weather for the snp_results and weather data together
  snp_weather <- data.frame(matrix(ncol = 11, nrow = 0))
  colnames(snp_weather) <- c("GDD.Interval", "GWAS", "SNP", "P.value", "effect", "significance", "data.type", "logp", "RainfallTotal", "cum.rain", "data.year")
   # Add precipitation data to snp_results
  for (y in 1:length(data.year)) {
    year <- data.year[y]
    # subset the data by year to merge it all together
    Temp_year <- subset(snp_results, data.year %in% year)
    temp_precip <- subset(All.weather, data.year %in% year)

    # remove the year columns
    Temp_year$data.year <- NULL
    temp_precip$data.year <- NULL

    # add in Interval and GDD so they will facet correctly
    temp_precip_int <- temp_precip
    temp_precip_int$data.type <- "interval"
    temp_precip_gdd <- temp_precip
    temp_precip_gdd$data.type <- "GDD"
    temp_precip_ter <- termp_precip
    temp_precip_ter$data.type <- "Terminal"
    temp_precip <- rbind(temp_precip_int, temp_precip_gdd)
    temp_precip <- rbind(temp_precip,temp_precip_ter)

    # merge the two and add year back in
    temp_all_year <- merge(Temp_year, temp_precip, by = "GDD.Interval", all.x = T,all.y = T)
    temp_all_year$data.year <- year

    # make a data.type column
    temp_all_year$data.type <- NA
    for (x in 1:nrow(temp_all_year)) {
      if (is.na(temp_all_year[x,7])) {
        next
      } else if(temp_all_year[x,7] == "interval") {
        temp_all_year[x, 13] <- "interval"
      } else if (temp_all_year[x, 7] == "GDD") {
        temp_all_year[x, 13] <- "GDD"
      } else if (temp_all_year[x,7] == "Terminal") {
        temp_all_year[x,13] <- "Terminal"
      }
    }
    for (x in 1:nrow(temp_all_year)) {
      if (is.na(temp_all_year[x,11])) {
        next
      } else if (temp_all_year[x,11] == "interval") {
        temp_all_year[x,13] <- "interval"
      } else if (temp_all_year[x,11] == "GDD") {
        temp_all_year[x,13] <- "GDD"
      } else if (temp_all_year[x,11] == "Terminal") {
        temp_all_year[x,13] <- "Terminal"
      }
    }

    # remove data.type.x and data.type.y
    temp_all_year$data.type.x <- NULL
    temp_all_year$data.type.y <- NULL

    # add to snp_weather
    snp_weather <- rbind(snp_weather, temp_all_year)
  }

  # plot the effect
  plot.effect <- ggplot(snp_weather, aes(x = GDD.Interval, group = SNP)) +
    geom_point(aes(y = effect, color = SNP, shape = significance)) +
    geom_line(aes(y = effect, color = SNP)) +
    geom_rug(aes(x = GDD.Interval, color = as.numeric(RainfallTotal)), sides = "b") +
    #geom_point(aes(y = RainfallTotal *2)) +
    #geom_segment(aes(x = GDD.Interval, xend = GDD.Interval, y = 0, yend = (RainfallTotal *2))) +
    theme(axis.text.x = element_text(angle = 90)) +
    theme_light() +
    xlim(100, 1500) + 
    xlab("GDD") +
    scale_y_continuous(
      name = "Effect"#,
      #sec.axis = sec_axis(trans=~./2, name="Precipitation")
    ) +
    ggtitle(paste0("SNP effect over time", inisnp)) +
    facet_wrap(~ data.year, scales = "free_y")

  #plot the p value
  plot.p.value <- ggplot(snp_weather, aes(x = GDD.Interval, group = SNP)) +
    geom_point(aes(y = logp, shape = significance)) +
    geom_line(aes(y = logp)) +
    geom_rug(aes(x = GDD.Interval, color = as.numeric(RainfallTotal)), sides = "b") +
    #geom_point(aes(y = RainfallTotal *2)) +
    #geom_segment(aes(x = GDD.Interval, xend = GDD.Interval, y = 0, yend = (RainfallTotal *2))) +
    theme(axis.text.x = element_text(angle = 90)) +
    theme_light() +
    xlim(100, 1500) +
    xlab("GDD") +
    scale_y_continuous(
      name = "-log10(P value)"#,
      #sec.axis = sec_axis(trans=~./2, name="Precipitation")
    ) +
    ggtitle(paste0("SNP P value over time", inisnp)) +
    facet_wrap(~ data.year, scales = "free_y")

  pdf(paste0("SNP_effect_graphs/SNP_ALL_Significant_", inisnp, ".pdf"), height = 5, width = 15)
    print(plot.effect)
    print(plot.p.value)
  dev.off()
  
  write.csv(snp_weather, paste0("SNP_effect_graphs/SNP_ALL_Significant_", inisnp, ".csv"))
    
}
```

## Running_Manhattan_Plot_Remake.sh

  - remake the Manhattan Plots automatically output by FarmCPU
  - code pasted into command line to submit script for each iteration: 

```{bash}
while read -r line; do sbatch --export=iter=${line} Running_Manhattan_Plot_Remake.sh; done < "IterationsForPValue.txt"
```

  - the actual bash script that will be submitted to run Manhattan_Plot_Remake.R using above code    

```{bash}
#!/bin/bash
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=8
#SBATCH --cpus-per-task=1
#SBATCH --mem=100gb
#SBATCH --time=12:00:00
#SBATCH --mail-type=ALL
#SBATCH --mail-user=kirsc168@umn.edu
#SBATCH -o /home/hirschc1/kirsc168/GWAS_project/%j.out
#SBATCH -e /home/hirschc1/kirsc168/GWAS_project/%j.err

echo ${iter}

# Load R
module load R/4.0.4
cd /home/hirschc1/kirsc168/GWAS_project/

Rscript --max-ppsize=500000 Manhattan_Plot_Remake.R ${iter}
```

### Manhattan_Plot_Remake

  - R script to remake Manhattan plots

```{r}
## Loading in packages
library("qqman")

## Getting shell script inputs
cli_arg <- commandArgs(trailingOnly = TRUE)
iter <- as.character(cli_arg[1])
print(iter)

## Set working directory
setwd(paste0("/home/hirschc1/kirsc168/GWAS_project/FarmCPU_output/"))
  
## Loading in the data
data <- read.csv(paste0("FarmCPU.Env_",iter,".BLUPS.GWAS.Results.csv"))

#removing maf and effect
data <- data[,c(-5:-6)]
#renaming columns
names(data)[1] <- "SNP"
names(data)[2] <- "CHR"
names(data)[3] <- "BP"
names(data)[4] <- "P"
str(data)
unique(data$CHR)
#NAs <- which(is.na(data$P)) #may need to run if the Error in plot.window(...) : need finite 'ylim' values appears when plotting
#data <- data[c(-NAs),]
new.names <- c("1", "2", "3", "4", "5", "6", "7", "8", "9", "10")
  
#temp_plot <- manhattan(data, chr="CHR", bp="BP", snp="SNP", p="P", main="Plant Height", chrlabs = new.names, suggestiveline = F, genomewideline = F, ylim = c(0,7))

# print manhattan plot
pdf(paste0("/home/hirschc1/kirsc168/GWAS_project/FarmCPU_output/Manhattan_",iter,".pdf"))
manhattan(data, chr="CHR", bp="BP", snp="SNP", p="P", main="Plant Height", chrlabs = new.names, suggestiveline = F, genomewideline = F, ylim = c(0,7))
dev.off()
```

## Identifying PVE for Significant SNPs

  - Group of scripts used in combination to calculate PVE for SNPs found significant in each iteration of GWAS
  - Runs on a combination of local computer and MSI (includes bash scripts to move data between the two)
  - Uses Tassel 5.0 GUI on local computer
  
### Pull Down Pruned Hapmap File Used for GWAS

  - Pull down Pruned Hapmap file used for GWAS to local computer to make a binary Plink file using the Tassel GUI
  
```{bash}
scp -r kirsc168@mangi.msi.umn.edu:/home/hirschc1/kirsc168/GWAS_project/Pruned_final_hapmap.hmp.txt /Users/dorothykirsch/Desktop/
```
### Binarize Total Hapmap File in Tassel

  - Completed on Local computer using Tassel 5.0 GUI found at https://www.maizegenetics.net/tassel
  - Load Pruned_final_hapmap.hmp.txt into Tassel GUI (File > Open > Desktop/Pruned_final_hapmap.hmp.txt) 
  - Make the hapmap binary  (Filter > Filter Genotype Table Sites > Check the box for 'Remove Minor SNP States' > OK)
  - Select Pruned_final_hapmap_Filter then save (File > Save_as > Desktop/Pruned_Final_Hapmap_Binarize_Filtered) format Hapmap
    AND
    Select Pruned_final_hapmap_Filter then save (File > Save_as > Desktop/Pruned_Final_Hapmap_Binarize_Filtered) format Plink

### Upload Binary Hapmap and Plink Files to MSI

  - Takes output from GUI work and uploads it to MSI to be used in future scripts
  
```{bash}
scp -r /Users/dorothykirsch/Desktop/Pruned_Final_Hapmap_Binarize_Filtered*  kirsc168@mangi.msi.umn.edu:/home/hirschc1/kirsc168/GWAS_project/
```
### Make Files for each GWAS Iteration with Just the Significant SNP Names

  - Runs on MSI
  - Make files with just the significant SNPs
  - Files containing just the significant SNPs are used to filter the hapmap files to only contain the SNPs significant in that environment
  - inputs: FarmCPU_significant_snps/FarmCPU_significant_snps_${filename}.txt
  - exports: FarmCPU_significant_snps_${filename}_list.txt
    - FarmCPU_significant_snps_All_list.txt
  
```{bash}
# Common string to append to each filename
common_string="FarmCPU_significant_snps/FarmCPU_significant_snps_"
common_string_2=".txt"

# Read each line from file_list.txt and create a list of input files
while IFS= read -r filename; do
    input_files+=("$common_string$filename$common_string_2")
done < "IterationsForPValue.txt"

# for loop to go through each iteration
for filename in "${input_files[@]}"; do
  input_file="$filename" # set the input file name
  
  echo "Processing input file: $input_file" # tell me which file it is on
  
  output_file="$(basename "$input_file" .txt)_list.txt" # set the output file name to add _list to the end
  # pull the SNPs from the first column, remove the column name and the quotes around the SNP names
  tail -n +2 "$input_file" | sed 's/"//g' | awk '{print $1}' > "$output_file" 

done
# Print the list of input files
echo "List of input files:"
printf '%s\n' "${input_files[@]}"

# move all SNP list files to the FarmCPU_significant_snps folder
mv FarmCPU_significant_snps_*_list.txt FarmCPU_significant_snps/

# make a file containing all significant SNPs together
sed 's/[\"\'']//g' Unique_Significant_SNPs.txt > FarmCPU_significant_snps/FarmCPU_significant_snps_All_list.txt
```
### Just_Taxa_Files.sh

  - Runs on MSI
  - make files containing just the taxa
  - Files containing just the taxa are used to filter the hapmap files to only contain the taxa in that environment
  - inputs: MyY_Splitting_Env/MyY_Env_BLUPs_${filename}.csv
  - exports: Just_Taxa/${filename}_taxa.txt
  
```{bash}
#!/bin/bash
#SBATCH --time=0:15:00
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=4
#SBATCH --mem=40gb
#SBATCH -J prune_marker_data_ld
#SBATCH -o /home/hirschc1/kirsc168/GWAS_project/%j.out
#SBATCH -e /home/hirschc1/kirsc168/GWAS_project/%j.err
#SBATCH --mail-type=ALL
#SBATCH --mail-user=kirsc168@umn.edu
#SBATCH --no-requeue

while read -r filename; do
  awk -F ',' 'NR>1 {print $1}' MyY_Splitting_Env/MyY_Env_BLUPs_${filename}.csv > Just_Taxa/${filename}_taxa.txt
done < "IterationsForPValue.txt"
```
### Make_Binary_Plk_Files_Sig_SNPs.sh

  - Runs on MSI
  - Makes Binary Plink files for each iteration
  - imports:
    - Pruned_Final_Hapmap_Binarize_Filtered.hmp.txt (Pruned, Filtered, Binary Hapmap containing all SNPs used in GWAS analysis)
    - FarmCPU_significant_snps/FarmCPU_significant_snps_${filename}_list.txt (txt)
    
  
```{bash}
#!/bin/bash
#SBATCH --time=2:00:00
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=4
#SBATCH --mem=40gb
#SBATCH -J Significant_SNP_Plk_Files
#SBATCH -o /home/hirschc1/kirsc168/GWAS_project/%j.out
#SBATCH -e /home/hirschc1/kirsc168/GWAS_project/%j.err
#SBATCH --mail-type=ALL
#SBATCH --mail-user=kirsc168@umn.edu
#SBATCH --no-requeue

### load the module
module load plink/1.90b6.10

# go to project folder
cd /home/hirschc1/kirsc168/GWAS_project/

# transform hmp to binary plk with just the Significant SNPs
while read -r filename; do

# run once for SNPs only significant in that GWAS
  run_pipeline.pl -Xmx100g -importGuess Pruned_Final_Hapmap_Binarize_Filtered.hmp.txt \
                    -includeSiteNamesInFile FarmCPU_significant_snps/FarmCPU_significant_snps_${filename}_list.txt \
                    -includeTaxaInFile Just_Taxa/${filename}_taxa.txt \
                    -export Pruned_Final_Hapmap_Binarize_Filtered_FarmCPU_significant_snps/FarmCPU_significant_snps_${filename}_list \
                    -exportType Plink
                    
# run for all Significant SNPs                    
  run_pipeline.pl -Xmx100g -importGuess Pruned_Final_Hapmap_Binarize_Filtered.hmp.txt \
                    -includeSiteNamesInFile FarmCPU_significant_snps/FarmCPU_significant_snps_All_list.txt \
                    -includeTaxaInFile Just_Taxa/${filename}_taxa.txt \
                    -export Pruned_Final_Hapmap_Binarize_Filtered_FarmCPU_significant_snps/FarmCPU_significant_snps_All_${filename}_list \
                    -exportType Plink
                    
done < "IterationsForPValue.txt"
```
### Plink_to_bed_file.sh (STILL A WORK IN PROGRESS)

  - Runs on MSI
  - First removes any erroneous variant calls with an extra space
  - Uses plink to turn a plink file set into a bed file.
  - Requires a binary plink dataset.
  - Produces a bedfile of the binary hapmap.
  - This script does both the full dataset as well as the significant SNP dataset.

```{bash}
#!/bin/bash
#SBATCH --nodes=1
#SBATCH --ntasks-per-node=8
#SBATCH --cpus-per-task=1
#SBATCH --mem=10gb
#SBATCH --time=1:00:00
#SBATCH --mail-type=ALL
#SBATCH --mail-user=kirsc168@umn.edu
#SBATCH -o /home/hirschc1/kirsc168/GWAS_project/%j.out
#SBATCH -e /home/hirschc1/kirsc168/GWAS_project/%j.err

# Load module
module load plink/1.07

sed -i 's/ C  / C /g' Pruned_Final_Hapmap_Binarize_Filtered.plk.ped

# Run Plink to make bed file for all of the SNPs
plink --noweb --file /home/hirschc1/kirsc168/GWAS_project/Pruned_Final_Hapmap_Binarize_Filtered.plk --make-bed --out /home/hirschc1/kirsc168/GWAS_project/Pruned_Final_Hapmap_Binarize_Filtered_bed

# For loop to run Plink to make bed file for each GWAS runthrough
while read -r line; do
  echo Working on Environment ${line}
	plink --noweb --file Pruned_Final_Hapmap_Binarize_Filtered_FarmCPU_significant_snps/FarmCPU_significant_snps_${line}_list1.plk --make-bed --out Pruned_Final_Hapmap_Binarize_Filtered_FarmCPU_significant_snps/FarmCPU_significant_snps_${line}_list_bed
	
	plink --noweb --file Pruned_Final_Hapmap_Binarize_Filtered_FarmCPU_significant_snps/FarmCPU_significant_snps_All_${line}_list1.plk --make-bed --out Pruned_Final_Hapmap_Binarize_Filtered_FarmCPU_significant_snps/FarmCPU_significant_snps_All_${line}_list_bed
done < "IterationsForPValue.txt"

```
### GCTA Conversion and PVE Calculation

  - GCTA from cnsgenomics.com
  - converts bedfile from 'Plink_to_bed_file.sh' into grm file set using GCTA
  - creates an appropriate pheno input file from MyY_Splitting_Env/MyY_Env_BLUPs_${line}.csv
  - calculates the PVE by the significant SNPs using GCTA
    - --pheno (no header line; columns are family ID, individual ID and phenotypes with family ID -9 for this study)
  - inputs:
    - Pruned_Final_Hapmap_Binarize_Filtered_FarmCPU_significant_snps/FarmCPU_significant_snps_${line}_list_bed
    - MyY_Splitting_Env/MyY_Env_BLUPs_${line}.csv
    - "IterationsForPValue.txt"
  - exports:
    - Data/gcta_grm_out_${line}
    - Data/GCTA_Pheno_${line}.txt
    - Data/GCTA_PVE_Env_${line}
    - Data/PVE_all_iterations_sig_SNPs.csv

```{bash}
#! /bin/bash
# set iteration count to write over previous PVE_all_iterations_sig_SNPs.csv file with first iteration
count=0
## JUST SIGNIFICANT SNPS FOR EACH RUN ##
# For loop to run Plink to make bed file for each GWAS runthrough
while read -r line; do
  echo Working on Environment ${line}
  
  # Convert bedfile into a grm file set
  ./gcta64 --bfile Pruned_Final_Hapmap_Binarize_Filtered_FarmCPU_significant_snps/FarmCPU_significant_snps_${line}_list_bed --make-grm --thread-num 10 --out Data/gcta_grm_out_${line}
  
    ./gcta64 --bfile Pruned_Final_Hapmap_Binarize_Filtered_FarmCPU_significant_snps/FarmCPU_significant_snps_All_${line}_list_bed --make-grm --thread-num 10 --out Data/gcta_grm_out_All_${line}
  
  # Use MyY_Splitting_Env BLUP files to make appropriate --pheno input file
  awk 'BEGIN{FS=OFS=","} {$1="-9," $1}1' MyY_Splitting_Env/MyY_Env_BLUPs_${line}.csv | sed '1d' | sed 's/,/\t/g' > Data/GCTA_Pheno_${line}.txt

  # calculates the variance explained by the Significant SNPs
  ./gcta64 --grm Data/gcta_grm_out_${line} --pheno Data/GCTA_Pheno_${line}.txt --reml --out Data/GCTA_PVE_Env_${line}
  
  ./gcta64 --grm Data/gcta_grm_out_All_${line} --pheno Data/GCTA_Pheno_${line}.txt --reml --out Data/GCTA_PVE_Env_All_${line}
  
  # extract the PVE from the .log file
  PVE=$(grep V\(G\)\/Vp Data/GCTA_PVE_Env_${line}.log | cut -f2)
  # create and append PVE value to PVE_all_iterations_sig_SNPs.csv
  if [$count == 0]; then
    echo ${line}","${PVE} > Data/PVE_all_iterations_sig_SNPs.csv
  else
    echo ${line}","${PVE} >> Data/PVE_all_iterations_sig_SNPs.csv
  fi
  
  # extract the PVE from the .log file
  PVE=$(grep V\(G\)\/Vp Data/GCTA_PVE_Env_All_${line}.log | cut -f2)
  # create and append PVE value to PVE_all_iterations_sig_SNPs.csv
  if [$count == 0]; then
    echo ${line}","${PVE} > Data/PVE_all_iterations_ALL_sig_SNPs.csv
  else
    echo ${line}","${PVE} >> Data/PVE_all_iterations_ALL_sig_SNPs.csv
  fi
  ((count+=1))
done < "IterationsForPValue.txt"
```
### Pull Down PVE by Significant SNPs

  - For use in local scripts
  
```{bash}
scp -r kirsc168@mangi.msi.umn.edu:/home/hirschc1/kirsc168/GWAS_project/Data/PVE_all_iterations_sig_SNPs.csv /Users/dorothykirsch/Desktop/Projects_Data/Flights/All/SNP_effect_graphs/

scp -r kirsc168@mangi.msi.umn.edu:/home/hirschc1/kirsc168/GWAS_project/Data/PVE_all_iterations_ALL_sig_SNPs.csv /Users/dorothykirsch/Desktop/Projects_Data/Flights/All/SNP_effect_graphs/
```

# Local Scripts for Graphing GWAS results

  - written to run on a local computer
  - continues to analyze and visualize output from GWAS analyses
  
## Graphing SNP Effect Over TIme

 - This script takes the outputted .csv files from Graphing_SNP_Effect_Over_Time_v2 that is written to run locally and
    makes graphs

```{r}
# load necessary libraries
library(dplyr)
library(tidyverse)
library(data.table)
library(patchwork)
library(ggplot2)
#### INPUTS ####
Path <- "/Users/dorothykirsch/Desktop/Projects_Data/Flights/All/"
################
# Read in Iterations file
SNP.ites <- read.delim(paste0(Path,"Unique_Significant_SNPs.txt"), header = F)
# Create for loop to go through each iteration
for (s in 1:nrow(SNP.ites)) {
  # name the iteration
  snp <- SNP.ites[s,1]
  # read in the .csv file for that iteration
  snp.weather <- read.csv(paste0(Path,"SNP_effect_graphs/SNP_ALL_Significant_",snp,".csv"))
  # make data.type column mean something
  for (t in 1:nrow(snp.weather)) {
    if (is.na(as.character(snp.weather[t,3]))) {
      snp.weather[t,12] = "Precipition"
    } else if (startsWith(as.character(snp.weather[t,3]), "GDD")) {
      snp.weather[t, 12] = "GDD"
    } else if (startsWith(as.character(snp.weather[t,3]), "interval")) {
      snp.weather[t,12] =  "Interval"
    }
  }
  # make files separate for interval
  snp.weather.interval <- snp.weather %>%
    filter(data.type != "GDD")
  # make file separate for GDD
  snp.weather.gdd <- snp.weather %>%
    filter(data.type != "Interval")
   # plot the effect of the snp over time
  plot.effect <- ggplot() +
    geom_line(data = snp.weather.gdd,aes(x = GDD.Interval,y = effect, group = SNP), color = "#3F8A18") +
    geom_point(data = snp.weather.gdd, aes(x = GDD.Interval, y = effect, shape = significance, color = significance, group = SNP, fill = significance)) +
    theme(axis.text.x = element_text(angle = 90)) +
    theme_light() +
    xlim(100, 1500) + 
    xlab("GDD") +
    scale_y_continuous(
      name = "Effect"
    ) +
    facet_wrap(~ data.year, ncol = 4) +
    scale_shape_manual(name = "Significance", labels = c("Not Significant", "Significant", ""), values = c(19,24,1)) +
    scale_color_manual(name = "Significance", labels = c("Not Significant", "Significant", ""), values = c("#3F8A18", "#000000")) +
    scale_fill_manual(name = "Significance", labels = c("Not Significant", "Significant", ""), values = c("#3F8A18", "#F38F00"))
  
  plot.effect.1 <- ggplot() +
    geom_line(data = snp.weather.interval,aes(x = GDD.Interval,y = effect, group = SNP), color = "#3F8A18") +
    geom_point(data = snp.weather.interval, aes(x = GDD.Interval, y = effect, shape = significance, color = significance, group = SNP, fill = significance)) +
    theme(axis.text.x = element_text(angle = 90)) +
    theme_light() +
    xlim(100, 1500) + 
    xlab("GDD") +
    scale_y_continuous(
      name = "Effect"
    ) +
    facet_wrap(~ data.year, ncol = 4) +
    scale_shape_manual(name = "Significance", labels = c("Not Significant", "Significant", ""), values = c(19,24,1)) +
    scale_color_manual(name = "Significance", labels = c("Not Significant", "Significant", ""), values = c("#3F8A18", "#000000")) +
    scale_fill_manual(name = "Significance", labels = c("Not Significant", "Significant", ""), values = c("#3F8A18", "#F38F00"))

  # export a pdf with the plots
  pdf(paste0(Path,"SNP_effect_graphs/SNP_ALL_Significant_",snp,".pdf"), width = 8, height = 2)
  print(plot.effect)
  print(plot.effect.1)
  dev.off()
  
}
```
## Plotting Locations of Significant SNPs

  - plots the location of significant SNPs on the chromosomes of maize
  
```{r}
library(tidyverse)
Path <- "/Users/dorothykirsch/Desktop/Projects_Data/Flights/All/"
# make a list of the significant snp files
sig.snps <- Sys.glob(file.path(paste0(Path,"SNP_effect_graphs"), "SNP_ALL_Sig*.csv"))
# initiate file to hold all significant SNPs
AllSigSNP.2 <- as.data.frame(matrix(ncol = 12, nrow = 0))
colnames(AllSigSNP.2) <- c("Row","GDD.Interval","GWAS","SNP","P.value","effect","significance","logp","RainfallTotal","cum.rain","data.year","data.type")
# for loop to go through all the sig snp files
for (a in 1:length(sig.snps)) {
  # read in the file
  snp.file <- read.csv(sig.snps[a])
    # filter to only significant
  snp.file.filtered <- snp.file %>%
    filter(significance == "sig")
  # remove multiple iterations of SNPs significant from 850-1250 in interval in 2020
  if (unique(snp.file.filtered$SNP) == "S1_207702224" | unique(snp.file.filtered$SNP) == "S1_304869218" | unique(snp.file.filtered$SNP) == "S2_194931232" | unique(snp.file.filtered$SNP) == "S2_218307839" | unique(snp.file.filtered$SNP) == "S4_44226047" | unique(snp.file.filtered$SNP) == "S4_220462642" | unique(snp.file.filtered$SNP) == "S10_108837807") {
    snp.file.filtered <- snp.file.filtered %>%
      filter(GDD.Interval == 1050)
  }
  # add significance to AllSigSNP.2
  AllSigSNP.2 <- rbind(AllSigSNP.2,snp.file.filtered)
}

AllSigSNP.3 <- AllSigSNP.2 %>%
  separate(GWAS, c("Data.Type","Year","GDD"), sep = "_") %>%
  separate(SNP, c(NA,"SNP.2"), sep = "S", remove = F) %>%
  separate(SNP.2, c("Chromosome","Position"), sep = "_")  %>%
  dplyr::select("SNP","Chromosome","Position","P.value","effect","logp","Year","Data.Type","GDD") %>%
  mutate(GDD = ifelse(is.na(GDD), "Terminal", GDD))
  
# AllSigSNP <- read.delim("/Users/dorothykirsch/Desktop/Projects_Data/Flights/All/All_Significant_SNPs_and_Descriptive_Info.txt", sep = " ")

Bases <- read.csv("/Users/dorothykirsch/Desktop/Projects_Data/Flights/All/MaizeChromosomes.csv")

ChromSigSNP <- merge(AllSigSNP.3, Bases, by = "Chromosome")
SNP <- AllSigSNP.3 %>%
  separate(GDD, into = c("X","GDD"), sep = "X") %>%
  dplyr::select(-X)

  # make chromosome and end1 as numeric, trait as factor
  SNP$Chromosome <- as.factor(SNP$Chromosome)
  SNP$Position <- as.numeric(SNP$Position)
  SNP$GDD <- as.factor(SNP$GDD)
  SNP$GDD <- factor(SNP$GDD, levels = c("150","200","250","300","350", 
                                            "400","450","500","550","600","650","700",
                                            "750","800","850","900","950","1000","1050",
                                            "1100","1150","1200","1250","1300","1350","1400","1450"
                                        ))
  
  #create a new table of maize chromosome length and make it a data frame
  maize_chromosomes <- cbind(chromosome = c(1:10), start = c(rep(0,10)), end = c(307041717, 243907191, 235667834, 246994605, 223902240, 174033170, 182381542, 181122637, 159769782, 150982314))
  maize_chromosomes <- data.frame(maize_chromosomes)
  str(maize_chromosomes)

  # plot location of significant snps
pdf("/Users/dorothykirsch/Desktop/Projects_Data/Flights/All/All_Significant_SNPs_Over_Time_Plots.pdf", height = 3, width = 6.5)

temp_plot <- AllSigSNP.3 %>%
  mutate(Chrom.Pos  = paste0(Chromosome, ".", Position)) %>%
  separate("GDD", into = c(NA,"GDD"), sep = "X", remove = T, fill = "left") %>%
  filter(Data.Type == "GDD" | Data.Type == "Terminal") %>%
  ggplot() +
  geom_point(aes(x = factor(GDD, levels = c("150","200","250","300","350","400","450","500","550","600","650","700","750","800","850", "900","950","1000","1050","1100","1150","1200","1250","1300","1350","1400","1450","Terminal")), y = as.numeric(Chrom.Pos), color = as.character(Year), shape = as.character(Year), group = SNP), size = 3) +
  geom_line(aes(x = factor(GDD, levels = c("150","200","250","300","350","400","450","500","550","600","650","700","750","800","850", "900","950","1000","1050","1100","1150","1200","1250","1300","1350","1400","1450","Terminal")), y = as.numeric(Chrom.Pos),color = as.character(Year),group = SNP)) +
  scale_shape_manual(name = "Year", labels = c("2018","2019","2020","2021"), values = c(0,3,4,20)) +
  scale_color_manual(name = "Year", labels = c("2018","2019","2020","2021"), values = c("#00B8BA","#FF5355","#5BA600","#C751FF")) +
  theme_light() +
  scale_y_continuous(breaks = seq(1,10, by = 1)) +
  scale_x_discrete(drop = F) +
  theme(
    axis.text.x = element_text(angle = 90),
    legend.position = "none"
  ) +
  xlab("Growing Degree Days") + 
  ylab("Chromosomes") 
print(temp_plot)



temp_plot <- AllSigSNP.3 %>%
  mutate(Chrom.Pos  = paste0(Chromosome, ".", Position)) %>%
  separate("GDD", into = c(NA,"GDD"), sep = "X", remove = T, fill = "left") %>%
  filter(Data.Type == "interval" | Data.Type == "Terminal") %>%
  ggplot() +
  geom_point(aes(x = factor(GDD, levels = c("150","200","250","300","350","400","450","500","550","600","650","700","750","800","850", "900","950","1000","1050","1100","1150","1200","1250","1300","1350","1400","1450","Terminal")), y = as.numeric(Chrom.Pos), color = as.character(Year), shape = as.character(Year), group = SNP), size = 3) +
  geom_line(aes(x = factor(GDD, levels = c("150","200","250","300","350","400","450","500","550","600","650","700","750","800","850", "900","950","1000","1050","1100","1150","1200","1250","1300","1350","1400","1450","Terminal")), y = as.numeric(Chrom.Pos),color = as.character(Year),group = SNP)) +
  scale_shape_manual(name = "Year", labels = c("2018","2019","2020","2021"), values = c(0,3,4,20)) +
  scale_color_manual(name = "Year", labels = c("2018","2019","2020","2021"), values = c("#00B8BA","#FF5355","#5BA600","#C751FF")) +
  theme_light() +
  scale_y_continuous(breaks = seq(1,10, by = 1)) +
  scale_x_discrete(drop = F) +
  theme(
    axis.text.x = element_text(angle = 90),
    legend.position = "none"
  ) +
  xlab("Growing Degree Day Slopes") + 
  ylab("Chromosomes") 
print(temp_plot)

dev.off()

AllSigSNP.gdd <- AllSigSNP.3 %>%
  filter(Data.Type == "GDD") 
  
# for loops to find the number of intersections for each year
for (a in c("2018","2019","2020","2021")) {
  for (b in c("2019","2020","2021")) {
    first <- AllSigSNP.gdd %>%
      filter(Year == a) %>%
      dplyr::select(SNP)
    second <- AllSigSNP.gdd %>%
      filter(Year == b) %>%
      dplyr::select(SNP)
    assign(paste0(a,"_clust_",b,"_gdd"),as.numeric(count(generics::intersect(first,second))))
  }
}

# make upset plot of gdd
upset_data <- c("2018&2019" = `2018_clust_2019_gdd`,
                "2018&2020" = `2018_clust_2020_gdd`,
                "2018&2021" = `2018_clust_2021_gdd`,
                "2019&2020" = `2019_clust_2020_gdd`,
                "2019&2021" = `2019_clust_2021_gdd`,
                "2020&2021" = `2020_clust_2021_gdd`)

pdf("/Users/dorothykirsch/Desktop/Projects_Data/Flights/All/All_Significant_SNPs_Over_Time_Upset_Plot_GDD.pdf", height = 2, width = 5)

UpSetR::upset(UpSetR::fromExpression(upset_data),
        sets = c("2018",
                 "2019",
                 "2020",
                 "2021"),
        mb.ratio = c(0.6, 0.4),
        number.angles = 0, 
        text.scale = 1.1, 
        point.size = 2.8, 
        line.size = 1,
        keep.order = T,
        sets.x.label = element_blank(), 
        empty.intersections = "on",
        #mainbar.y.label = paste0("Intersection of ",data.type," Clusters"),
        #sets.bar.color = c("red","red","red","red","blue","blue","blue","blue","orange","orange","orange","orange"),
        show.numbers = "no",
        set_size.show = F
        #main.bar.color = c("red","red","red","blue","orange","orange","orange","blue","blue","blue","blue","red","blue","orange","red","orange","red","orange","blue","orange","red")
  )

dev.off()


AllSigSNP.interval <- AllSigSNP.3 %>%
  filter(Data.Type == "interval")
  
# for loops to find the number of intersections for each year
for (a in c("2018","2019","2020","2021")) {
  for (b in c("2019","2020","2021")) {
    first <- AllSigSNP.interval %>%
      filter(Year == a) %>%
      dplyr::select(SNP)
    second <- AllSigSNP.interval %>%
      filter(Year == b) %>%
      dplyr::select(SNP)
    assign(paste0(a,"_clust_",b,"_interval"),as.numeric(count(generics::intersect(first,second))))
  }
}  
```
## Local identification of SNPs significant over multiple years & timepoints

  - identifies SNPs significant over multiple iterations and over multiple years

```{r}
library(tidyverse)
# set path
Path <- "/Users/dorothykirsch/Desktop/Projects_Data/Flights/All/"
# make a list of the significant snp files
sig.snps <- Sys.glob(file.path(paste0(Path,"SNP_effect_graphs"), "SNP_ALL_Sig*.csv"))
# initiate file to count the SNPs significant in more than 1 year
year.list <- data.frame()
# initiate file to count the SNPs significant in more than 1 timepoint
point.list <- data.frame()
# initiate file to count the SNPs not found by terminal height
not.term.list <- data.frame()
# initiate file to count the SNPs found by terminal height
term.list <- data.frame()
# initiate file to count the SNPs only found at terminal height
only.term.list <- data.frame()
# for loop to go through all the sig snp files
for (a in 1:length(sig.snps)) {
  # read in the file
  snp.file <- read.csv(sig.snps[a]) %>%
    mutate(GDD.Interval = ifelse(is.na(GDD.Interval), "Terminal",GDD.Interval))
  # filter to only significant
  snp.file.filtered <- snp.file %>%
    filter(significance == "sig")
  # see if multiple years and add to snp.list if so
  if (length(unique(snp.file.filtered$data.year)) > 1) {
    year.list <- append(year.list, as.character(unique(snp.file.filtered$SNP)))
  }
  # see if multiple timepoints and add to point.list
  if (length(unique(snp.file.filtered$GWAS)) > 1) {
    point.list <- append(point.list, as.character(unique(snp.file.filtered$SNP)))
  }
  snp.file.filtered$GDD.Interval <- as.character(snp.file.filtered$GDD.Interval)
  if (sum(str_detect(snp.file.filtered$GDD.Interval, '^Terminal$')) == 0) {
    not.term.list <- append(not.term.list, as.character(unique(snp.file.filtered$SNP)))
  }
  if (sum(str_detect(snp.file.filtered$GDD.Interval, '^Terminal$')) > 0) {
    term.list <- append(term.list, as.character(unique(snp.file.filtered$SNP)))
  }
  if (sum(str_detect(snp.file.filtered$GDD.Interval, '^Terminal$')) == nrow(snp.file.filtered)) {
    only.term.list <- append(only.term.list, as.character(unique(snp.file.filtered$SNP)))
  }
}
```
## Functional Annotation of Significant SNPs

  - uses B73v4 reference annotation to designate a functional annotation for each significant SNP based on closest gene

```{r}
library(tidyverse)

SNPs_of_Interest <- read.delim("/Users/dorothykirsch/Desktop/Projects_Data/Flights/All/All_Significant_SNPs_and_Descriptive_Info.txt", sep = " ") %>%
  dplyr::select(SNP, Chromosome, Position) %>%
  mutate(Chromosome = paste0("Chr",Chromosome))


gff3_data <- read_delim("/Users/dorothykirsch/Desktop/Projects_Data/Flights/All/Zm-B73-REFERENCE-GRAMENE-4.0_Zm00001d.2.gff3", delim = '\t', skip = 2, col_names = c('Chromosome', 'Var_2', 'Feature_Type', 'Start', 'End', 'Var_X', 'Var_Y', 'Var_Z', 'Info')) %>% # I have already filtered the gff3 file for gene designations
  dplyr::select(Chromosome, Feature_Type, Start, End, Info) %>% # Only use the info I need so things take up less memory
  separate(Info, into = c('Gene', 'Name', 'Biotype'), sep = ';') %>% # Split the Info column into the things I will be combining by
  dplyr::select(-c(Name, Biotype)) %>% # Remove unnecessary information
  mutate(Gene = str_remove(Gene, "ID=")) %>%# Remove unneeded strings
  separate(Gene, into = c("Type", "Gene"), sep = ":") %>%
  dplyr::select(-Type) %>%
  separate(Gene, into = c("Gene","Suffix"), sep = "_") %>%
  dplyr::select(-Suffix) %>%
  filter(Feature_Type == "gene")

# Load B73v4 Annotation File
B73_Ann <- read_delim('/Users/dorothykirsch/Desktop/Projects_Data/Flights/All/B73v4.gene_function.txt', col_names = c('Gene', 'Functional_Annotation', 'Source'), delim = '\t') %>%
  mutate(Functional_Annotation = str_remove_all(Functional_Annotation, '^ '),
         Source = str_remove_all(Source, '^ ')) %>%
  dplyr::select(-Source)

# Combine GFF3 file with SNP and Expression Data
joined.data <- gff3_data %>%
  left_join(SNPs_of_Interest) %>% # Pulls in more data on SNPs of interest
  mutate(Within_Int = case_when(Position >= Start & Position <= End ~ "YES",
                                Position < Start | Position > End ~ "NO")) %>%
  distinct() %>%
  left_join(B73_Ann) 

# Significant SNPs inside the gene
in.gene <- joined.data %>%
  filter(Within_Int == "YES") %>%
  dplyr::select(SNP,Chromosome, Gene, Position, Functional_Annotation)

# list SNPs found in genes
in.gene.snps <- unique(as.character(in.gene$SNP))

# Significant SNPs next to genes
by.gene <- joined.data %>%
  filter(Within_Int == "NO",
         !(SNP %in% in.gene.snps)) %>%
  mutate(Distance.start = abs(Start - Position),
         Distance.end = abs(End - Position)) %>%
  group_by(SNP) %>%
  mutate(Closest.start = min(Distance.start),
         Closest.end = min(Distance.end), 
         Closest = min(c(Closest.start,Closest.end))) %>%
  filter(Closest == Distance.start | Closest == Distance.end) %>%
  ungroup() %>%
  dplyr::select(SNP, Chromosome, Gene, Position, Functional_Annotation)
  
# merge Significant SNPs in genes with significant SNPs with closest gene
in.by.gene <- rbind(in.gene, by.gene)

temp_plot <- na.omit(in.by.gene) %>%
  count(Functional_Annotation) %>%
  ggplot(aes(x = Functional_Annotation, y = n, fill = as.character(n))) +
  geom_bar(stat = "identity") +
  theme(
    axis.ticks.x = element_blank(),
    axis.text.x = element_text(angle = 90, size = 4),
    legend.position = "none"
  ) +
  xlab("Functional Annotation") +
  ylab("Count")

# print pdf 0f barplot
pdf("/Users/dorothykirsch/Desktop/Projects_Data/Flights/All/GWAS_Result_Gene_Info.pdf", height = 9.48, width = 13.75)
print(temp_plot)
dev.off()
  
# print out the list of SNPs and the associated genes
write.csv(in.by.gene, file = "/Users/dorothykirsch/Desktop/Projects_Data/Flights/All/GWAS_Result_Gene_Info.csv")

```
## Local heatmap of SNP effect throughout growing season

 - separates SNP effects into GDD and slope due to far smaller effects in slope
 - plots heatmap of SNP effect for each significant SNP for each iteration
 
```{r}
library(tidyverse)
library(RColorBrewer)
library(gplots)
# set path
Path <- "/Users/dorothykirsch/Desktop/Projects_Data/Flights/All/"
# make a list of the significant snp files
sig.snps <- Sys.glob(file.path(paste0(Path,"SNP_effect_graphs"), "SNP_ALL_Sig*.csv"))
# initiate master.snp.file
master.snp.file <- data.frame()
# for loop to go through all the sig snp files
for (a in 1:length(sig.snps)) {
  # read in the file
  snp.file <- read.csv(sig.snps[a])
  # make data.type column mean something
  for (t in 1:nrow(snp.file)) {
    if (is.na(as.character(snp.file[t,3]))) {
      snp.file[t,12] = "Precipition"
    } else if (startsWith(as.character(snp.file[t,3]), "GDD")) {
      snp.file[t, 12] = "GDD"
    } else if (startsWith(as.character(snp.file[t,3]), "interval")) {
      snp.file[t,12] =  "Interval"
    }
  }
  # filter to only GWAS results
  snp.file <- snp.file %>%
    filter(data.type == "GDD" | data.type == "Interval") %>%
    dplyr::select(GDD.Interval,GWAS,SNP,effect,data.year,data.type)
  # add snp.file to master file with all SNPs
  master.snp.file <- rbind(master.snp.file, snp.file)
}
# remove duplicates present from precipitation datat
master.snp.file.1 <- unique(master.snp.file)
# set list with either GDD or Interval
iteration <- c("GDD", "Interval")

# for loop to do both GDD and slopes
for (b in 1:2) {
  ite <- iteration[b]
  # break down to just GDD, remove unnecessary columns and pivot wider
  master.snp.file.2 <- master.snp.file.1 %>%
    filter(data.type == ite) %>%
    mutate(snp.year = paste(data.year,SNP,sep = "_")) %>%
    dplyr::select(GDD.Interval,effect,snp.year)  %>%
    pivot_wider(names_from = GDD.Interval, values_from = effect)

  # make the snp.year column the row names - different for Interval and GDD
  pulled.row.names <- master.snp.file.2$snp.year # keep rownames separate
  master.snp.file.2$snp.year <- NULL # remove string column
  if(b == 1) {
    col_order <- seq(from = 150, to = 1450, by = 50)
    master.snp.file.2 <- master.snp.file.2[,as.character(col_order)]
  } else if (b ==2) {
    col_order <- seq(from = 150, to = 1400, by = 50)
    master.snp.file.2 <- master.snp.file.2[,as.character(col_order)]
    master.snp.file.2$`450` <- NULL
  }
  master.snp.file.2 <- as.matrix(master.snp.file.2) # make numeric matrix
  rownames(master.snp.file.2) <- pulled.row.names # add snp.year as row names

  ### make a heatmap ###
  #my_group <- as.numeric(as.factor(substr(rownames(master.snp.file.2), start = 3 , stop = 4))) #   make a file for the colsidecolors as years
  my_group <- as.factor(as.numeric(substr(rownames(master.snp.file.2), start = 3 , stop = 4))) #   make a file for the colsidecolors as years
  colSide <- brewer.pal(4, "Dark2")[my_group] # assign colors based on years
  master.snp.file.2[is.na(master.snp.file.2)] <- 0 # turn NA values into zeros
  my_colors <- RColorBrewer::brewer.pal(9, "Greens")[c(1,3:9)]
  # assign location for year color legend
  coords <- data.frame(c(0.9569458),c(1.16734))
  colnames(coords) <- c("x","y")
  coords <- as.list(coords)
  # export same heatmap twice, once with the key and once without
  pdf(paste0(Path,"SNP_Effect_Heatmap_All_",ite,".pdf"), height = 4, width = 8)
    # actually make the heatmap
    heatmap.2(t(abs(master.snp.file.2)), Rowv = F, dendrogram = "column",trace = "none", col = my_colors, ColSideColors = colSide, xlab = NA, labCol = F,margins = c(2,5), key = F, lwid = c(0.2,5), lhei = c(.5,2,20), colsep = F)
    # make a heatmap with the key to put on the other one
    heatmap.2(t(abs(master.snp.file.2)), Rowv = F, dendrogram = "column",trace = "none", col = my_colors, ColSideColors = colSide, xlab = NA, labCol = F,lwid = c(0.75,5), lhei = c(0.5,2,10), key.title = NA, key.ylab = NA, key.xlab = NA)
    legend(coords,xpd = T, title = "Year",legend=c("2018","2019","2020","2021"), 
       fill=brewer.pal(4, "Dark2"), cex=0.8, box.lty=0, horiz = T, xjust = 1, yjust = 0)
  dev.off()
    # plot the heatmap again and save it for extraction of dendrogram

    out <- heatmap.2(t(abs(master.snp.file.2)), Rowv = F, dendrogram = "column",trace = "none", col = colorRampPalette(brewer.pal(8, "Greens")), ColSideColors = colSide, xlab = "Significant SNPs", labCol = F,lwid = c(1,5))
    
    dendro <- as.hclust(out$colDendrogram)
    
   # pdf(paste0(Path,"SNP_Effect_Dendrogram_All_",ite,".pdf"), height = 25, width = 250)
    #  plot(as.dendrogram(dendro))
    #dev.off()
    
}
```
## Different SNP Plots

  - plots SNP results in multiple different ways
  
```{r}
library(tidyverse)
library(ggridges)
library(forcats)
library(gtExtras)

Path <- "/Users/dorothykirsch/Desktop/Projects_Data/Flights/All/"
# make a list of the significant snp files
sig.snps <- Sys.glob(file.path(paste0(Path,"SNP_effect_graphs"), "SNP_ALL_Sig*.csv"))
# initiate file to hold all significant SNPs
AllSigSNP.2 <- as.data.frame(matrix(ncol = 12, nrow = 0))
colnames(AllSigSNP.2) <- c("Row","GDD.Interval","GWAS","SNP","P.value","effect","significance","logp","RainfallTotal","cum.rain","data.year","data.type")
# for loop to go through all the sig snp files
for (a in 1:length(sig.snps)) {
  # read in the file
  snp.file <- read.csv(sig.snps[a])
  # filter to only significant
  snp.file.filtered <- snp.file %>%
    filter(significance == "sig")
  # remove multiple iterations of SNPs significant from 850-1250 in interval in 2020
  if (unique(snp.file.filtered$SNP) == "S1_207702224" | unique(snp.file.filtered$SNP) == "S1_304869218" | unique(snp.file.filtered$SNP) == "S2_194931232" | unique(snp.file.filtered$SNP) == "S2_218307839" | unique(snp.file.filtered$SNP) == "S4_44226047" | unique(snp.file.filtered$SNP) == "S4_220462642" | unique(snp.file.filtered$SNP) == "S10_108837807") {
    snp.file.filtered <- snp.file.filtered %>%
      filter(GDD.Interval == 1050)
  }
  # add significance to AllSigSNP.2
  AllSigSNP.2 <- rbind(AllSigSNP.2,snp.file.filtered)
}

AllSigSNP.3 <- AllSigSNP.2 %>%
  separate(GWAS, c("Data.Type","Year","GDD"), sep = "_") %>%
  separate(SNP, c(NA,"SNP.2"), sep = "S", remove = F) %>%
  separate(SNP.2, c("Chromosome","Position"), sep = "_")  %>%
  dplyr::select("SNP","Chromosome","Position","P.value","effect","logp","Year","Data.Type","GDD") %>%
  mutate(GDD = ifelse(is.na(GDD), "Terminal", GDD))

# AllSigSNP <- read.delim("/Users/dorothykirsch/Desktop/Projects_Data/Flights/All/All_Significant_SNPs_and_Descriptive_Info.txt", sep = " ")

Bases <- read.csv("/Users/dorothykirsch/Desktop/Projects_Data/Flights/All/MaizeChromosomes.csv")

ChromSigSNP <- merge(AllSigSNP.3, Bases, by = "Chromosome")
SNP <- AllSigSNP.3 %>%
  separate(GDD, into = c("X","GDD"), sep = "X") %>%
  dplyr::select(-X)

# make chromosome and end1 as numeric, trait as factor
SNP$Chromosome <- as.factor(SNP$Chromosome)
SNP$Position <- as.numeric(SNP$Position)
SNP$GDD <- as.factor(SNP$GDD)
SNP$GDD <- factor(SNP$GDD, levels = c("150","200","250","300","350", 
                                      "400","450","500","550","600","650","700",
                                      "750","800","850","900","950","1000","1050",
                                      "1100","1150","1200","1250","1300","1350","1400","1450"
))

#create a new table of maize chromosome length and make it a data frame
maize_chromosomes <- cbind(chromosome = c(1:10), start = c(rep(0,10)), end = c(307041717, 243907191, 235667834, 246994605, 223902240, 174033170, 182381542, 181122637, 159769782, 150982314))
maize_chromosomes <- data.frame(maize_chromosomes)
str(maize_chromosomes)


# read in PVE for all significant SNPs
PVE <- read.csv(paste0(Path,"SNP_effect_graphs/PVE_all_iterations_sig_SNPs.csv"), header = F)
colnames(PVE) <- c("Iteration","PVE")

PVE.1 <- PVE %>%
  separate(Iteration, into = c("Data.Type","Year","GDD")) %>%
  mutate(GDD = ifelse(is.na(GDD),"Terminal",GDD))

pdf("/Users/dorothykirsch/Desktop/Projects_Data/Flights/All/Significant_SNPs_Timepoints_Sig.pdf", height = 6.25, width = 2.5)
AllSigSNP.3 %>%
  group_by(SNP) %>%
  mutate(count = n()) %>%
  dplyr::select(SNP,Year,count) %>%
  unique() %>%
  ggplot(aes(x = count)) + #, fill = Year
  geom_bar(stat = "count", width = 1) +
  theme_minimal() +
  theme(
    #panel.spacing = unit(0.1, "lines"),
    legend.position = "none",
    panel.grid = element_blank(),
    panel.background = element_rect(fill = NA, color = "black"),
    axis.text.x = element_text(size = 7.5)
  ) +
  xlab("Number of Times Significant") +
  ylab("Count") + 
  scale_x_continuous(breaks = c(1,2,3,4,5,6,7,8,9,10,11), limits = c(0,12)) +
  facet_grid(Year~.)
dev.off()

pdf("/Users/dorothykirsch/Desktop/Projects_Data/Flights/All/All_Significant_SNPs_Over_Time_Plots_2.pdf", height = 3, width = 4)
AllSigSNP.3 %>%
  group_by(Year, GDD) %>%
  mutate(count = n()) %>%
  left_join(PVE.1, by = c("Data.Type","Year","GDD")) %>%
  separate(GDD, into = c(NA, "GDD"), fill = "left", sep = "X") %>%
  dplyr::select(Data.Type,Year, GDD, count,PVE) %>%
  mutate(PVE = ifelse(is.na(PVE),0,PVE)) %>%
  filter(Data.Type != "interval") %>%
  unique() %>%
  ggplot(aes(x = factor(GDD, levels = c(seq(150,1450,50),"Terminal")), y = count, fill = PVE)) +
  geom_bar(stat = "identity") + 
  theme_minimal() +
  scale_x_discrete(drop = F) + 
  theme(
    axis.text.x = element_text(angle = 90),
    legend.position = "none",
    panel.grid = element_blank(),
    panel.background = element_rect(fill = NA, color = "black")
  ) +
  xlab("Growing Degree Days") +
  ylab("Number of Significant SNPs") +
  scale_fill_gradient(low = "#F1BFCE", high = "#7C0024", limits = c(0,0.35)) +
  facet_grid(Year ~.)

AllSigSNP.3 %>%
  group_by(Year, GDD) %>%
  mutate(count = n()) %>%
  left_join(PVE.1, by = c("Data.Type","Year","GDD")) %>%
  separate(GDD, into = c(NA, "GDD"), fill = "left", sep = "X") %>%
  dplyr::select(Data.Type,Year, GDD, count,PVE) %>%
  mutate(PVE = ifelse(is.na(PVE),0,PVE)) %>%
  filter(Data.Type == "interval") %>%
  unique() %>%
  ggplot(aes(x = factor(GDD, levels = c(seq(150,1450,50),"Terminal")), y = count, fill = PVE)) +
  geom_bar(stat = "identity") + 
  theme_minimal() +
  scale_x_discrete(drop = F) + 
  theme(
    axis.text.x = element_text(angle = 90),
    legend.position = "none",
    panel.grid = element_blank(),
    panel.background = element_rect(fill = NA, color = "black")
  ) +
  xlab("Growing Degree Days") +
  ylab("Number of Significant SNPs") +
  scale_fill_gradient(low = "#F1BFCE", high = "#7C0024", limits = c(0,0.35)) +
  facet_grid(Year ~.) 
dev.off()
```

